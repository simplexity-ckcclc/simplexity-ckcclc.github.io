<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[DID分布式身份（2） - 账户体系架构]]></title>
    <url>%2F2020%2F11%2F18%2Fdid-account-system%2F</url>
    <content type="text"><![CDATA[Post is cheap, show me the password Incorrect Password! No content to display! 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]]></content>
      <tags>
        <tag>DID</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DID分布式身份（1） - W3C规范]]></title>
    <url>%2F2020%2F11%2F17%2Fdid-w3c%2F</url>
    <content type="text"><![CDATA[DID(decentralized identifier)分布式身份的特点： 自主。实体可自主完成DID的注册、解析、更新或撤销操作（上链），⽆需中央注册机构实现全局唯⼀性。 身份互通。多个dapp应用可以共用一个DID，共用一套身份账户体系。 隐私。⼀个实体可以拥有多个身份，由实体自⼰进⾏管理、维护，不同的身份之间没有关联信息，避免身份信息被第三⽅归集。同时，未授权的第三方dapp应用无法得知DID对应的具体用户信息。 W3C标准认可度较高，是行业的主要标准，大多数平台遵从W3C的规范，这样数据可以方便移植和共享，实现底层区块链平台的兼容。 1. 格式规范1Scheme:DID Method:DID Method Specific String Scheme : 通常固定为did，指明该串为DID标志 DID Method : 指定具体遵从的格式规范，通常为不同的服务运营商 DID Method Specific String ：具体的实体的标志，通常为URL或者URI标识符。例如，微众的WeIdentity为 did:weid:101:0x0086eb1f712ebc6f1c276e12ec21, 具体参看Ref[2]。 2. DID文档（Document）JSON结构，自描述的文档，通常包含有DID本身，公钥信息等内容，可支持解析，更新，撤销等功能。可以存在区块链或者非区块链的分布式账本底层平台，或者存放在IPFS等分布式文件系统里。可以理解成DID与Document是K-V结构。具体字段格式内容参看Ref1。 3. 可验证声明（Verifiable Credential） 建立DID体系的价值所在。DID可以自管理，但是只有被相关方认证过的身份才会被信任，才有业务上的价值。 包含若⼲个“声明(claims) ”。声明信息是与身份关联的属性信息，如姓名，年龄、学历、职业等等。 凭证由发⾏者签名，可通过密码学证明是否由凭证中声称的实体签发且未被篡改，因此被称为可验证凭证。 VC也可以依赖于区块链实现流转，验证。 可以通过授权控制，只透露Document里的指定字段。结合零知识证明，可以只证明某些论断，而不透露具体的信息。 模型参看W3C Draft: Verifiable Credentials Data Model 1.0 Ref： W3C Draft: Decentralized Identifiers (DIDs) v1.0: W3C的规范介绍。 WeIdentity 规范： 微众的DID规范介绍，科普]]></content>
      <tags>
        <tag>DID</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[链上隐私数据]]></title>
    <url>%2F2020%2F11%2F04%2Fprivacy-on-chain%2F</url>
    <content type="text"><![CDATA[由于区块链各参与方可能存在利益冲突，不愿意将私有数据上链，催生了链上隐私数据的需求。对于不同的隐私等级存在相应的处理方式。 短期内隐私数据。指的是易变数据，一定期限后该数据的价值降低，例如波动的商品成交价。因为密码学破解在可预见的短期内无法达成，可以认为安全风险很小，因此可以使用流行的对称加密/非对称加密的方式，密文上链，相关方使用链下方式或者链上方式交换密钥。甚至可以单笔交易更换密钥，这样对于可变但是数值范围有限的数据，例如状态值，单价/数量等数值，则避免了统计学上破解。 长期固定隐私数据。例如身份证号码，企业社会信用代码等，该数据长期不变，存在若干年后由于量子计算或者相应的密码学被破解导致泄露（当然，随着破解方案的演进，加密方案也会相应的演进，但是对于链上的历史数据则极易受到新的破解方案的威胁）。因此，链上可以保存数值hash，具体的数据链下交换，使用链上的hash值作为验证。同时，对于数值类等取值空间有限的，可以针对每笔交易数据添加盐值后（同样通过链下交换），再做hash上链，避免彩虹表攻击。 Fabric的private data设计1. transient data vs. transient data store vs. private data collection transient data: 链码chaincode的输入数据，client发起交易时指定，endorser在背书过程中，将这些数据以gossip的方式点对点发送到其他的拥有权限的相关节点（例如同属于该private data collection的不同组织），链上block里的交易信息并不会包含有这些数据。 transient data store: 中间存储，存放transient data进行运算后的RW-set，方便后续的gossip交换到其他的authorized节点，在commit阶段，private data hash将会与该部分数据进行校验，无误后提交到private data collection，然后清除transient data store。 private data collection: 长期存储（由blockToLive参数控制生命）区块执行之后的world state世界状态里，包含有public state和private state，（分别对应着public RW-set和private RW-set）这部分是通过gossip协议从其他节点发送过来的，针对相关的org开放读写权限。包含原始数据和该数据的hash值组成，这样的hash值会被在commit阶段比较private RW-set，以及后续在chaincode里确认该笔数据或者交易的存在真实性。 Fig1. 隐私数据存储结构（摘自Ref.3） Fig2. 隐私数据交互流程（摘自Ref.2） The transactions with the private data hashes get included in blocks as normal. The block with the private data hashes is distributed to all the peers.即private data hash会提交到orderer进行排序，对于没有权限的private data collection，只能获取到其hash作为存证和后续验证。 Note that because gossip distributes the private data peer-to-peer across authorized organizations, it is required to set up anchor peers on the channel, and configure CORE_PEER_GOSSIP_EXTERNALENDPOINT on each peer, in order to bootstrap cross-organization communication.。意味着gossip协议会根据collection的json定义文件里的policy政策传播到符合条件的各个组织。 当前fabric提供的是hash校验的基本功能，只是数据的真实性校验和数据交换，对于更多的应用场景例如需要计算/比较的，将来可以扩展叠加零知识证明和同态加密运算等，会有更广的应用想象空间。 Ref： HyperLedger Fabric官网 Demystifying Hyperledger Fabric (2/3): Private Data Collection Private Data and Transient Data in Hyperledger Fabric 视频 ： IBM工程师介绍private data]]></content>
      <categories>
        <category>Blockchain</category>
      </categories>
      <tags>
        <tag>Hyperledger Fabric</tag>
        <tag>Blockchain</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多方安全计算MPC的若干个概念]]></title>
    <url>%2F2020%2F10%2F20%2Fmpc-terminology%2F</url>
    <content type="text"><![CDATA[多方安全计算（MPC）与可信计算环境（TEE），联邦学习（Federated Learning）是隐私计算领域的三个重要实现方式。 1. 不经意传输（OT，Oblivious Transfer）目标如下： Sender提供可选参数，但是不知道Receiver具体选择哪个 Receiver可以选择其中一个，但是不知道除此之外的其他参数值 1-ou-of-2的实现方案有多种，其中一个基于公私钥机制的实现方案： Sender拥有两个消息，分别为m0, m1,并且生成两对密钥对（pub0, pri0）和（pub1, pri1） Receiver随机生成对称密钥k，以Receiver选择消息m0为例，用公钥pub0加密k，得到k0和k1，返回给Sender。 Sender使用私钥（pri0，pri1）分别解密（k0，k1），得到（k0’, k1’）。此时，Sender无法得知具体选择了哪个，满足条件（1）。对Receiver而言，其中k0’=k，k1’则是一串不明字符串。 Sender使用对称密钥k0’和k1’分别对m0, m1加密，得到m0’， m1’，并回传给Receiver。 Receiver可以用k对m0’解密，得到m0。另外，m1’则是不可解，满足条件（2）。 参考资料： Oblivious Transfer (OT) 2. 混淆电路（GC，Garbled Circuit）将计算逻辑转换成门电路，以与门为例，流程如下 A生成x为0，1对应的密钥kx0,kx1, y为0，1时对应的密钥ky0,ky1，并且计算如下真值表 假设A输入x=0，A将真值表打乱顺序，将对应的kx0和真值表发送给B。因为顺序乱，所以B不知道收到A的k是kx0还是kx1，即B无法得知A的输入。 假设B选择y=1,通过OT协议获取ky1，此时A也无法得知B的输入。 B通过对真值表里的z进行解密，只有一个可以解出有意义的数值，也就是最终结果。 x y z kx0 ky0 Ekx0(Eky0(0)) kx0 ky1 Ekx0(Eky1(0)) kx1 ky0 Ekx1(Eky0(0)) kx1 ky1 Ekx1(Eky1(1)) 表1: 加密真值表 3. 秘密共享(SS, Secret-Sharing)秘密共享通过把秘密进行分割，并把秘密在n个参与者中分享，使得只有多于特定t个参与者合作才可以计算出或是恢复秘密，而少于t个参与者则不可以得到有关秘密。实现方式例如： 构造t-1次多项式 f(x)=a0 + a1·x + … + at-1 · xt-1。 其中，分享的秘密 S = a0 计算f(1), f(2), …, f(n)，分别分发给参与方P1, …, Pn。此时任意至少t个P组合在一起可以接触方程f(x),也就得到秘密S。 4. 差分隐私(DP, Differential Privacy)中心思想：无法从结果中反推出单个输入个体的信息。临近数据集：指的是两个数据集D1，D2只有单个记录不一致。例如，D1={1, 0, 1}, D1={1, 0, 1, 1}。数据操作是count，则count(D1)=2, count(D2)=2, 则可以反推出最后一个输入为1。主要通过两种方式添加噪声，一是根据数据敏感的数值型（服从拉普拉斯分布的随机噪声），二是离散值(指数分布)。添加的噪声分类如下： 输入噪声： f(x0, x1, …, xn, c) -&gt; f(x0, x1, …, xn + noice, c) 参数噪声： f(x0, x1, …, xn, c) -&gt; f(x0, x1, …, xn, c + noice) 输出噪声： f(x0, x1, …, xn, c) -&gt; f(x0, x1, …, xn, c) + noice]]></content>
      <tags>
        <tag>MPC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Why and How zk-SNARK Works：Definitive Explanation论文学习]]></title>
    <url>%2F2020%2F08%2F12%2Fzk-snark-paper%2F</url>
    <content type="text"><![CDATA[本文为Why and How zk-SNARK Works: Definitive Explanation论文学习心得，纯粹个人在阅读过程中的疑问和思考，详细的理解请阅读附录引用的文章。 2. The Medium of a Proof 多项式是zk-snark的核心 基本流程 1234• Verifier chooses a random value for x and evaluates his polynomial locally• Verifier gives x to the prover and asks to evaluate the polynomial in question• Prover evaluates his polynomial at x and gives the result to the verifier• Verifier checks if the local result is equal to the prover’s result, and if so then the statement is proven with a high confidence 存在问题 prover可以通过其他方式得到多项式结果，没有方法确保该结果是经过多项式计算得来 多项式的阶数需要足够高 3 Non-Interactive Zero-Knowledge of a Polynomial这一章是zk-snark的理论基础。 3.1 Proving Knowledge of a Polynomial 流程 123• Verifier samples a random value r, calculates t = t(r) (i.e., evaluates) and gives r to the prover• Prover calculates h(x) = p(x) and evaluates p(r) and h(r); the resulting values p,h are t(x) provided to the verifier• Verifier then checks that p = t · h, if so those polynomials are equal, meaning that p(x) has t(x) as a cofactor. Q1： 为什么prover不直接传递t ?A1： 需要证明的是 prover知道t，即其中的两个根。下文有说明，prover可能通过别的方式恰好知道t(s)的值，协议不安全。而zk-snark里是将要证明的表达成多项式，然后由verifier构造基础多项式，让prover来赋值输入/输出，通过证明t的根是verifier的目标多项式的根，从而证明其输出的正确性。 Q2: 如果试验次数足够多，例如h的阶数，是否可能推算出h，从而推算出p ?A2: 下文引入零知识证明，通过3.3的同态加密的模运算实现。 存在问题 prover计算得到t后，可以随意选择h，计算p=h*t，verifier无法验证prover是否知道p。 prover恰好知道x=r处的值t(r)，可以构造任何的多项式 3.3 Obscure Evaluation3.3.1 Homomorphic Encryption（同态加密）The general idea is that we choose a base natural number g (say 5) and to encrypt a value we exponentiate g to the power of that value.15^3 = 125, Where 125 is the encryption of 3. 存在问题将125持续的除以5，直到结果为1，此时的step就是加密的3 3.3.3 Strong Homomorphic Encryption模运算。基本流程如下：123encryption : 5^3 = 6 (mod 7)multiplication : 6^2 = (5^3)^2 = 5^6 = 1 (mod 7)addition: 5^3·5^2 =5^5 =3 (mod7) 局限性while we can multiply an encrypted value by an unencrypted value, we cannot multiply (and divide) two encrypted values, as well as we cannot exponentiate an encrypted value.两个加密结果间无法直接进行乘法运算。（下文引入pairing配对函数，解决乘法运算） 3.3.4 Encrypted Polynomial (说明：图中r即论文中的s) 能解决prover恰好知道x=s的值t(s)，可以构造任何的多项式这个问题，因为已经prover已经无法得知s以及t(s)。 不能 解决prover计算得到t后，可以随意选择h，计算p=h*t，verifier无法验证prover是否知道p，例如，可以随意选择r的值，构造 zh = gr 和 zp = (gt(s))r 3.4 Restricting a Polynomial (说明：图中r即论文中的s) 结论：1234– Bob has applied the same exponent (i.e., c) to both values of the tuple– Bob could only use the original Alice’s tuple to maintain the α relationship– Bob knows the applied exponent c, because the only way to produce valid (b,b′) is to use the same exponent– Alice has not learned c for the same reason Bob cannot learn α 即Prover必须使用verifier提供的基于si计算出来的值进行计算，verifier可以通过验证(gp, gp’)间仍然保持 α 的指数偏移量关系。同时verifier无法得知prover使用的参数c的值。 存在问题： Q1: 如果原始的p=(x+1)*(x+2)，其中t=x+1,h=x+2。但是prover捏造h&#39;=x+3，即p&#39;=(x+1)*(x+3)，verifier是否无法验证？A1: 在zk-snark实际应用中看4.8小节 3.5 Zero-Knowledgeverifier可以从E（p）知道关于p的多项式的具体参数。例如上面的例子对E（p）因式分解？解决方案：类似verifier引入 α 保证prover使用其提供的值进行运算，相应的prover也映入 δ 指数偏移量。例如验证 gδ·p = gδ·t(s)h 3.6 Non-Interactivity非交互式。多方各自验证，防止verifier和prover联合作假或者泄漏，proof可以重用提高效率，并且prover无需时刻在线响应。核心是Cryptographic pairings(配对函数)，即找到算法可以支持加密后数值的同态乘法运算，即e(ga, gb) = e(g, g)ab（小节3.3.3中不支持）。这样在setup阶段可以构造composite CRS（common reference string）（小节3.6.3） 小节3.6.2中，由于引入了配对函数，验证加密方法是 e(gp, g1)=e(gt, gh)，而3.3.4同态加法加密小节里，则是验证gp=gt(s)·h。 3.7 Succinct Non-Interactive Argument of Knowledge of Polynomial重要，基础。Setup-Proving-Verification 三步骤，详见文章。 Setup proving key: { gsi&lt;/sup&gt; } i∈[d] , { gαsi&lt;/sup&gt; } i∈[d] (提供给prover) verification key: gα, gt(s) （自留，后续在Verification阶段使用） Proving set the randomized proof π = （gδp(s), gδh(s), gδαp(s))，即（gp, gh, gp′) Verification check polynomial restriction : e(gp′ , g) = e(gp, gα) check polynomial cofactors : e(gp, g) = e(gt(s), gh) 证明了p(x)的根，即p(x）因式分解后含有多项式 t(x)。 4 General-Purpose Zero-Knowledge Proofs上面一章铺垫基础知识，这一章节拓展到实际的应用场景。 4.1 Computation需要证明的是 for the input (1,4,2) of expression f(w,a,b) the output is 8, in other words, we check the equality: w(a × b) + (1 − w)(a + b) = 8根据输入，证明计算的输出结果。将计算逻辑转化成多项式运算。 4.3 Enforcing Operation简化多项式 l(x) operator r(x) = o(x) Q1: 这里证明3·2=6,如果构造时候输入为4·3=12，verifier验证的等式同样成立。如何校验输入的正确性？ A1: 4.8小节的例子 4.4 Proof of Operationp(x) = l(x) · r(x) − o(x)， 并且拥有已知的根。（假如只有一个根a, 则此处t(x)=x-a ） Proving proof π = (gl, gr,go, …) Verification valid operation check: e(gl,gr) = e(gt(s),gh)·e(go,g) 关键在与Proving环节，构造这个p(x)，使用了Pairing函数实现加密的乘法运算。存在4.3上面提到的问题，因为计算这个l,g,o时候使用的是需要证明的输入输出来计算系数。需要保证此l(x),g(x),o(x)能反映需要证明的输入operand和输出output 4.6 Variable Polynomials Setup construct la(x), ld(x) : 在小节4.5的例子中，满足For our example la(x) must conform to evaluations la(1) = 1, la(2) = 1 and la(3) = 0 and ld(x) is zero at 1 and 2 but 1 at x = 3。也就是满足这个特性。 proving key: ( gla(x), gld(x), gαla(x), gαld(x)) Proving assign values a and d and add and provide proof π = （gL(s) = gala(x) · gdld(x) = gala(x) + dld(x))，gαL(s)) : 然后Prover再来构造 L(x) = a·la(x) + d·ld(x)，具体的赋值，也就是输入值。在4.5的例子中，a=2,d=6。也就是限定了，prover必须使用指定的输入。 Verification check polynomial restriction : e(gp′ , g) = e(gp, gα) check polynomial cofactors : e(gp, g) = e(gt(s), gh) Q1: 这里证明3X2=6,如果构造时候输入为4X3=12，verifier验证的等式同样成立。如何校验输入的正确性？ A1: 这里verifier可以检查 gL(s) = gala(s) · gdld(s) = gala(s) + dld(s)，即可判断输入参数是否相符。verifie是否知道a,d的取值？ Q2: 提供的是l(x)的阶数为多阶的，verifier提供是以{ gsi&lt;/sup&gt; }i∈[d]的方式提供的，在Proving阶段prover赋值时，如何保证对于每一阶都赋予相同的值？A2: 12345As a consequence the prover:• is not able to modify provided variable polynomials by changing their coefficients, except “assigning” values, because prover is presented only with encrypted evaluations of these polynomials, and because necessary encrypted powers of s are unavailable separately with their α-shifts• is not able to add another polynomial to the provided ones because the α-ratio will be broken• is not able to modify operand polynomials through multiplication by some other polyno- mial u(x), which could disproportionately modify the values because encrypted multipli- cation is not possible in pre-pairings space 4.7 Construction Properties1Note the operation’s construction is also called “constraint” because the operation represented by polynomial construction does not compute results per se, but rather checks that the prover already knows variables (including result), and they are valid for the operation, i.e., the prover is constrained to provide consistent values no matter what they are. 也就是说，验证的是输入/输出计算结果，即代入上面构造的L(x),R(x),O(x)，需要证明其中的根，即t(x)。通过验证约束的方式取代直接提供输出结果的方式。 4.8 Example Computationl(x),r(x),o(x)系列子函数的这些都是已知的，也是verifier加密后传递到prover。prover使用输入w=1,a=3,b=2,计算出结果v=6，并且代入计算得到L(x),R(x),O(x),然后即传统的p(x)=L(x)·R(x)-O(x)，其中t(x)已知，因为这个是verifier用来计算l(x),r(x),o(x)的根。也就是，需要证明的是v=6。这个是通过将v=6代入不等式证明的。 12m=a·bw·(m-a-b)=v-a-b 即图中在x=1,2,3三个点处都满足L(x)·R(x)-O(x)=0，如果改变v的值使其不等于6，则L(x)·R(x)-O(x)该等式在x=2处w·（m-a-b）!= v-a-b，即x=2不是其中一个根，verifier验证失败。 4.9 Verifiable Computation Protocol4.9.1 - 限定L(x)只由 li(x) 构成，并且证明的等式必须是L(x) · R（x）= O(x)。 防止左操作数，右操作数，输出在证明中混用。Solution: l(x),r(x),o(x)分别使用不同的αl, αr, αo。 4.9.2 - 限定在L(x),R(x),O(x)使用的参数vi(即4.8例子中的a,b,w等）是一致的。Solution: 为校验校验 vl,i = vr,i = vo,i = vβ,i，关键是通过以下等式e(gvl,i ·li(s) · gvr,i ·ri(s) · gvo,i ·oi(s), gβ) = e(gvβ,i ·β·(li(s)+ri(s)+oi(s)), g)。 进一步为防止如l(x) = r(x)的情况，选择各自不等的βl, βr, βo Setup proving key the variable consistency polynomials:gβz={gβlli(s)+&gt;βrri(s)+&gt;βooi(s)}i∈{1,…,n} verification key: (gβl , gβr , gβo, gβz) Proving 赋值计算，gzi(s)=(gβz)vi for i ∈ {1,…,n}，并且相乘得到proof: gZ(s) Verification e(gL, gβl) · e(gR, gβr) · e(gO, gβo) = e(gZ , g) 4.9.3 Non-malleability of Variable and Variable Consistency PolynomialsQ1:（Malleability of Variable Polynomials） gαl 和 gβl是在verification key里的，为什么prover能拿得到？Q2：（Non-Malleability） 作为verification key，这里的gγ是否就是类似与Q1里的g ? 这样的区别在哪里？ 4.10 Constraints1However, the protocol is not actually “computing” but rather is checking that the output value is the correct result of an operation for the operand’s values. That is why it is called a constraint, i.e., a verifier is constraining a prover to provide valid values for the predefined “program” no matter what are they. 4.11 Public Inputs and One 限定常量的值。Solution:L(x)=Lv(x)+Lp(x)，其中verifier提供Lv(x) = l0(x) + l1(x) + . . . + lm(x)。限定了0阶（即常量）的值vone Setup . . . separate all n variable polynomials into two groups: verifier’s m + 1:Lv(x) = l0(x) + l1(x) + . . . + lm, and alike for Rv(x) and Ov(x), where index 0 is reserved for the value of vone = 1 prover’s n − m:Lp(x) = lm+1(x) + . . . + ln(x), and alike for Rp(x) and Op(x) 1Effectively this is taking some variables from the prover into the hands of verifier while still preserving the balance of the equation. Therefore the valid operations check should still hold, but only if the prover has used the same values that the verifier used for his input. 4.12 Zero-Knowledge Proof of Computation零知识证明，各自加上变换值。 δl, δr, δo。通过加法实现同态加密。L(s) + δlt(s)) · (R(s) + δrt(s)) − (O(s) + δot(s)) = t(s) · (∆ + h(s))∆ = δrL(s) + δlR(s) + δlδrt(s) − δo 只要满足以上关系，可以推导，在verification阶段可以就可以验证L · R − O + t · ∆ = t(s)h + t(s) · ∆ Ref: 从零开始学习 zk-SNARK : 来自 安比实验室，翻译原文，并且加入理解和拓展，推荐 零知识证明学习资源汇总]]></content>
      <categories>
        <category>Consensus &amp; Cryptography</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[大数据与区块链]]></title>
    <url>%2F2020%2F07%2F25%2Fbigdata-and-blockchain%2F</url>
    <content type="text"><![CDATA[Post is cheap, show me the password Incorrect Password! No content to display! U2FsdGVkX1+4G0kOPxZsqFg6cRiXLbZY4545TBNBKYcLs8bzCpSZdwduYwHXweleACh1R6Un6v8yOcVfHQeatJj1EgxNRP94FQk1E79vtldwHwBEF052usY2k4IEgQMVrCP7RBgkmNEhMqwWCbNDIRHUcECa5My+m9yRwuFWCEvk/+lOtYREHeKEtUutR+hG64EvoGkblriT8yRD1glHC9E0lTx604n4TqdupxFtg72FfrmNMd2eZhNsp66AHl1PUygnoM6p1J8eLpsNOWZQOWg4USsAe6hrHKWYAADGvwGjtjIPBf3UITmiIymVjGV4hAsL+LC30J+iHTATINUlIvilvH1az/ehbrtHrzb6SkIlT5z5IdiOJkx1GH6NcAtgPsnc6fi8sJU8/3v0hKHm4VyhNPtI5Qw0ICXkNYCp3eef4jNZs7pCSu7zCuOw260o]]></content>
  </entry>
  <entry>
    <title><![CDATA[Fabric 2.0新特性]]></title>
    <url>%2F2020%2F04%2F02%2Ffabric-2-0-new-feature%2F</url>
    <content type="text"><![CDATA[Hyperledger Fabric升级到2.0大版本，主要以下有3个新特性。参看官网What’s new in Hyperledger Fabric v2.0。同时可以观看IBM员工也是Fabric开发者的系列视频讲座介绍新特性。参看超级账本Hyperledger频道（超级账本网络研讨会2020年系列）。总体来说，2.0版本引入的新特性都是在实际应用过程中的痛点，增进了可用性。 Decentralized governance for smart contracts 视频Fabric chaincode lifecycleDeploying a smart contract to a channel Private data enhancements 视频 External chaincode launcher 视频Chaincode as an external service]]></content>
      <categories>
        <category>Hyperledger Fabric</category>
      </categories>
      <tags>
        <tag>Hyperledger Fabric</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Bitcoin概念理解]]></title>
    <url>%2F2019%2F11%2F14%2Funderstanding-bitcoin%2F</url>
    <content type="text"><![CDATA[锁定脚本与解锁脚本两者用于确定UTXO的所有权，其中，锁定脚本（sciptPubKey）存在于transaction的output，解锁脚本（scriptSig）存在于transaction的input。 具体规则如下： 交易验证过程： 以上摘自比特币的交易过程-知乎 解锁脚本里签名的内容为： We define an electronic coin as a chain of digital signatures. Each owner transfers the coin to the next by digitally signing a hash of the previous transaction and the public key of the next owner and adding these to the end of the coin. A payee can verify the signatures to verify the chain of ownership. 摘自Bitcoin Whitepaper 总的来说，校验过程先校验解锁脚本里公钥PubKey和锁定脚本里是吻合的，然后校验解锁脚本里的签名。使用对应的私钥，对来源（产生消费的该UTXO的transaction）和去处（转账接受者的pubkey）进行签名。校验过程只需要校验（签名，该UTXO的公钥，签名内容）这三者是吻合的。]]></content>
      <categories>
        <category>Bitcoin</category>
      </categories>
      <tags>
        <tag>Bitcoin</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fabric实战（2）- Commercial Paper的Fabric-Java-SDK客户端]]></title>
    <url>%2F2019%2F11%2F02%2Ffabric-inpractice-commercialpaper-java-sdk%2F</url>
    <content type="text"><![CDATA[到目前为止都是通过在cli容器执行peer chaincode命令调用chaincode，实际上官方提供SDK（当前只有Node.js和Java两种）。这里提供基于Fabric Java SDK的commercial paper客户端，设置使用TLS，详细项目代码在github。项目采用Springboot，swagger提供网页调用。注意，这里只是demo，不能作为生产环境使用。具体步骤大致拆解如下： 构建HFClient12345678910111213141516171819202122232425262728293031323334353637383940public HFClient getObject() throws Exception &#123; // 使用Org1的admin用户身份。由于使用TSL，这里提供keyFile：admin的私钥和certFile：由ca.org1.example.com签发的证书 // 也可以通过使用HFCAClient向ca发送enroll和register请求用于新增注册和登记身份证书。具体可以参看参考文档。 Enrollment enrollment = PaperUser.createEnrollmentFromPemFile(keyFile, certFile); PaperUser admin = new PaperUser(name, mspId, enrollment); HFClient client = HFClient.createNewInstance(); client.setCryptoSuite(CryptoSuite.Factory.getCryptoSuite()); client.setUserContext(admin); initChannel(); return client;&#125;protected static Enrollment createEnrollmentFromPemFile(String keyFile, String certFile) throws Exception&#123; byte[] keyPem = Files.readAllBytes(Paths.get(keyFile)); //载入私钥PEM文本 byte[] certPem = Files.readAllBytes(Paths.get(certFile)); //载入证书PEM文本 CryptoPrimitives suite = new CryptoPrimitives(); //载入密码学套件 PrivateKey privateKey = suite.bytesToPrivateKey(keyPem); //将PEM文本转换为私钥对象 return new X509Enrollment(privateKey,new String(certPem)); //创建并返回X509Enrollment对象&#125;// 使用TSL，需要提供peerTLScaCertFile：tlsca.org1.example.com的证书和ordererTLScaCertFile：tlsca.example.com的证书private void initChannel() throws Exception &#123; Channel channel = client.newChannel(channelID); Properties peerProps = new Properties(); peerProps.put("pemFile", peerTLScaCertFile); peerProps.setProperty("sslProvider", "openSSL"); peerProps.setProperty("negotiationType", "TLS"); Peer peer = client.newPeer(peerName, peerURL, peerProps); channel.addPeer(peer); Properties ordererProps = new Properties(); ordererProps.put("pemFile", ordererTLScaCertFile); ordererProps.setProperty("sslProvider", "openSSL"); ordererProps.setProperty("negotiationType", "TLS"); Orderer orderer = client.newOrderer(ordererName, ordererURL, ordererProps); channel.addOrderer(orderer); channel.initialize();&#125; peerURL设置为grpcs://peer0.org1.example.com:7051。首先，TLS需要设置使用grpcs。其次，只能使用peer0.org1.example.com（同时需要配置hosts文件），不能使用 127.0.0.1或者localhost(报错Caused by: java.security.cert.CertificateException: No subject alternative DNS name matching IP address 127.0.0.1/localhost found) 。因为peer0返回的证书的CN是peer0.org1.example.com，java ssl会校验这个域名。 chaincode - query 12345678910111213141516171819202122public Result query(String issuer, String paperNumber) &#123; QueryByChaincodeRequest req = client.newQueryProposalRequest(); ChaincodeID cid = ChaincodeID.newBuilder().setName(chaincodeName).build(); req.setChaincodeID(cid); req.setFcn(QUERY_FUNC); req.setArgs(issuer, paperNumber); // Channel对象已在上面构建 Channel channel = client.getChannel(channelID); Collection&lt;ProposalResponse&gt; propResps = channel.queryByChaincode(req); // endorser返回的ProposalResponse需要保持一致 Collection&lt;Set&lt;ProposalResponse&gt;&gt; proposalConsistencySets = SDKUtils.getProposalConsistencySets(propResps); if (proposalConsistencySets.size() != 1) &#123; return Result.fail(ErrorCode.CHAINCOED_SERVICE_ERROR) .withErrorMsg("Expected only one set of consistent proposal responses but got more"); &#125; FabricProposalResponse.Response res = propResps.iterator().next().getProposalResponse().getResponse(); if (res.getStatus() == 200) &#123; return Result.success().withResponse(res.getPayload().toStringUtf8()); &#125; return Result.fail(ErrorCode.CHAINCOED_SERVICE_ERROR).withErrorMsg(res.getMessage());&#125; chaincode - invoke 123456789101112131415161718192021222324252627public Result issue(IssueRequest request) &#123; TransactionProposalRequest proposalRequest = client.newTransactionProposalRequest(); ChaincodeID cid = ChaincodeID.newBuilder().setName(chaincodeName).build(); proposalRequest.setChaincodeID(cid); proposalRequest.setFcn(ISSUE_FUNC); proposalRequest.setArgs(request.getIssuer(), request.getPaperNumber(), issueDate, maturityDate, String.valueOf(request.getFaceValue())); Channel channel = client.getChannel(channelID); // Endorsing(simulate) phase Collection&lt;ProposalResponse&gt; propResps = channel.sendTransactionProposal(proposalRequest); // endorser返回的ProposalResponse需要保持一致 Collection&lt;Set&lt;ProposalResponse&gt;&gt; proposalConsistencySets = SDKUtils.getProposalConsistencySets(propResps); if (proposalConsistencySets.size() != 1) &#123; return Result.fail(ErrorCode.CHAINCOED_SERVICE_ERROR) .withErrorMsg("Expected only one set of consistent proposal responses but got more"); &#125; // Orderer phase // 这里同步调用，需要等到提交orderer并且orderer排序后发送到各个peer，peer处理了block里的交易后才返回。 // 或者同步返回txid。可以配置eventHub异步接收和执行block，通过txid来返回最终的执行结果 BlockEvent.TransactionEvent event = channel.sendTransaction(propResps).get(); if (event.isValid()) &#123; return Result.success().withResponse("txid : " + event.getTransactionID()); &#125; return Result.fail(ErrorCode.CHAINCOED_SERVICE_ERROR);&#125; WIP: eventHub Ref. Fabric Java SDK最新教程【201904】]]></content>
      <categories>
        <category>Hyperledger Fabric</category>
      </categories>
      <tags>
        <tag>Hyperledger Fabric</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fabric实战（1） - 基础篇，基于byfn的Commercial Paper案例]]></title>
    <url>%2F2019%2F10%2F23%2Ffabric-inpractice-commercialpaper-byfn%2F</url>
    <content type="text"><![CDATA[之前分享过Fabric官网Commercial Paper案例chaincode的golang实现，现在基于官网Building Your First Network再次记录过程，同时添加部分新操作及分析，以此作为以后的进阶实战基础。 使用命令./byfn.sh up启动网络 查看配置文件/fabric-samples/first-network/docker-compose-cli.yaml,可见cli的挂载volume设置 123cli: volume: - ./../chaincode/:/opt/gopath/src/github.com/chaincode 将编写的chaincode源文件放置在该挂载的路径/chaincode/go下 12cp commercial_paper.go /chaincode/gocp paper.go /chaincode/go docker exec -it cli bash登入容器cli 12345678- 执行peer chaincode install -n commercialpaper -v 0 -p github.com/chaincode/commercial_paper/go- 执行echo $CORE_PEER_MSPCONFIGPATH。可见当前角色为admin --- /opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp- 执行peer chaincode list -C mychannel --installed --- Get installed chaincodes on peer: Name: commercialpaper, Version: 0, Path: github.com/chaincode/commercial_paper/go, Id: f033307bf72f876fb5a95883d53ed5209e6639386e21361eed65f459d8a9f276 登入peer0.org1.example.com(容器cli内CORE_PEER_ADDRESS=peer0.org1.example.com:7051，也就是其endorser节点) 1234- 执行ll var/hyperledger/production/chaincodes，存在文件commercialpaper.0（即$&#123;chaincodeName&#125;.$&#123;version&#125;）- 执行peer chaincode list -C mychannel --installed --- Error: Bad response: 500 - access denied for [getinstalledchaincodes]: Failed verifying that proposal&apos;s creator satisfies local MSP principal during channelless check policy with policy [Admins]: [This identity is not an admin] 实际上，docker inspect peer0.org1.example.com： 12345678&quot;HostConfig&quot;: &#123; &quot;Binds&quot;: [ &quot;/Users/meitu/Project/github/fabric-samples/first-network/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls:/etc/hyperledger/fabric/tls:rw&quot;, &quot;/Users/meitu/Project/github/fabric-samples/first-network/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/msp:/etc/hyperledger/fabric/msp:rw&quot;, &quot;/var/run:/host/var/run:rw&quot;, &quot;net_peer0.org1.example.com:/var/hyperledger/production:rw&quot; ],&#125; 没有将Admin@org1.example.com/msp挂载进来，当然可以修改启动配置文件base/docker-compose-base.yaml，或者后续添加到挂载的volume上，设置$CORE_PEER_MSPCONFIGPATH指向挂载的路径也可以赋予权限访问 查看/fabric-samples/first-network/scripts/路径下script.sh和util.sh，包含设置CORE_PEER_LOCALMSPID=(“Org1MSP”/“Org2MSP”) cli执行chaincode instantiate 1234567891011121314151617181920212223242526272829peer chaincode instantiate -n commercialpaper -v 0 -c &apos;&#123;&quot;Args&quot;:[]&#125;&apos; -C mychannel -P &quot;AND (&apos;Org1MSP.member&apos;)&quot; \--tls true --cafile /opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pemenv---OLDPWD=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacertspeer0.org1.example.com:2019-10-22 14:28:19.931 UTC [endorser] callChaincode -&gt; INFO 1e62ef [mychannel][3156a1b7] Entry chaincode: name:&quot;lscc&quot;2019-10-22 14:28:19.936 UTC [endorser] callChaincode -&gt; INFO 1e630e [mychannel][3156a1b7] Exit chaincode: name:&quot;lscc&quot; (5ms)2019-10-22 14:28:19.940 UTC [comm.grpc.server] 1 -&gt; INFO 1e631b unary call completed &#123;&quot;grpc.start_time&quot;: &quot;2019-10-22T14:28:19.927Z&quot;, &quot;grpc.service&quot;: &quot;protos.Endorser&quot;, &quot;grpc.method&quot;: &quot;ProcessProposal&quot;, &quot;grpc.peer_address&quot;: &quot;172.25.0.7:51012&quot;, &quot;grpc.code&quot;: &quot;OK&quot;, &quot;grpc.call_duration&quot;: &quot;12.4301ms&quot;&#125;2019-10-22 14:28:21.993 UTC [gossip.privdata] StoreBlock -&gt; INFO 1e647d [mychannel] Received block [5] from buffer2019-10-22 14:28:22.022 UTC [committer.txvalidator] Validate -&gt; INFO 1e64c9 [mychannel] Validated block [5] in 28ms2019-10-22 14:28:22.033 UTC [cceventmgmt] HandleStateUpdates -&gt; INFO 1e64db Channel [mychannel]: Handling deploy or update of chaincode [commercialpaper]2019-10-22 14:28:22.065 UTC [kvledger] CommitWithPvtData -&gt; INFO 1e6506 [mychannel] Committed block [5] with 1 transaction(s) in 40ms (state_validation=15ms block_commit=18ms state_commit=4ms)2019-10-22 14:28:31.257 UTC [endorser] callChaincode -&gt; INFO 1e6ed6 [mychannel][b206b359] Entry chaincode: name:&quot;lscc&quot;2019-10-22 14:28:31.262 UTC [endorser] callChaincode -&gt; INFO 1e6efc [mychannel][b206b359] Exit chaincode: name:&quot;lscc&quot; (4ms)2019-10-22 14:28:31.263 UTC [comm.grpc.server] 1 -&gt; INFO 1e6f09 unary call completed &#123;&quot;grpc.start_time&quot;: &quot;2019-10-22T14:28:31.255Z&quot;, &quot;grpc.service&quot;: &quot;protos.Endorser&quot;, &quot;grpc.method&quot;: &quot;ProcessProposal&quot;, &quot;grpc.peer_address&quot;: &quot;172.25.0.4:55986&quot;, &quot;grpc.code&quot;: &quot;OK&quot;, &quot;grpc.call_duration&quot;: &quot;7.5554ms&quot;&#125;peer1.org1.example.com:2019-10-22 14:28:22.009 UTC [gossip.privdata] StoreBlock -&gt; INFO 1e0540 [mychannel] Received block [5] from buffer2019-10-22 14:28:22.011 UTC [committer.txvalidator] Validate -&gt; INFO 1e0588 [mychannel] Validated block [5] in 2ms2019-10-22 14:28:22.017 UTC [cceventmgmt] HandleStateUpdates -&gt; INFO 1e05a0 Channel [mychannel]: Handling deploy or update of chaincode [commercialpaper]2019-10-22 14:28:22.018 UTC [ccprovider] ExtractStatedbArtifactsForChaincode -&gt; INFO 1e05a3 Error while loading installation package for ccname=commercialpaper, ccversion=0. Err=open /var/hyperledger/production/chaincodes/commercialpaper.0: no such file or directory2019-10-22 14:28:22.018 UTC [cceventmgmt] HandleChaincodeDeploy -&gt; INFO 1e05a4 Channel [mychannel]: Chaincode [Name=commercialpaper, Version=0, Hash=[]byte&#123;0xf0, 0x33, 0x30, 0x7b, 0xf7, 0x2f, 0x87, 0x6f, 0xb5, 0xa9, 0x58, 0x83, 0xd5, 0x3e, 0xd5, 0x20, 0x9e, 0x66, 0x39, 0x38, 0x6e, 0x21, 0x36, 0x1e, 0xed, 0x65, 0xf4, 0x59, 0xd8, 0xa9, 0xf2, 0x76&#125;] is not installed hence no need to create chaincode artifacts for endorsement2019-10-22 14:28:22.041 UTC [kvledger] CommitWithPvtData -&gt; INFO 1e05c1 [mychannel] Committed block [5] with 1 transaction(s) in 27ms (state_validation=5ms block_commit=13ms state_commit=6ms)orderer：2019-10-22 14:28:19.969 UTC [comm.grpc.server] 1 -&gt; INFO 027 streaming call completed &#123;&quot;grpc.start_time&quot;: &quot;2019-10-22T14:28:19.947Z&quot;, &quot;grpc.service&quot;: &quot;orderer.AtomicBroadcast&quot;, &quot;grpc.method&quot;: &quot;Broadcast&quot;, &quot;grpc.peer_address&quot;: &quot;172.25.0.7:53034&quot;, &quot;grpc.code&quot;: &quot;OK&quot;, &quot;grpc.call_duration&quot;: &quot;23.1002ms&quot;&#125; peer1.org1.example.com上没有install这个chaincode cli执行peer chaincode invoke -c &#39;{&quot;Args&quot;:[&quot;issue&quot;,&quot;MagnetoCorp&quot;, &quot;00001&quot;, &quot;2020-05-31&quot;, &quot;2020-11-30&quot;, &quot;5000000&quot;]}&#39; -C mychannel -n commercialpaper&quot;，失败。看日志是cli与orderer之间tls失败 1234567891011cli:Error: error sending transaction for invoke: could not send: EOF - proposal response: version:1 response:&lt;status:200 payload:&quot;&#123;\&quot;PaperNumber\&quot;:\&quot;00004\&quot;,\&quot;Issuer\&quot;:\&quot;MagnetoCorp\&quot;,\&quot;Owner\&quot;:\&quot;MagnetoCorp\&quot;,\&quot;IssueDateTime\&quot;:\&quot;2020-05-31\&quot;,\&quot;MaturityDateTime\&quot;:\&quot;2020-11-30\&quot;,\&quot;FaceValue\&quot;:5000000,\&quot;Status\&quot;:0&#125;&quot; &gt; payload:&quot;\n /!\373\020OI\024\270_1\267\251\332\337\037 $\370\347\356\004aEXZ\345\332/\366|&apos;7\022\306\003\n\204\002\022\340\001\n\017commercialpaper\022\314\001\n\022\n\020MagnetoCorp00004\032\265\001\n\020MagnetoCorp00004\032\240\001&#123;\&quot;PaperNumber\&quot;:\&quot;00004\&quot;,\&quot;Issuer\&quot;:\&quot;MagnetoCorp\&quot;,\&quot;Owner\&quot;:\&quot;MagnetoCorp\&quot;,\&quot;IssueDateTime\&quot;:\&quot;2020-05-31\&quot;,\&quot;MaturityDateTime\&quot;:\&quot;2020-11-30\&quot;,\&quot;FaceValue\&quot;:5000000,\&quot;Status\&quot;:0&#125;\022\037\n\004lscc\022\027\n\025\n\017commercialpaper\022\002\010\005\032\246\001\010\310\001\032\240\001&#123;\&quot;PaperNumber\&quot;:\&quot;00004\&quot;,\&quot;Issuer\&quot;:\&quot;MagnetoCorp\&quot;,\&quot;Owner\&quot;:\&quot;MagnetoCorp\&quot;,\&quot;IssueDateTime\&quot;:\&quot;2020-05-31\&quot;,\&quot;MaturityDateTime\&quot;:\&quot;2020-11-30\&quot;,\&quot;FaceValue\&quot;:5000000,\&quot;Status\&quot;:0&#125;\&quot;\024\022\017commercialpaper\032\0010&quot; endorsement:&lt;endorser:&quot;\n\007Org1MSP\022\252\006-----BEGIN CERTIFICATE-----\nMIICKDCCAc6gAwIBAgIQbaczcznil/PbPlby6iZKsjAKBggqhkjOPQQDAjBzMQsw\nCQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEWMBQGA1UEBxMNU2FuIEZy\nYW5jaXNjbzEZMBcGA1UEChMQb3JnMS5leGFtcGxlLmNvbTEcMBoGA1UEAxMTY2Eu\nb3JnMS5leGFtcGxlLmNvbTAeFw0xOTEwMjIwODU0MDBaFw0yOTEwMTkwODU0MDBa\nMGoxCzAJBgNVBAYTAlVTMRMwEQYDVQQIEwpDYWxpZm9ybmlhMRYwFAYDVQQHEw1T\nYW4gRnJhbmNpc2NvMQ0wCwYDVQQLEwRwZWVyMR8wHQYDVQQDExZwZWVyMC5vcmcx\nLmV4YW1wbGUuY29tMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAESYh6SwhdBrUf\nsekjyFVukFQXI4q5O4DCnaTwARTlHffruISr0wnM1UkCI/OlrApnr/1Y3jagLiko\nkgwZbeVrgqNNMEswDgYDVR0PAQH/BAQDAgeAMAwGA1UdEwEB/wQCMAAwKwYDVR0j\nBCQwIoAgnPSDYZGp0kd3Lamn88Z/R0vxSeMxwJp/FKSVeVOsmsMwCgYIKoZIzj0E\nAwIDSAAwRQIhAO3ASI59+lSdlcY9RgDUlRWMe3qzXwI870wWI+ozpscLAiBf4MIV\nUcGoLpoB5GPnRxOY++CabOCmYDBZMfYAE+SKlA==\n-----END CERTIFICATE-----\n&quot; signature:&quot;0E\002!\000\257D\303\234\024\210C&gt;\203\215-\204\257 \304*\367sL9\223\207\233\312\351\277\270LR\231\236&amp;\002 )\331\334\006\025\214\337\355\262\252\305\345\214\315ae\3527\317y\274\336\352oX\037F\220\3036%\025&quot; &gt;orderer: 2019-10-22 15:58:14.380 UTC [core.comm] ServerHandshake -&gt; ERRO 034 TLS handshake failed with error tls: first record does not look like a TLS handshake &#123;&quot;server&quot;: &quot;Orderer&quot;, &quot;remote address&quot;: &quot;172.25.0.7:53104&quot;&#125;2019-10-22 15:58:15.383 UTC [core.comm] ServerHandshake -&gt; ERRO 035 TLS handshake failed with error tls: first record does not look like a TLS handshake &#123;&quot;server&quot;: &quot;Orderer&quot;, &quot;remote address&quot;: &quot;172.25.0.7:53106&quot;&#125;peer0.org1.example.com:2019-10-22 15:58:15.391 UTC [endorser] callChaincode -&gt; INFO 2c3af6 [mychannel][89d96407] Entry chaincode: name:&quot;commercialpaper&quot;2019-10-22 15:58:15.398 UTC [endorser] callChaincode -&gt; INFO 2c3b04 [mychannel][89d96407] Exit chaincode: name:&quot;commercialpaper&quot; (7ms)2019-10-22 15:58:15.398 UTC [comm.grpc.server] 1 -&gt; INFO 2c3b11 unary call completed &#123;&quot;grpc.start_time&quot;: &quot;2019-10-22T15:58:15.389Z&quot;, &quot;grpc.service&quot;: &quot;protos.Endorser&quot;, &quot;grpc.method&quot;: &quot;ProcessProposal&quot;, &quot;grpc.peer_address&quot;: &quot;172.25.0.7:51082&quot;, &quot;grpc.code&quot;: &quot;OK&quot;, &quot;grpc.call_duration&quot;: &quot;9.4211ms&quot;&#125; cli执行chaincode invoke 1234567891011121314151617181920peer chaincode invoke -c &apos;&#123;&quot;Args&quot;:[&quot;issue&quot;,&quot;MagnetoCorp&quot;, &quot;00001&quot;, &quot;2020-05-31&quot;, &quot;2020-11-30&quot;, &quot;5000000&quot;]&#125;&apos; -C mychannel -n commercialpaper \--tls true --cafile /opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pemcli:2019-10-22 15:55:07.564 UTC [chaincodeCmd] chaincodeInvokeOrQuery -&gt; INFO 0c9 Chaincode invoke successful. result: status:200 payload:&quot;&#123;\&quot;PaperNumber\&quot;:\&quot;00003\&quot;,\&quot;Issuer\&quot;:\&quot;MagnetoCorp\&quot;,\&quot;Owner\&quot;:\&quot;MagnetoCorp\&quot;,\&quot;IssueDateTime\&quot;:\&quot;2020-05-31\&quot;,\&quot;MaturityDateTime\&quot;:\&quot;2020-11-30\&quot;,\&quot;FaceValue\&quot;:5000000,\&quot;Status\&quot;:0&#125;&quot;peer0.org1.example.com:2019-10-22 15:55:07.497 UTC [endorser] callChaincode -&gt; INFO 2ba6d0 [][7b225797] Entry chaincode: name:&quot;cscc&quot;2019-10-22 15:55:07.502 UTC [endorser] callChaincode -&gt; INFO 2ba6ec [][7b225797] Exit chaincode: name:&quot;cscc&quot; (5ms)2019-10-22 15:55:07.502 UTC [comm.grpc.server] 1 -&gt; INFO 2ba6ef unary call completed &#123;&quot;grpc.start_time&quot;: &quot;2019-10-22T15:55:07.494Z&quot;, &quot;grpc.service&quot;: &quot;protos.Endorser&quot;, &quot;grpc.method&quot;: &quot;ProcessProposal&quot;, &quot;grpc.peer_address&quot;: &quot;172.25.0.7:51076&quot;, &quot;grpc.code&quot;: &quot;OK&quot;, &quot;grpc.call_duration&quot;: &quot;7.6522ms&quot;&#125;2019-10-22 15:55:07.553 UTC [endorser] callChaincode -&gt; INFO 2ba716 [mychannel][443bff3e] Entry chaincode: name:&quot;commercialpaper&quot;2019-10-22 15:55:07.557 UTC [endorser] callChaincode -&gt; INFO 2ba724 [mychannel][443bff3e] Exit chaincode: name:&quot;commercialpaper&quot; (4ms)2019-10-22 15:55:07.559 UTC [comm.grpc.server] 1 -&gt; INFO 2ba731 unary call completed &#123;&quot;grpc.start_time&quot;: &quot;2019-10-22T15:55:07.545Z&quot;, &quot;grpc.service&quot;: &quot;protos.Endorser&quot;, &quot;grpc.method&quot;: &quot;ProcessProposal&quot;, &quot;grpc.peer_address&quot;: &quot;172.25.0.7:51076&quot;, &quot;grpc.code&quot;: &quot;OK&quot;, &quot;grpc.call_duration&quot;: &quot;14.7305ms&quot;&#125;2019-10-22 15:55:09.574 UTC [gossip.privdata] StoreBlock -&gt; INFO 2ba941 [mychannel] Received block [8] from buffer2019-10-22 15:55:09.584 UTC [committer.txvalidator] Validate -&gt; INFO 2ba96b [mychannel] Validated block [8] in 9ms2019-10-22 15:55:09.619 UTC [kvledger] CommitWithPvtData -&gt; INFO 2ba99e [mychannel] Committed block [8] with 1 transaction(s) in 33ms (state_validation=19ms block_commit=7ms state_commit=5ms)orderer:2019-10-22 15:55:07.569 UTC [orderer.common.broadcast] Handle -&gt; WARN 032 Error reading from 172.25.0.7:53098: rpc error: code = Canceled desc = context canceled2019-10-22 15:55:07.569 UTC [comm.grpc.server] 1 -&gt; INFO 033 streaming call completed &#123;&quot;grpc.start_time&quot;: &quot;2019-10-22T15:55:07.545Z&quot;, &quot;grpc.service&quot;: &quot;orderer.AtomicBroadcast&quot;, &quot;grpc.method&quot;: &quot;Broadcast&quot;, &quot;grpc.peer_address&quot;: &quot;172.25.0.7:53098&quot;, &quot;error&quot;: &quot;rpc error: code = Canceled desc = context canceled&quot;, &quot;grpc.code&quot;: &quot;Canceled&quot;, &quot;grpc.call_duration&quot;: &quot;24.1982ms&quot;&#125; 从peer0.org1.example.com日志可见，先后调用了cscc（system chaincode。 调用GetConfigBlock方法，获取channel config，从而获取orderer地址提供给cli建立orderer client）和commercialpaper。后续收到orderer传来的block后再验证和提交本地。 docker network inspect net_byfn,发现 172.25.0.7 &lt;-&gt; cli登入cli，netstat -anp如下。估计是端口过早关闭 123Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program nametcp 0 0 127.0.0.11:35373 0.0.0.0:* LISTEN -udp 0 0 127.0.0.11:36128 0.0.0.0:* peer0.org1.example.com/cli执行 1234peer chaincode list -C mychannel --instantiated---Get instantiated chaincodes on channel mychannel:Name: commercialpaper, Version: 0, Path: github.com/chaincode/commercial_paper/go, Escc: escc, Vscc: vscc cli执行chaincode query 1peer chaincode query -c &apos;&#123;&quot;Args&quot;:[&quot;query&quot;,&quot;MagnetoCorp&quot;, &quot;00001&quot;]&#125;&apos; -C mychannel -n commercialpaper 当前commeicialpaper只install在peer0.org1.example.com，此时cli执行 1234CORE_PEER_ADDRESS=peer1.org1.example.com:8051peer chaincode query -c &apos;&#123;&quot;Args&quot;:[&quot;query&quot;,&quot;MagnetoCorp&quot;, &quot;00001&quot;]&#125;&apos; -C mychannel -n commercialpaper---Error: endorsement failure during query. response: status:500 message:&quot;cannot retrieve package for chaincode commercialpaper/0, error open /var/hyperledger/production/chaincodes/commercialpaper.0: no such file or directory&quot; cli在peer1.org1.example.com:8051上chaincode install，并且chaincode query 1234567891011121314151617181920212223peer chaincode query -c &apos;&#123;&quot;Args&quot;:[&quot;query&quot;,&quot;MagnetoCorp&quot;, &quot;00001&quot;]&#125;&apos; -C mychannel -n commercialpaper \--peerAddresses peer1.org1.example.com:8051 \--tlsRootCertFiles /opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer1.org1.example.com/tls/server.crt或者（`CORE_PEER_TLS_ROOTCERT_FILE`可不修改，因为同org相同的ca证书）CORE_PEER_ADDRESS=peer1.org1.example.com:8051CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer1.org1.example.com/tls/ca.crtpeer chaincode install -n commercialpaper -v 0 -p github.com/chaincode/commercial_paper/gopeer chaincode query -c &apos;&#123;&quot;Args&quot;:[&quot;query&quot;,&quot;MagnetoCorp&quot;, &quot;00001&quot;]&#125;&apos; -C mychannel -n commercialpaper---&#123;&quot;PaperNumber&quot;:&quot;00001&quot;,&quot;Issuer&quot;:&quot;MagnetoCorp&quot;,&quot;Owner&quot;:&quot;MagnetoCorp&quot;,&quot;IssueDateTime&quot;:&quot;2020-05-31&quot;,&quot;MaturityDateTime&quot;:&quot;2020-11-30&quot;,&quot;FaceValue&quot;:5000000,&quot;Status&quot;:0&#125;peer1.org1.example.com:2019-10-23 04:43:39.847 UTC [endorser] callChaincode -&gt; INFO 5078dd [][e21cf78b] Entry chaincode: name:&quot;lscc&quot;2019-10-23 04:43:39.891 UTC [lscc] executeInstall -&gt; INFO 5078ea Installed Chaincode [commercialpaper] Version [0] to peer2019-10-23 04:43:39.892 UTC [endorser] callChaincode -&gt; INFO 5078ee [][e21cf78b] Exit chaincode: name:&quot;lscc&quot; (45ms)2019-10-23 04:43:39.892 UTC [comm.grpc.server] 1 -&gt; INFO 5078f1 unary call completed &#123;&quot;grpc.start_time&quot;: &quot;2019-10-23T04:43:39.846Z&quot;, &quot;grpc.service&quot;: &quot;protos.Endorser&quot;, &quot;grpc.method&quot;: &quot;ProcessProposal&quot;, &quot;grpc.peer_address&quot;: &quot;172.25.0.7:50220&quot;, &quot;grpc.code&quot;: &quot;OK&quot;, &quot;grpc.call_duration&quot;: &quot;46.0634ms&quot;&#125;2019-10-23 04:43:54.861 UTC [endorser] callChaincode -&gt; INFO 5084f7 [mychannel][b060b92a] Entry chaincode: name:&quot;commercialpaper&quot;2019-10-23 04:43:54.914 UTC [chaincode.platform.golang] GenerateDockerBuild -&gt; INFO 508508 building chaincode with ldflagsOpt: &apos;-ldflags &quot;-linkmode external -extldflags &apos;-static&apos;&quot;&apos;2019-10-23 04:44:19.578 UTC [endorser] callChaincode -&gt; INFO 509732 [mychannel][b060b92a] Exit chaincode: name:&quot;commercialpaper&quot; (24751ms)2019-10-23 04:44:19.581 UTC [comm.grpc.server] 1 -&gt; INFO 50973f unary call completed &#123;&quot;grpc.start_time&quot;: &quot;2019-10-23T04:43:54.857Z&quot;, &quot;grpc.service&quot;: &quot;protos.Endorser&quot;, &quot;grpc.method&quot;: &quot;ProcessProposal&quot;, &quot;grpc.peer_address&quot;: &quot;172.25.0.7:50224&quot;, &quot;grpc.code&quot;: &quot;OK&quot;, &quot;grpc.call_duration&quot;: &quot;24.7588062s&quot;&#125;orderer日志无变化 此时docker ps，新启动了容器dev-peer1.org1.example.com-commercialpaper-0。 Fabric Documents: If you want additional peers to interact with ledger, then you will need to join them to the channel, and install the same name, version and language of the chaincode source onto the appropriate peer’s filesystem. A chaincode container will be launched for each peer as soon as they try to interact with that specific chaincode. Again, be cognizant of the fact that the Node.js images will be slower to compile. 以下对peer0.org2.example.com节点重复实验（从属不同的org）。在cli容器install/invoke chaincode。12345678910peer chaincode install -n commercialpaper -v 0 -p github.com/chaincode/commercial_paper/go \ --peerAddresses peer0.org2.example.com:9051 \--tlsRootCertFiles /opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/peers/peer0.org2.example.com/tls/server.crt* 指定--peerAddresses和--tlsRootCertFiles参数。或者修改环境变量（对应关系）CORE_PEER_ADDRESS=peer0.org2.example.com:9051CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/peers/peer0.org2.example.com/tls/ca.crtpeer chaincode install -n commercialpaper -v 0 -p github.com/chaincode/commercial_paper/go---报错Error: error getting channel (mychannel) orderer endpoint: error endorsing GetConfigBlock: rpc error: code = Unknown desc = access denied: channel [] creator org [Org1MSP] 这是因为CORE_PEER_LOCALMSPID和CORE_PEER_MSPCONFIGPATH这两个参数的设置使当前cli的msp证书从属于Org1。修改使用Org2的msp以及Admin或者User角色即可12345CORE_PEER_LOCALMSPID=Org2MSPCORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/users/User1@org2.example.com/msppeer chaincode install -n commercialpaper -v 0 -p github.com/chaincode/commercial_paper/go \ --peerAddresses peer0.org2.example.com:9051 \--tlsRootCertFiles /opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/peers/peer0.org2.example.com/ Ref. 分步详解 Fabric 区块链网络的部署 : IBM员工，相当棒的系列实践文章]]></content>
      <categories>
        <category>Hyperledger Fabric</category>
      </categories>
      <tags>
        <tag>Hyperledger Fabric</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fabric 1.4 chaincode lifecycle(install & instantiate)]]></title>
    <url>%2F2019%2F10%2F19%2Ffabric-chaincode-lifecycle-uml%2F</url>
    <content type="text"><![CDATA[chaincode install &amp; instantiate的大致流程，其中install无需order流程。具体细节可参考之前的源码分析系列。]]></content>
      <categories>
        <category>Hyperledger Fabric</category>
      </categories>
      <tags>
        <tag>Hyperledger Fabric</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fabric 1.4源码分析 - orderer的raft实现]]></title>
    <url>%2F2019%2F09%2F29%2Ffabric-source-orderer-etcdraft%2F</url>
    <content type="text"><![CDATA[Raft可以说是Fabric 1.X系列的首个真正意义的共识算法。Fabric的实现主要涉及到三个类，chain.go &lt;-&gt; etcdraft/node.go &lt;-&gt; raft/node.go, 其中raft/node.go是etcd的开源包，chain.go是实现共识算法的主要类，etcdraft/node.go则是相当于适配模式下的适配器，用于连接两者，对实现屏蔽Raft的具体实现方案。 首先看chain.go的struct（略去部分field），包含etcdraft/node.go对象。调用chain.go#Start方法时（省略），内部调用了etcdraft/node.go#start方法。1234567891011121314151617181920212223242526272829303132333435363738394041type Chain struct &#123; rpc RPC raftID uint64 channelID string lastKnownLeader uint64 submitC chan *submit applyC chan apply observeC chan&lt;- raft.SoftState // Notifies external observer on leader change (passed in optionally as an argument for tests) snapC chan *raftpb.Snapshot // Signal to catch up with snapshot gcC chan *gc // Signal to take snapshot configInflight bool // this is true when there is config block or ConfChange in flight blockInflight int // number of in flight blocks // needed by snapshotting sizeLimit uint32 // SnapshotIntervalSize in bytes accDataSize uint32 // accumulative data size since last snapshot lastSnapBlockNum uint64 confState raftpb.ConfState // Etcdraft requires ConfState to be persisted within snapshot createPuller CreateBlockPuller // func used to create BlockPuller on demand // this is exported so that test can use `Node.Status()` to get raft node status. Node *node&#125;// Start instructs the orderer to begin serving the chain and keep it current.func (c *Chain) Start() &#123; c.Node.start(c.fresh, isJoin) // 响应c.gcC channel的信号，进行c.Node.takeSnapshot操作，并且将过期的可配置个数前的消息和snapshot清空 go c.gc() go c.serveRequest() // es := c.newEvictionSuspector() interval := DefaultLeaderlessCheckInterval c.periodicChecker.Run()&#125; 12345678type node struct &#123; chainID string storage *RaftStorage // raft的wal, ram等持久化或者暂存内存实现类 config *raft.Config rpc RPC // 负责节点间的grpc通信，管理与各个节点的grpc client/stream chain *Chain raft.Node // 开源库etcd的raft节点实现&#125; etcdraft/node.go#run是其主要逻辑。(以下截取展示部分逻辑)。实际上可以看到，开源库etcd的raft节点实现只管理raft相关的propose, commit, election等过程，而把其他的业务相关留给使用方，包括节点间通信等。123456789101112131415161718192021222324for &#123; //// n为来自开源库etcd的raft节点raft.Node，通知消息 case rd := &lt;-n.Ready(): // wal if err := n.storage.Store(rd.Entries, rd.HardState, rd.Snapshot); err != nil &#123; n.logger.Panicf("Failed to persist etcd/raft data: %s", err) &#125; // 落后和新加入节点需要同步的snapshot if !raft.IsEmptySnap(rd.Snapshot) &#123; n.chain.snapC &lt;- &amp;rd.Snapshot &#125; // skip empty apply。 来自raft节点的新消息（提议的提交信息rd.CommittedEntries，或者状态变换rd.SoftState，如新leader，节点数量变化等等） if len(rd.CommittedEntries) != 0 || rd.SoftState != nil &#123; n.chain.applyC &lt;- apply&#123;rd.CommittedEntries, rd.SoftState&#125; &#125; n.Advance() // TODO(jay_guo) leader can write to disk in parallel with replicating to the followers and them writing to their disks. Check 10.2.1 in thesis // 调用rpc RPC，交由管理的grpc client发送 n.send(rd.Messages) &#125; Orderer消息的入口还是chain.go#Order和chain.go#Configure，实际上最后调用chain.go#Submit，其如注释所说，如果本节点是leader则发送到submitC channel内，否则通过rpc发送到leader节点。1234567891011121314151617// Submit forwards the incoming request to:// - the local serveRequest goroutine if this is leader// - the actual leader via the transport mechanism// The call fails if there's no leader elected yet.func (c *Chain) Submit(req *orderer.SubmitRequest, sender uint64) error &#123; leadC := make(chan uint64, 1) select &#123; case c.submitC &lt;- &amp;submit&#123;req, leadC&#125;: lead := &lt;-leadC if lead != c.raftID &#123; if err := c.rpc.SendSubmit(lead, req); err != nil &#123; c.Metrics.ProposalFailures.Add(1) return err &#125; &#125; &#125;&#125; chain.go的运行主体在chain.go#serveRequest，其中主要是select。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596select &#123; // 来自于`chain.go#Submit`，也就是提交的proposal，这里主要是判断当前是leader才会进行propose。 // 如果当前节点是leader，则调用`consensus.ConsenterSupport#ProcessConfigMsg/ProcessNormalMsg`，然后调用`support.BlockCutter().Ordered`切割batch。 // 对切割后还有pending的消息启动timer，也就是下面的`&lt;-timer.C():`分支，到期后在进行切割。 case s := &lt;-submitC: // 与orderer的其他实现一致，可参考[Fabric 1.4源码分析 - chaincode instantiate(8）orderer的排序过程] batches, pending, err := c.ordered(s.req) if pending &#123; startTimer() // no-op if timer is already started &#125; else &#123; stopTimer() &#125; c.propose(propC, bc, batches...) // 来自上文提到的`etcdraft/node.go#run`，也就是开源库etcd的raft节点raft.Node的通知消息 // 这部分消息可能包含leader的切换，最新的记录在消息的`chain.go/apply/raft.SoftState/Lead`字段，进而相应的`propC, cancelProp = becomeLeader()`或者`becomeFollower()`。 // propC即上面的propose方法的参数，在becomeLeader内处理，即调用`raft.go#Node.Propose`进行propose case app := &lt;-c.applyC: // 这里的entries是CommittedEntries，即需要commit的entry，也就是leader当初propose的block // n.chain.applyC &lt;- apply&#123;rd.CommittedEntries, rd.SoftState&#125; c.apply(app.entries) case &lt;-timer.C(): // snapC chan *raftpb.Snapshot // Signal to catch up with snapshot // 来自于`etcdraft/node.go#run`, select-case的`rd := &lt;-n.Ready():`内`n.chain.snapC &lt;- &amp;rd.Snapshot`，即底层raft的需要同步的snapshot。 // 用于落后或者新加入的节点追上当前的消息状态 case sn := &lt;-c.snapC: c.catchUp(sn); err != nil case &lt;-c.doneC:&#125;func (c *Chain) propose(ch chan&lt;- *common.Block, bc *blockCreator, batches ...[]*common.Envelope) &#123; // 如果前面调用的c.ordered(s.req)切割出来的batches非空，则创建一个新block，并且在becomeLeader里的propC里调用c.Node.Propose() for _, batch := range batches &#123; b := bc.createNextBlock(batch) select &#123; case ch &lt;- b: default: &#125; // if it is config block, then we should wait for the commit of the block if utils.IsConfigBlock(b) &#123; c.configInflight = true &#125; c.blockInflight++ &#125; return&#125;becomeLeader := func() (chan&lt;- *common.Block, // Leader should call Propose in go routine, because this method may be blocked // if node is leaderless (this can happen when leader steps down in a heavily // loaded network). We need to make sure applyC can still be consumed properly. ctx, cancel := context.WithCancel(context.Background()) go func(ctx context.Context, ch &lt;-chan *common.Block) &#123; for &#123; select &#123; case b := &lt;-ch: data := utils.MarshalOrPanic(b) if err := c.Node.Propose(ctx, data); err != ni &#123;...&#125; &#125; &#125; &#125;(ctx, ch) return ch, cancel&#125;func (c *Chain) apply(ents []raftpb.Entry) &#123; var position int for i := range ents &#123; switch ents[i].Type &#123; // 调用writeBlock，将commitEntry里的block写入账本 // accDataSize是累计的block数据大小 case raftpb.EntryNormal: position = i c.accDataSize += uint32(len(ents[i].Data)) block := utils.UnmarshalBlockOrPanic(ents[i].Data) c.writeBlock(block, ents[i].Index) case raftpb.EntryConfChange: ... if ents[i].Index &gt; c.appliedIndex &#123; c.appliedIndex = ents[i].Index &#125; &#125; // accDataSize是累计的block数据大小，大于配置的sizeLimit后，写入c.gcC channel,在`chain.go#gc`内处理c.Node.takeSnapshot if c.accDataSize &gt;= c.sizeLimit &#123; b := utils.UnmarshalBlockOrPanic(ents[position].Data) select &#123; case c.gcC &lt;- &amp;gc&#123;index: c.appliedIndex, state: c.confState, data: ents[position].Data&#125;: c.accDataSize = 0 c.lastSnapBlockNum = b.Header.Number &#125;&#125; chain.go#gc12345678func (c *Chain) gc() &#123; for &#123; select &#123; case g := &lt;-c.gcC: c.Node.takeSnapshot(g.index, g.state, g.data) &#125; &#125;&#125; 总体架构流程如上，具体细节可以参考以下，这些参考比较详细都描述了实现细节。 Ref.Hyperledger-Fabric源码分析（orderer-consensus-etcdraft）Fabric raft 共识源码浅析]]></content>
      <categories>
        <category>Hyperledger Fabric</category>
      </categories>
      <tags>
        <tag>Hyperledger Fabric</tag>
        <tag>源码分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go语言穷人版渠道匹配demo]]></title>
    <url>%2F2019%2F09%2F07%2Fgo-channel-demo%2F</url>
    <content type="text"><![CDATA[github地址: simplexity-ckcclc/gochannel 这是穷人版的渠道匹配，处理新增激活设备与点击的匹配，简化了后台管理接口及第三方平台对接接口，仅保留逻辑。 TODO 短链匹配 android匹配（imei无法获取情况，使用android_id） 后台线程定期删除ES，mysql里过期数据 异常情况处理，监控 匹配结果导出，报表等 GO工程的参数设置]]></content>
      <tags>
        <tag>Go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Blockchain性能扩容（1）- (Size, Segwit, Sidechain)]]></title>
    <url>%2F2019%2F07%2F14%2Fblockchain-performance-scaleup-concern%2F</url>
    <content type="text"><![CDATA[区块大小扩容(Block size)比特币引入1MB区块大小的背景是大量的粉尘攻击导致区块巨大，区块需要同步的时间长，从而引发DOS（Denial of Service）。同时全节点的带宽，存储要求增高，导致性能不足的节点无法及时与主链同步。 Pros 提高TPS 更多的交易得到确认（当前限制下，交易手续费低的费用可能无法得到确认） Cons 节点性能要求增高准入门槛提高，导致网络全节点数量减少，挖矿中心化趋势 升级配置带来的硬分叉风险 区块全网同步时间增加，上一个区块的矿工周围节点比网络边缘节点（相对的）更早同步更早开始挖矿，中心化趋势。同时出块时间保持不变的前提下（同步时间占区块时间比重上升），会增加分叉，孤块（stale block）的可能性，以及增加交易回滚，双花等风险。类似的，减小区块时间提高出块速度也面临这个问题。 存在其他的性能提升替代方案，如隔离见证，侧链等 隔离见证（SegWit）这个是针对比特币的改进，提出主要是要解决Transaction Malleability（交易延展性）这个问题。（因为早期的transactionId是由utxo的来源，目的以及签名计算得到的。根据密码学的特性，可以从签名通过计算得到新的的签名，并且也是密码学意义上有效的（比如椭圆曲线数字签名算法（ECDSA），这种算法下签名（r，s）和签名（r，-s （mod n））都是有效的）。因此恶意节点可以从正常交易复制出新交易，签名有效，从而生成新的transactionId。当撤回前一个交易再换utxo输入重发时，后一个交易仍然是有效的，可能会导致重复转账的欺诈行为）。此外，第三方增加Push data到验证信息的起始位置，改变交易ID(txid)同时验证结果也通过。因此，将签名信息（见证）从交易抽离出来放到交易的后面（锁定脚本，这部分是不包含在1MB块大小的计算限制里），即Hash出来的transactionId是不包括签名的，这时就无法通过改变签名计算生成新的交易。 Pros 解决Transaction Malleability这个问题 闪电网络的技术基础 区块大小限制并没有打破，旧节点仍然可以接受新的块格式（一个隔离见证的输出看起来像一个任何人都能花费的输出，该输出可以被一个空的签名见证，所以一个交易里面没有签名（签名被隔离）也可以通过旧节点的验证）。后相兼容的软分叉。 Cons 每笔交易平均250字节，见证部分的数据约为150字节。这样相当于实际上增大了区块大小，隔离见证的整个区块大小为2.5到3MB左右。增加区块实际大小带来的风险依然存在（当然，也有通过将见证信息放在侧链上，交易主链通过引用的方式）。 bips/bip-0062.mediawiki ：Transaction Malleability的官方文档What is SegWit and How it Works Explained 侧链（Sidechain）侧链只需符合侧链协议，能够与主链进行安全的资产/数据转移的都可以成为侧链，可以作为二层网络分担主链上的压力（例如比特币闪电网络进行结算），或者实现更复杂的逻辑功能，将主链作为最后的清算链，另外也可以实现异构链间的资产转移。实现的技术基础是双向锚定（Two-way Peg），通过双向锚定技术，可以实现暂时的将数字资产在主链中锁定，同时将等价的数字资产在侧链中释放，同样当等价的数字资产在侧链中被锁定的时候，主链的数字资产也可以被释放。具体有多种实现方式，常见的是SPV（Simplified Payment Verification）。SPV主要涉及到两个东西, 第一个是Merkle Tree，另一个是布隆过滤器（Bloom Filter）。譬如说转账后watch event，如果不在乎隐私的话, 可以直接告诉全节点所关注的地址就可以；但是如果不想暴露隐私, 那就把关注的信息映射到BF上, 然后全节点返回所有命中BF的event。 双向锚定分为以下几个阶段： 用户在主链上将数字资产发送到主链的一个特殊的地址，把数字资产锁定在主链上。 该锁定会等待一个确认期，确认期的作用是等待锁定交易被更多区块确认，可防止假冒锁定交易和拒绝服务攻击。 确认期结束后，用户在侧链上创建一个对应的带有SPV证明的交易，同时验证主链上的数字资产已经被锁住，然后就可以在侧链上打开具有相同价值的另一种数字资产。该交易称为赎回交易，SPV工作量证明是指赎回交易所在区块的工作量证明。 当这种数字资产返回到主链上时，该过程会进行重复。它们被发送到侧链上锁定的输出中，锁定一个竞争期，竞争期的作用是防止双花。然后就可以创建一个SPV证明，来将其发送回主区块链上，以解锁主链上的数字资产。 全面理解区块链侧链技术 ： 提及多种侧链实现方式 闪电网络（Bitcoin lightning network）在侧链建立点对点通道channel，进行小额和持续多次的资产往来，然后等达成共识后关闭通道，将资产合约发主链上完成结算。核心思想是提高欺诈成本，进行惩罚。 参考 什么是比特币的闪电网络？一文中的例子，在双方达成C2a共识后(C1a和C2a始终由Alice持有，达成c2a时Alice需要将Alice2秘钥给到Bob。这里C1a只有Alice持有，因此Bob即使拥有Alcie2秘钥也无法签发经过双方原始秘钥Alice和Bob签名的合同)，若然Alice广播C1a并且关闭通道（主动关闭通道方需要等待指定seq数量的块后才能执行RD1a），此时Bob监控到Alice广播旧的分配合约(这需要Bob节点在线，或者事先构建好BR1a交付给信任的第三方或者机器人在Alice欺诈时自动触发)，可以使用Alice2秘钥对BR1a签名并广播，这个是无需等待即时结算的，所以原本属于Alice的都分配给Bob。这就是欺诈作恶成本。 之所以说Segwit隔离见证是闪电网络的基础，是因为Funding Tx只有在第一个Commitment Tx完成签名并交换之后，才会继续签名和广播（否则，广播Funding Tx后若Bob拒绝提供第一个Commitment的签名，这笔资金将被永久锁定，无法消费。拥有Commitment Tx后可以广播赎回）。此时第一个Commitment Tx消费的Funding Tx未曾广播入链，因此需要保证该Funding Tx未被篡改，即txid的固定性。 而对于多方中转交易，在上文中描述有误。根据Bitcoin Lightning Network Payment Channels Explained - Thaddeus Dryja（Youtube视频，需要梯子），在上文的例子中，Alice和Bob之间没有建立直接支付通道，需要经过第三方Charlie中转（与Alice和Bob各自建立支付通道，非可信第三方），同时Alice与Bob之间需要建立正常的TCP/IP等通信通道。若Alice需要向Bob转账，Bob选择随机数R，并且生成哈希H，将H通过通信信道发给Alice。Alice与Charlie之间达成合约，即Charlie能在指定时间内展示出正确的R值，Alice即支付指定金额，否则撤回该合约。同理Charlie与Bob间也是如此。 实际上，这样的Charlie相当于目前金融系统里的支付宝角色，一个持久而且大多数节点建立支付通道的节点（或者节点群，类似与Alice与Bob间经过多个节点，这些节点群之间建立支付通道，因此Alice与Bob之间能比较容易找到路由）。同时，对于频繁支付或者大额支付的节点（假设Bob是商家），则Bob与Charlie间的通道容量需要足够大，也就是双方在主链上锚定的资产价值足够高，否则需要经常的关闭（主动方提现需要等一定区块时间）和建立多个通道。这样的并非极端意义的分布式去中心化，但已经利用了区块链的无需信任属性。]]></content>
      <tags>
        <tag>Blockchain</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[共识算法(3) - PoS]]></title>
    <url>%2F2019%2F07%2F08%2Fconsensus-pos%2F</url>
    <content type="text"><![CDATA[PoS(Proof of Stake权益证明)相对于PoW的优点： 不需要消耗大量电力成本 发动51%攻击成本更高（PoW矿池或者大量租用矿机集中算力发动攻击，或者快速算法，硬件升级带来的算力提升等，都可能会增加攻击风险。PoS需要收购全网51%的Token，并且持有者的权益在于所持有的Token也更大动力维护系统安全） PoS为什么比PoW更能避免51%攻击？ PoS运行过程 质押stake，声明为验证节点（区别于全节点拥有出块的权利，会有一定的门槛，如持有stake量，占有比例，或者top n等条件，意在控制验证节点数量，缩小选举的范围以及达成bft共识的节点数） 选举 （周期性选举，以之前已出块的值作为随机种子，选取下一周期的出块节点和顺序，选举算法包含Follow-the-Satoshi选择随机stake的持有节点和RR-BFT轮询算法） 打包交易 （该论的出块节点从交易缓存池选择，通常为交易手续费高，时间早等，验证，包括签名，账户余额等。如果该论的出块节点没有出块，则跳过该论，并且惩罚） 广播交易，确认（PoS分为基于链的PoS和BFT风格的PoS。在基于链的PoS中，最长链原则，随机算法选出验证者创建新区块，但是验证者要确保该块指向最长链，通常也需要一定数量的验证节点背书签名后上链。在BFT风格的PoS中，分配给验证者相对的权力，以RR的方式提出块并且给被提出的块投票，从而决定哪个块是新块，并在每一轮选出一个新块加入区块链。在每一轮中，每一个验证者都为某一特定的块进行投票，最后所有在线和诚实的验证者都将商量被给定的块是否可以添加到区块链中，并且意见不能改变） 一文读懂 PoS 共识运行七大步骤 PoS的攻击风险比较突出的是Nothing-at-stake(无利害关系)和Long-range-atack(长程攻击）两种 无利害关系 Nothing-at-stake Nothing At Stake, a situation where someone loses nothing when behaving badly, but stands to gain everything. 在出现多条区块链相互竞争的情况下，会激励验证者在每条链上都创造区块，以确保利益最大化。因为PoS创造新区块基本上无需成本，只受持有的资产（token）影响，这样使得双花攻击更加容易，而PoW则受算力影响，分散算力反而降低收益期望值，即经济学意义上的机会成本。这个可以一个例子说明。 如下图所示，仍然是 A、B、C 三个出块节点，假如 A是攻击节点，它在产生分叉时创造两笔交易。一笔将X个币发给自己的一个钱包地址，同时在另外一个分叉上将X个币发到交易所。B、C出块节点因 Nothing At Stake 所以同时会在两条分叉链上出块。当交易被交易所确认后， A 将 X 个币出售兑换成隐私币种，移出交易所。之后 A 通过增加质押币量，或创建多个其他出块节点的方式提升出块权重，只在分叉链继续出块。此时最长链很明显，且逐渐拉开差距，会最终成为最长链，A 成功将 X 个币双花。 摘自 ：PoS背后的Nothing At Stake与Long-Range Attack 解决方案 ： 无成本攻击则引入惩罚机制。验证节点提前抵押一定的stake做为保证金，如果发现验证节点“在多条链上同时出块” 或者 “在错误的链上出块”，立即惩罚扣除一定保证金。只要保证金设置合理，使“双挖”的收益期望值降低，可以从经济学意义上消除这个风险。 长程攻击 Long-range-atack 恶意节点从很早以前的甚至是创世块开始重新建造一条链，目的来代替正常运行的主链。由于PoS创建区块的成本极小，新链的构造速度可以很快，赶上主链的高度。如果不考虑timestamp，那么恶意节点可以超前当前时刻，创造大量的未来区块时间，从而增加链高度代替正常链。这种情况加上timestamp即可避免。 考虑timestamp的主要有两种形态，一种是Posterior Corruption。譬如如果当前区块高度为X，想从之前的区块K开始分支。那么从K到X之间所有的区块的签发者联合起来（或者通过贿赂，盗窃秘钥等方式），将质押的stake全部撤回，然后创建从K开始创建新链（没有质押就不存在签发X之后的新的区块，但是可以签发之前的，而且避开了双签的惩罚措施），此时创建的新链长度就赶上甚至替代正常链。Casper通过“检查点”（checkpoint）机制来应对这种方案。取消抵押保证金有一个“解冻”时期。同时限制区块分叉上限，节点拒绝回滚比保证金被锁定时间更久的区块，特定的上限区块称之为“检查点”，这样这些撤回的stake在可以用来重签时已经过了检查点。（要理解这段需要了解节点随机选举过程的Follow-the-Satoshi算法，选择随机stake的持有节点）。另外也有研究KEC（Key-Evolving Cryptography）即秘钥签发一次即失效的方案防止二次签发。还有的方案是在交易时也带上前任区块的hash值，这样通过检查交易的这个值，可以避免链的中途恶意分叉。 另一种方式是Stake Bleeding。如图，恶意节点M在正常链选择在M的出块时刻不出块，即Liveness Denial attack，则该轮被跳过，链高度不变，M被惩罚，逐渐的演变下去stake减少，出块机会减少。而在恶意链，M在其时刻出块，其他的则不可能出块，导致M的出块机会增加，演变到最后只有其出块。随着时间的推移会赶上主链。但是研究表明，拥有30%stake的攻击者也需要6年才能实现，现实可行性比较低。 长程攻击通常与Weak Subjectivity(弱主观性)相关联。区块链网络中的新节点或是长期离线的节点加入到网络中时，会收到当前区块链上所有公开的分支，但是节点并不能分辨出哪些分支是从属于主链的（PoS里最长链原则并不足够，因为PoS里创建区块成本极小，可能存在多条等长链，这与PoW是区别的）。因此，在PoS里，新节点要求验证链外最新状态来解决，节点可以通过询问他们的朋友、区块浏览器等进行区块同步，离线的节点要求在“还原上限”时间内至少要登录同步一次。在线节点因为一直与主链同步，则不存在这个风险。 Rewriting History: A Brief Introduction to Long Range Attacks]]></content>
      <categories>
        <category>Consensus &amp; Cryptography</category>
      </categories>
      <tags>
        <tag>Consensus</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[技术沙龙（1）- 迅雷链同构跨链]]></title>
    <url>%2F2019%2F07%2F01%2Fthunderchain-cross-chain%2F</url>
    <content type="text"><![CDATA[目前公链关键性问题：1. TPS低，确认时间长；2.安全性。迅雷链（同构多链）主要两个特性：1. 各链相互独立平等，独立运行共识机制；2. 保证链间安全。具体可以参看迅雷链官网查看其性能及特性，这里列举沙龙上交流获得的信息。 同构多链。各链平等独立，相当于将单链分片（zone）。每个链负责部分账号，当链性能不足时动态扩链增加新链（目前还是中心化人工扩展），将账号归属重新分配，同时下发新的路由配置到接入层和各链PBFT共识节点。 共识算法PBFT+DPOA。DPOA(ability)作用为单个链周期性（当前为一天，也可能为若干轮出块，可配置）的选举出部分节点（这个是由中心化根据检测到各个节点和其上报的性能计算出比重分值而选择，指标包含网络带宽，计算能力，网络情况，历史名声等）作为候选节点。然后在这些候选节点中选出PBFT节点（避免同一网络，区域等，根绝PBFT节点超过20多个后性能下降的特性，目前是21个）。此处利用的是PBFT的确定性共识。这些PBFT节点负责在该周期内出块。然后由中心将这些配置（包含这些PBFT节点的ip端口，公钥等）下发到该链（同时也会下发到其他链的PBFT节点用作跨链交易）。 当前仍然存在中心控制（包括参数动态调整，选举，监控等）。迅雷链并非需要严格意义上的完全去中心化，而是核心思想，分布式共识账本，不可篡改，以及智能合约。 交易过程。交易发起方通过接入层接入链，由接入层根据发起账号路由到其归属链。当动态扩链时，中心（metachain）会将更新路由规则。如果交易当接收方与发起方在同一个链上，该交易为单链交易，按照正常程序打包入块。如果交易接受方在其他链上，当交易在该链打包出块后（例如转账中对from账号扣钱），同时将跨链交易按照接收方所在链以目的链（从路由配置得到）为分割单独打包成交易。from链的relay节点（在PBFT节点中选取两个）将改打包后的交易（看作一个交易，由from链的PFBT节点签名）发送到to链的relay节点，然后to链的relay节点将该跨链交易与其他的普通同链交易进行后续的打包出块（例如转账中对to账号加钱）。to链的PBFT会验证from链PBFT节点对交易的签名。出块后执行结果会回送到from链的relay节点，同时relay节点也有重试功能。（当前是push的方式，后续可能增加pull的方式，由to链的relay节点定期去from链的relay节点拉取交易） 相当于一笔交易被拆分，没法保证单链下的原子性。from链的执行成功后，没法同时保证to链的执行成功（即不存在分布式事务）。当前没有回滚机制（主要是因为from一直在出块，跨链交易回滚会造成from链的扣款操作和该回滚操作中间的其他区块的操作也需要回滚，演变成雪球效应）。或者使用类似于分布式锁，将账户锁定，但相应的会损害性能。据说在研究使用UTXO模型来解决改问题。 另外，其他的话题点包括： 智能合约的安全性检测。通过账户的交易特征，合约的字节码特征等，使用机器学习的方法，识别诈骗等恶意智能合约。 5G+区块链。改变对设备对控制权方式（拥有设备对私钥）]]></content>
  </entry>
  <entry>
    <title><![CDATA[Ethereum概念理解]]></title>
    <url>%2F2019%2F06%2F25%2Funderstanding-ethereum%2F</url>
    <content type="text"><![CDATA[必读： Ethereum White Paper Ethereum Yellow Paper Block, transaction, account state objects and Ethereum tries The world state trie contains the mapping between addresses and account states. The hash of the root node of the world state trie is included in a block (in the stateRoot field) to represent the current state when that block was created. We only have one world state trie. The account storage trie contains the data associated to a smart contract. The hash of the root node of the Account storage trie is included in the sccount state (in the storageRoot field). We have one Account storage trie for each account. The transaction trie contains all the transactions included in a block. The hash of the root node of the Transaction trie is included in the block header (in the transactionsRoot field). We have one transaction trie per block. The transaction receipt trie contains all the transaction receipts for the transactions included in a block. The hash of the root node of the transaction receipts trie is included in also included in the block header (in the receiptsRoot field); We have one transaction receipts trie per block. 摘自: Merkle Tree and Ethereum Objects - Ethereum Yellow Paper Walkthrough (2/7) Externally owned account(EOA) VS. Contract account 两者都使用20字节的地址。EOA外部拥有账户是由用户通过公钥/私钥秘钥对控制的，通过以太坊客户端创建，由秘钥对生成的账户地址。而Contract account则是EOA通过调用创建合约的方法创建的账户，账户内包含合约代码，由调用方EOA的地址和调用方等nonce（该值针对EOA递增，目的为防止重放攻击）经过PLP编码和KEC散列生成账户地址。交易只能由EOA发起，目的方可以是EOA（常见的转账行为，交易transaction的data字段为空），也可以是合约账户（调用执行智能合约，一次调用中也可能包含跨合约调用，即合约账户到另一个合约账户，此时交易transaction的data字段存放的是合约调用的方法名，参数等二进制数据），还有合约部署（目的地址为0，交易transaction的data字段存放）。 |字段|含义| |—-|—-| |nonce|A scalar value equal to the number of transactions sent from this address or, in the case of accounts with associated code, the number of contract-creations made by this account. | |balance|A scalar value equal to the number of Wei owned by this address.| |storageRoot|A 256-bit hash of the root node of a Merkle Patricia tree that encodes the storage contents of the account (a mapping between 256-bit integer values), encoded into the trie as a mapping from the Keccak 256-bit hash of the 256-bit integer keys to the RLP-encoded 256-bit integer values.| |codeHash|The hash of the EVM code of this account—this is the code that gets executed should this address receive a message call; it is immutable and thus, unlike all other fields, cannot be changed after construction. All such code fragments are contained in the state database under their corresponding hashes for later retrieval.| It is not the code that is executed on subsequent transactions sent to the contract. That code is returned by the initialization code. Essentially, the code in the data field is a program that is going to write a program that gets deployed as a smart contract.（部署字节码 + 合约字节码 + AUXDATA + Swarm hash） The standard initialization code generated by the Solidity compiler does the following: Runs the code in the contract’s constructor, setting storage values, etc. Copies the code for the rest of the contract into memory and returns it. 新部署的合约代码被打包提交到链上后，所有到运行节点都会执行data字段里的部署代码（特殊情况是部分节点只按需存储部分数据），其合约存在于各运行节点各自的EVM内，并且对应的地址都是一致，即地址与合约对应，保证后续的合约调用。合约代码存放在EVM的virtual rom内，code hash值为代码的哈希值，storage hash存放的是账户的MPT树的root hash。简之，部署合约三个步骤：1. 创建合约账户 2. 执行合约的初始化 3. 拷贝到virtual rom How Ethereum Transactions WorkHow Smart Contract Deployment Works MPTMPT(Merkle Patricia Tries)是以太坊中存储区块数据的核心数据结构，它是 Merkle Tree 和 Patricia Tree 融合一个树形结构。附上经典的官方结构示例。具体的代码分析网上资料详细。 以太坊采用改进的Merkle树，因为前后两个block的数据绝大部分都是相同的，不需要同时存储两份。新block的树可以引用上一个block，加上新block内的交易修改的账户。 以太坊源码分析—MPT树Ethereum以太坊源码分析（三）Trie树源码分析（上）Ethereum以太坊源码分析（三）Trie树源码分析（下） EventIn Ethereum, when a transaction is mined, smart contracts can emit events and write logs to the blockchain that the frontend can then process. Event在Ethereum里主要有三种用途： smart contract return values for the user interface 发送交易调用合约的返回结果是交易的哈希值，而不是合约的返回值（只有在被打包进区块时才执行合约）。执行合约产生的结果可以作为时间写入区块，调用方可以根据合约哈希值相应获取。 asynchronous triggers with data 异步时间触发，类似于观察者模式。 a cheaper form of storage 称为log（通过LOG这个EVM操作码）。相比stroage耗费的gas小得多，可以用作存储，但是不能在合约中被调用。例如可以存放历史数据，前端可以通过遍历区块去获取。 Technical Introduction to Events and Logs in Ethereum GHOSTGHOST协议（Greedy Heaviest Observed Subtree protocol）。同样是POW，比特币由于出块时间10min，取最长链。而Ethereum出块时间短（6s），不足扩撒至全网，如果取最长链，则地域优势明显。因此，Ethereum选择最重的链，也就是兄弟节点之间子树节点最多的被选为主链。例子可以参考 以太坊Ghost协议和叔块。主要是要解决 1. 出块速度快，存在多个节点同时出块，但是在自身出块后才接收到别的节点传输的块，浪费算力。2. 更严重的是因为出块速度块带来的地域优势，越早接收到合法块的节点出下一个块的可能性更大，从而造成的中心化风险。通过增加叔块选择最重的链，也就是选择工作量最大的链，降低地域优势。同时奖励叔块，激励节点出块。 官方文档 - Modified GHOST Implementation ： 详细介绍了GHOST稀释风险，以及叔块的选择标准和奖励方案。 Transaction Structure 字段 含义 nonce A scalar value equal to the number of transactions sent by the sender gasPrice A scalar value equal to the number of Wei to be paid per unit of gas for all computation costs incurred as a result of the execution of this transation gasLimit A scalar value equal to the maximum amount of gas that should be used in executing this transaction. This is paid up-front, before any computation is done and may not be increased later to The 160-bit address of the message call’s recipient or, for a contract creation transaction, ∅, used here to denote the only member of B0 value A scalar value equal to the number of Wei to be transferred to the message call’s recipient or, in the case of contract creation, as an endowment to the newly created account v, r, s Values corresponding to the signature of the transaction and used to determine the sender of the transaction.For the signing, Ethereum uses the same elliptic curve as Bitcoin’s, which is secpk256k1.用于从签名中恢复ECDSA公钥，从而得到发送方from的地址Ref: ECDSA: (v, r, s), what is v? data An unlimited size byte array specifying the input data of the message FISCO BCOS 2.0+ 的交易结构在原以太坊的交易结构的基础上，有所增减字段。比较重要的新增字段是blockLimit，表示 交易生命周期，该交易最晚被处理的块高.]]></content>
      <categories>
        <category>Ethereum</category>
      </categories>
      <tags>
        <tag>Ethereum</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ethereum设计理念Design Rationale学习心得]]></title>
    <url>%2F2019%2F06%2F12%2Fethereum-design-rationale%2F</url>
    <content type="text"><![CDATA[本文为Ethereum设计理念官方文档Design Rationale的阅读理解，并非完整的中文翻译，可参考中文翻译。而这个官方文档阐述的是Ethereum演进过程中区别于传统加密货币之处，包含优点，一些有争议性的话题，以及各自的风险。 基本准则 三文治式的复杂度模型 尽量简单的底层结构（如核心共识算法）以及易于使用理解的接口层（developers and users）。复杂度在中间层（serialization and deserialization scripts, storage data structure models, the leveldb storage interface and the wire protocol, etc）。也就是，底层基本的核心思想维持稳定和简单，将易变的可替换的具体实现放在中间层。 尽可能的包含底层概念，使得可以任意方式去组合。同时当剥离不必要的功能也能使底层运行得更有效率。 核心层拒绝内嵌高层应用特性，而鼓励以合约的方式来模拟实现高层应用协议。 自由度保持”net neutrality”网络中性。对contracts or transactions没有先天的偏好性。通过设置费用引入激励。这里涉及到gas的概念，基本理念就是对需要计算和存储的操作需要付出代价。对计算，对存储的字节数等计算一个gas值，而每个发送方指定一个gas值所等值的Ether(即gas fee)，相乘得出为当前的交易费用。交易费用高的当然优先被确认。这就是所谓的将自由度交还给交易发起方，而Ethereum本身保持中性无偏好。引入计算和存储费用，也可以防止DOS攻击。 高风险承受力可以接受高收益带来的高风险（例如出块速度提升50倍，共识效率等），意味着协议演进过程可能会变动较大 使用Account模型而非UTXOunspent transaction outputs (UTXOs)在数字货币内广泛使用，即所有者关联着多笔UTXO，其和为所有者对账户余额，每次交易以单笔或者多笔UTXO为输入，输出也为UTXO。主要符合三个约束： 输入的UTXO有效，并且未被消费 交易者能证实UTXO的所有权（签名） 输入UTXO等于或者超过输出UTXO（消费，转账等） UTXO的好处在于： 高隐私度 - 每次交易都使用新地址，则这些账户很难被关联起来。但是dapp反而通常需要关联用户状态。 潜在的高扩展度 - 用户自身保管UTXO，全网丢失只会影响当前用户。而账户模型下，是记录在merkle tree内的，如果全网丢失该账户记录，则永久影响该账户。 （实际上，UTXO模型还有以下特点） 账户数据库会不断膨胀，因为账户不会被删除，而UTXO数据库体积会小很多。 由于只有未花费的输出会被保留，所以每一个比特币用户可以拥有几乎无限多的地址，提高了匿名性。 UTXO为高并发的交易带来可能，试想传统的账户模型，每个人的账户交易必须是线性的，无法并发。而现在每个人可能拥有多个UTXO，可同时发起多笔交易，实现了并发。 而Account模型好处在于： 节约存储空间 - 多个UTXO的存储合并成一个Account 可以判断来源，从而建立红黑名单 理解和编程模型简单，而且light client可以直接跟踪特定的account，而不用跟踪每笔交易带来的影响 Account的问题在于重放攻击，因此可以每个交易添加nonce，每次交易nonce递增加1。同时，交易包含区块号，定期的修剪删除不再使用的账户。 Gas and FeeBitcoin等数字货币操作比较简单和单一的交易转账行为，而Ethereum则是图灵完备，具有很高的合约自由度。引入费用防止合约执行无限循环的DOS，实际上就是引入作恶成本。 发起调用合约会预缴startGas * gasPrice（即一个gas等价于的Ether），途中每个交易操作会相应扣除gas，合约结束后剩余的gas退还到发起账户，消耗的gas奖励给miner。如果中途gas已消耗完，则该次交易内之前已执行的操作回滚，交易失败，gas奖励给miner。合约间调用也需要消耗gas，由发起合约设置值。 按照计算复杂度（opcode），数据的字节数，存储的字节数等计算gas值。此外还有基础gas，用以支付计算椭圆曲线从签名确认身份等。也即是按消耗付款。 删除存储的空间的操作有gas refund奖励，鼓励减少存储空间。 gasPrice的设置则变成经济供求关系，自由竞争最终达到动态平衡。出价高收益大的交易会被优先执行和确认。 RLPRLP (“recursive length prefix”) encoding is the main serialization format used in Ethereum。例如，Account等数据从内存写入持久化存储时会将Hex16进制转换成RLP持久化(结合MPT树，将Extension node, Leaf node, Branch node等数据结构进行RLP编码)，紧凑节省空间。相比protocol buffer和BSON等现存的序列化方案，从新开发RLP序列化方案的目的是，1）实现简单。目的就是要高度简化，存储嵌套的字节序列，由上层应用决定这些字节序列的含义。2）保证字节级的绝对一致性（Key/value maps的有序性，浮点数的精度问题等）。RLP将 1）字符串（字节序列）作为item，2）item列表也看作一个item，针对这些item使用其定义的序列化编解码方式。 EVM虚拟机设计理念： 简单 尽量少的数据类型，尽量少的操作指令。尽量节省的空间。结合起gas消费模型，也从根本上保证安全不被无限制利用。 定制化 针对以太坊常见应用，对20字节地址，32字节对加密算法，加密算法中的数学运算，读取存储区块及交易信息等，作虚拟机层面的定制化的优化 具体的设计： 存储模型 EVM是基于栈的计算模型。stack（32字节，运行堆栈，所有运算在栈上执行，算子和执行中间结果都从栈存储和读取），memory（byte数组，存储函数调用的参数，本地变量，返回值等），storage（KV持久化存储，256-bit words to 256-bit words） 合约间调用内存隔离，区分临时存储和持久化存储，递归调用每次都使用独立的实例，拥有独立的内存地址空间。具体实例参看原文。 32字节的位宽 加密算法使用32字节，因此32字节位宽不需要分开存储，也不至于浪费空间。 1024层的栈调用深度，并且单个操作码最多只能操作栈顶的16个元素 定制化VM 没有使用JVM等原因在于：1）EVM相对简单，避免复杂带来的安全性等风险；2）可以针对性定制化设计；3）不存在部署的外部依赖；4）不需要改造其他VM时带来的安全性审核工作 EVM详解 : 详细的ppt版本A Deep Dive into the Ethereum Virtual Machine (EVM) - part 2: Memory and Storage : 包含EVM的操作指令opcodes]]></content>
      <categories>
        <category>Ethereum</category>
      </categories>
      <tags>
        <tag>Ethereum</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fabric的configtxgen工具生成区块及配置文件分析]]></title>
    <url>%2F2019%2F05%2F14%2Ffabric-configtxgen-tool%2F</url>
    <content type="text"><![CDATA[本节主要基于fabric-samples/basic-network分析。查看generate.sh这个脚本，里面主要执行了如下四个命令1234567891011// 生成msp模块需要的证书cryptogen generate --config=./crypto-config.yaml// 生成创世块configtxgen -profile OneOrgOrdererGenesis -outputBlock ./config/genesis.block// 生成创建channel的配置configtxgen -profile OneOrgChannel -outputCreateChannelTx ./config/channel.tx -channelID $CHANNEL_NAME// 生成设置anchor节点的配置configtxgen -profile OneOrgChannel -outputAnchorPeersUpdate ./config/Org1MSPanchors.tx -channelID $CHANNEL_NAME -asOrg Org1MSP cryptogen和configtxgen分别用于加密和配置的工具，其代码路径为hyperledger/fabric/common/tools/cryptogen和hyperledger/fabric/common/tools/configtxgen。cryptogen产生了各级msp，在peer，orderer里皆有用到，包括后面介绍的configtxgen里包含msp信息这些也是由此产生。这里暂且不展开分析。 在configtxgen/main.go里，依然使用viper作为命令行管理工具。代码比较直观，阅读简单不展开分析，感兴趣可以参考Hyperledger Fabric（V1.2）源码深度解析－configtxgen命令解析，写的比较概述。其中读取配置文件的common/tools/configtxgen/localconfig/config.go#Load方法里，主要由以下几行1234const Prefix string = "CONFIGTX"var configName = strings.ToLower(Prefix)viper.SetConfigName(configName) 可见，默认读取的是当前目录的configtx.yaml配置文件。生成创世块和创建channel的配置对应的分别是doOutputBlock和doOutputChannelCreateTx这两个方法。doOutputBlock生成cb.Block对象，这个对象通过ioutil.WriteFile(outputBlock, utils.MarshalOrPanic(genesisBlock), 0644)写到指定的genesis.block这个文件。这个对象可以使用configtxgen -inspectBlock genesis.block这个命令查看。生成的JSON格式文件可参见JSON结构。其中ChannelHeaderType设置为HeaderType_CONFIG，block.number为0，previousHash为nil。payload里主要是ConfigGroup对象，包含有序号sequence，以及channel_group对象。channel_group主要是Consortium（包含version，policies，configtx.yaml里定义的orgs，此处orgs里又包含MSP和policies）和Orderer（包含policies，msp）。同时ConfigGroup还包含OrdererAddress,hashAlgorithm,BlockDataHashiungStructure这些。从中，还可以看出，每一级的设置，都包含不同的polocies。 doOutputChannelCreateTx生成cb.Envelope对象，写到channel.tx。ChannelHeaderType设置为HeaderType_CONFIG_UPDATE，channel_id为命令后参数$CHANNEL_NAME。payload里是cb.ConfigUpdateEnvelope对象，包含channel_id, read_set, write_set. 这里，read_set里policies为空，version为0；write_set里version为1，并且定义了policies。所以当使用该channel.tx文件再次创建已经存在的channel时，会报错verifyReadset失败。同样的可以用configtxgen --inspectChannelCreateTx channel.tx这个命令查看其JSON结构。 查看start.sh脚本，实际上在peer节点使用Admin角色执行了创建channel命令。peer channel create -o orderer.example.com:7050 -c mychannel -f /etc/hyperledger/configtx/channel.tx。当顺利执行后，结果返回一个当前channel的首个block，名称为${CHANNEL_NAME}.block。这里block的number为0，pre_hash为null，channel_header里的channel_id是当前channel名称，ChannelHeaderType还是HeaderType_CONFIG。这里因为使用了Admin角色创建，因此signature_header里记录了creator的信息。而payload同时包含了创世块genesis.block和创建channel配置文件channel.tx的信息，特别的是，里面last_update字段则完整记录了channel.tx。同样的可以用configtxgen -inspectBlock ${CHANNEL_NAME}.block这个命令查看其JSON结构。这个区块在start.sh脚本里后续用于channel join -b mychannel.block将节点加入到该channel。 Ref Hyperledger Fabric权限进阶篇]]></content>
      <categories>
        <category>Hyperledger Fabric</category>
      </categories>
      <tags>
        <tag>Hyperledger Fabric</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[密码学基础知识汇总及Fabric的MSP体系]]></title>
    <url>%2F2019%2F04%2F29%2Ffabric-cryptography-msp-summary%2F</url>
    <content type="text"><![CDATA[这篇只是汇总在学习密码学和Fabric的MSP体系过程中的阅读，方便日后回头翻阅，可以迅速回忆。涉及内容较多主体较为分散，挂一漏万。仍有待深入研究学习。 密码学的基础知识概述（Hash算法，对称加密算法，X.509证书） CSDN笔记 - MD5, SHA, AES, DES, X.509, PKCS概述 : 记录基础知识 Structure of X509 Certificates : X.509结构 RSA RSA算法原理(一) RSA算法原理(二) : 数学原理介绍，欧拉函数，大数因数分解 ECC（椭圆曲线） Elliptic Curve Cryptography: a gentle introduction : 英文介绍系列，原理解释。非欧式几何，有限域离散对数问题。强力推荐 Comparing ECDSA vs RSA : ECDSA和RSA的全方位对比。RSA历史更悠久更成熟，部分CA还不支持ECDSA。在提供相同安全程度上，ECDSA需要的bit更少。同时，RSA seems to be significantly faster than ECDSA in verifying signatures, though it is slower while signing. Higher security level require more bits, this results in RSA’s performance to decline dramatically, whereas ECDSA is only slightly affected. 当前，Fabric的签名算法只支持ECDSA（The signing key used for signing by the node (currently only ECDSA keys are supported)），摘自Fabric官网-Membership Service Providers (MSP) Ref: Bitcoin和Ethereum中的vrs（v：recovery id） ECDSA: (v, r, s), what is v? ： Strictly speaking the recid is not necessary, as we can just cycle through all the possible coordinate pairs and check if any of them match the signature. The recid just speeds up this verification. How does recovering the public key from an ECDSA signature work? 零知识证明 Zero-Knowledge-Proof Zero-Knowledge-Proof Wikipedia MSP Implementation with Identity Mixer : Fabric里对零知识证明的使用 Fabric官网的MSP相关文档汇总 Identity : 最基本的概念，PKI体系，X.509证书，CA，CRL，概念介绍 Membership : 概念介绍，MSP的层次设计（Local MSP，Channel MSP），MSP的结构（主要是在节点内的证书存放目录），以及与Org的关系 Membership Service Providers (MSP) : 配置，偏实践应用]]></content>
      <categories>
        <category>Consensus &amp; Cryptography</category>
      </categories>
      <tags>
        <tag>Hyperledger Fabric</tag>
        <tag>Cryptography</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fabric官网Commercial Paper案例chaincode的golang实现]]></title>
    <url>%2F2019%2F04%2F19%2Ffabric-commercial-paper-go%2F</url>
    <content type="text"><![CDATA[官网上描述了Commercial Paper这个应用场景，并且提供了nodejs实现。这里补充chaincode的golang实现，保持逻辑一致，以及记录实际操作中的一些步骤和要点。实现的源码已上传github。 按照官网Commercial Paper nodejs实现里的步骤，启动net_basic网络，并且启动了peer0.org1.example.com, couchdb, ca.example.com, orderer.example.com, cliDigiBank, cliMagnetoCorp这些容器（按照步骤里执行下来，docker ps能看到这些容器）。 install chaincode。执行docker inspect cliMagnetoCorp，能看到/Users/ckcclc/github/fabric-samples/commercial-paper/organization/magnetocorp:/opt/gopath/src/github.com这个路径绑定。在/Users/ckcclc/github/fabric-samples/commercial-paper/organization/magnetocorp创建目录gocontract，将golang实现放在这个目录里。这时候登录到cliMagnetoCorp容器，可以看到容器内/opt/gopath/src/github.com/application下有gocontract这个目录，并且go实现在内。 1peer chaincode install -n gopapercontract -v 0 -p github.com/gocontract 同时，docker inspect cliMagnetoCorp(或者cliMagnetoCorp容器执行env查看），GOPATH=/opt/gopath, CORE_PEER_ADDRESS=peer0.org1.example.com:7051。install会到${GOPATH}/src下寻找source code。然后install chaincode到${CORE_PEER_ADDRESS}。 登录peer0.org1.example.com容器。env出来FABRIC_CFG_PATH=/etc/hyperledger/fabric，查看改路径下的core.yaml配置文件，其中peer.fileSystemPath : /var/hyperledger/production。查看/var/hyperledger/production/chaincodes目录，存在文件gopapercontract.0(即${chaincodeName}.${chaincodeVersion}). instantiate chaincode。登录cliMagnetoCorp容器，执行 1peer chaincode instantiate -n gopapercontract -v 0 -c '&#123;"Args":[]&#125;' -C mychannel -P "AND ('Org1MSP.member')" 此时，docker ps可以看到dev-peer0.org1.example.com-gopapercontract-0这个容器。可以在cliMagnetoCorp容器执行。 12peer chaincode invoke -c '&#123;"Args":["issue","MagnetoCorp", "00001", "2020-05-31", "2020-11-30", "5000000"]&#125;' -C mychannel -n gopapercontractpeer chaincode query -c '&#123;"Args":["query","MagnetoCorp", "00001"]&#125;' -C mychannel -n gopapercontract 如果后续更新chaincode，假设更新版本为1，则执行 1peer chaincode upgrade -n gopapercontract -v 1 -c '&#123;"Args":[]&#125;' -C mychannel -P "AND ('Org1MSP.member')" 修改fabric-samples/commercial-paper/organization/magnetocorp/application/issue.js这个文件 12345// 63行 const contract = await network.getContract('papercontract', 'org.papernet.commercialpaper'); const contract = await network.getContract('gopapercontract');// 73行 let paper = CommercialPaper.fromBuffer(issueResponse); let paper = JSON.parse(issueResponse); 然后，就可以正常调用node issue.js访问刚启动的golang commercial paper容器了。在整个流程中，可以执行./monitordocker.sh net_basic查看chaincode的日志。 另外，在peer0.org1.example.com节点上可以使用peer chaincode list命令查询channel内instantiated的chaincode以及peer上installed的chaincode。 查询channel内instantiated的chaincode。 12345$ peer chaincode list -C mychannel --instantiatedGet instantiated chaincodes on channel mychannel:Name: gopapercontract, Version: 1, Path: github.com/gocontract, Escc: escc, Vscc: vsccName: papercontract, Version: 0, Path: /opt/gopath/src/github.com/contract, Escc: escc, Vscc: vscc 查询peer上installed的chaincode。 123$ peer chaincode list -C mychannel --installedError: Bad response: 500 - access denied for [getinstalledchaincodes]: Failed verifying that proposal's creator satisfies local MSP principal during channelless check policy with policy [Admins]: [This identity is not an admin] 从返回结果可以看出，当前使用的角色并非Admin（这与MSP相关，使用的非Admin的X509证书）。因此可以指定使用Admin。 1234567891011121314151617# 默认使用的MSP config$ echo $CORE_PEER_MSPCONFIGPATH/etc/hyperledger/msp/peer/# 区别在于keystore存放的用来签名请求的私钥。$ ls $CORE_PEER_MSPCONFIGPATHadmincerts cacerts keystore signcerts tlscacerts# 设置使用Admin的MSP config$ CORE_PEER_MSPCONFIGPATH=/etc/hyperledger/msp/users/Admin@org1.example.com/msp$ peer chaincode list -C mychannel --installedGet installed chaincodes on peer:Name: gopapercontract, Version: 0, Path: github.com/gocontract, Id: 44777e9463329c5b03a93abd48975980a795152907d0c6226b0b85589f4a0ca7Name: gopapercontract, Version: 1, Path: github.com/gocontract, Id: d000a5b0e4ed415ae1739e716c13a481096b947cb31dd5f25ab1d67633d18bb0Name: papercontract, Version: 0, Path: /opt/gopath/src/github.com/contract, Id: f09e75abcf455f6fb83257080ebd739f1d2397cf50ce2e52cd805d992a64bc03 使用openssl查看X.509证书。下面是Admin证书的示例 12345678910111213141516171819202122232425262728293031323334353637$ openssl x509 -noout -text -in Admin\@org1.example.com-cert.pemCertificate: Data: Version: 3 (0x2) Serial Number: 15:2c:67:2c:01:ac:bb:4e:33:ac:59:00:13:0c:e7:eb Signature Algorithm: ecdsa-with-SHA256 Issuer: C=US, ST=California, L=San Francisco, O=org1.example.com, CN=ca.org1.example.com Validity Not Before: Aug 31 09:14:32 2017 GMT Not After : Aug 29 09:14:32 2027 GMT Subject: C=US, ST=California, L=San Francisco, CN=Admin@org1.example.com Subject Public Key Info: Public Key Algorithm: id-ecPublicKey Public-Key: (256 bit) pub: 04:57:57:5f:98:ac:6c:14:a5:a8:ee:8e:83:34:12: 1a:21:57:9b:08:23:c0:33:d0:bf:b0:b0:6d:e2:92: 51:ad:ef:69:58:4f:7c:ec:38:d7:66:86:77:82:57: 38:f8:3a:0f:32:d4:e6:05:1a:9b:3d:5c:18:71:4b: e9:6d:86:3c:a7 ASN1 OID: prime256v1 NIST CURVE: P-256 X509v3 extensions: X509v3 Key Usage: critical Digital Signature X509v3 Basic Constraints: critical CA:FALSE X509v3 Authority Key Identifier: keyid:42:39:AA:0D:CD:76:DA:EE:B8:BA:0C:DA:70:18:51:D1:45:04:D3:1A:AD:1B:2D:DD:DB:AC:6A:57:36:5E:49:7C Signature Algorithm: ecdsa-with-SHA256 30:44:02:20:5c:cc:b6:e8:01:14:fc:65:0c:3d:f0:8c:b3:f9: d0:8d:03:04:d5:9c:41:1c:06:19:b4:a1:2e:45:1d:fa:d4:9b: 02:20:48:3d:04:e6:5d:22:88:a8:46:2b:c9:0b:e6:54:ce:f2: 54:9e:45:ee:a3:61:e4:5a:2b:b1:2e:c8:9b:1b:1b:44]]></content>
      <categories>
        <category>Hyperledger Fabric</category>
      </categories>
      <tags>
        <tag>Hyperledger Fabric</tag>
        <tag>Chaincode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fabric 1.4实战 - 从现象看概念]]></title>
    <url>%2F2019%2F04%2F02%2Ffabric-inpractice-concept%2F</url>
    <content type="text"><![CDATA[通过Fabric实战现象厘清和解析概念，本文只列举现象，未深入源码分析。 Chaincode 【失败】 同一个节点peer重复install同一个chaincode报错。Error: Bad response: 500 - error installing chaincode code papercontract:0(chaincode /var/hyperledger/production/chaincodes/papercontract.0 exists) 【失败】 同一个channle重复initiate同一个chaincode报错。Error: could not assemble transaction, err proposal response was not successful, error code 500, msg chaincode with name &#39;papercontract&#39; already exists 【失败】 重复创建同一个channel报错。BAD_REQUEST -- error authorizing update: error validating ReadSet: readset expected key [Group] /Channel/Application at version 0, but got version 1。这是因为在之前的peer channel create -o orderer.example.com:7050 -c mychannel -f /etc/hyperledger/configtx/channel.tx指定的channel.tx的write_set里的version为1，而read_set的version为0。之前已经创建过，因此当前的version为1，common/configtx/update.go#authorizeUpdate-&gt;common/configtx/update.go#verifyReadSet校验不通过，报错。]]></content>
      <categories>
        <category>Hyperledger Fabric</category>
      </categories>
      <tags>
        <tag>Hyperledger Fabric</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[共识算法(2) - Practical Byzantine Fault Tolerance论文学习笔记]]></title>
    <url>%2F2019%2F03%2F28%2Fconsensus-pbft-paper%2F</url>
    <content type="text"><![CDATA[Service Properties 算法的适用范围 Our algorithm can be used to implement any deterministic replicated service with a state and some operations. deterministic ：确定性服务，即相同的参数执行相同操作，每次得到的结果都是一致的，不存在随机性（the execution of an operation in a given state and with a given set of arguments must always produce the same result） replicated : 多副本 a state and some operations : 服务有相同的初始状态及相同的操作，保证当按相同顺序执行完相同的操作，各个副本的即时状态是完全一致的 (must start in the same state) 在异步系统中要容忍f个恶意节点，则至少需要3f+1个节点才能提供safety and liveness（Asynchronous Consensus and Broadcast Protocols这篇论文中证明）。 safety : 副本服务满足线性一致性，像在中心化服务中同时原子性的执行操作。不受恶意client数量影响，恶意client的操作可以通过被正常client感知，利用access control来限制。 liveness : 利用同步提供liveness（论文证明）。只要满足3f+1个节点，可以保证client持续重传，并且到达destination的传输时间没有无限的增长情况下，client最终会收到回复。 算法不提供隐私容错性，即隐私信息会泄漏给恶意节点 The algorithm does not address the problem of fault- tolerant privacy: a faulty replica may leak information to an attacker. The Algorithm0. 术语 view ：类似raft的term，view number在变更时+1 primary ：在一个view中只有单独一个节点p成为primary，选择的方式为p = v mod |R|(p为节点的id，v是当前view number，|R| = 3f+1) backup ： 一个view内除primary外的其他所有节点，与primary都称为replica算法的粗略过程： A client sends a request to invoke a service operation to the primary The primary multicasts the request to the backups Replicas execute the request and send a reply to the client The client waits for f + 1 replies from different replicas with the same result; this is the result of the operation. 1. The Clientclient c点对点发送&lt;REQUEST,o,t,c&gt;$c到primary，然后由primary广播到所有backup。o是操作，t是时间（如本地机器时间），用来保证exactly-once语义，&lt;&gt;$c表示对消息签名（下同）。replica i相应的回复&lt;REPLY,v,t,c,i,r&gt;$i。v是当前view number（存在view-change情况），r是执行结果，然后各自对消息签名。client可以根据回复的view number跟踪当前view，相应的得知当前primary。client接收到f+1个相同结果的有效回复（校验签名），则结果有效，完成。 如果client在有效时间内没法收到f+1个有效结果，则广播request到所有的replica。如果该request已经被处理过，replica就简单的重新发送reply到client。如果没有，backup会将改request重定向到primary。如果primary没有再次将该request广播，则会被认为是异常节点，最终触犯view change。 2. Normal-Case Operationreplica记录的状态 The state of each replica includes the state of the service, a message log containing messages the replica has accepted, and an integer denoting the replica’s current view. 整个流程分为三阶段，pre-prepare, prepare, commit (pre-prepare, prepare用于保持同一个view内request的有序性，prepare, commit用于保证跨view的reqeust的有序性）。 pre-prepare 阶段 primary广播&lt;&lt;PRE-PREPARE,v,n,d&gt;$p, m&gt;到所有的backup。n是primary分配给该消息的序号，全局递增，m是client的请求消息，d是消息的digest摘要。然后签名。 如果满足以下条件，则backup会接受该pre-prepare请求，并且进入prepare阶段(The last condition prevents a faulty primary from exhausting the space of sequence numbers by selecting a very large one.): 1234* the signatures in the request and the pre-prepare message are correct and `d` is the digest for `m`;* it is in view `v`;* it has not accepted a pre-prepare message for view `v` and sequence number `n` containing a different digest; * the sequence number in the pre-prepare message is between a low water mark, `h`, and a high water mark, `H`. prepare 阶段 backup广播&lt;PREPARE,v,n,d,i&gt;$i消息到所有replica（包含primary和backup），i为该节点id。并且将&lt;PRE-PREPARE&gt;和PREPARE消息保存到本地log。replica接收到prepare消息后，校验签名正确，v是当前view，n在水位h和H之间，摘要d和本地保存的PRE-PREPARE消息里的d相等。当replica i收到2f个这样的PREPARE消息后，就达到了prepared(m,v,n,i) 状态，进入commit阶段。 commit 阶段 replica i广播&lt;COMMIT,v,n,D(m),i&gt;$i消息到所有replica。replica收到后校验通过后，写入log。如果replica i达到了prepared(m,v,n,i) 状态，并且收到2f+1个（包括自身的commit消息）校验通过的commit（v,n,d相同且与log里记录的prepare吻合），则达到committed-local(m,v,n,i)状态，并且其当前状态是消息序号小于n的之前所有消息按序执行后的最新最全结果，则该replica执行m里指定的操作。操作的结果返回到client。 The commit phase ensures the following invariant: if committed-local(m,v,n,i) is true for some non-faulty then committed(m,v,n) is true. This invariant and the view-change protocol described in Section 4.4 ensure that non-faulty replicas agree on the sequence numbers of requests that commit locally even if they commit in different views at each replica. Furthermore, it ensures that any request that commits locally at a non-faulty replica will commit at f+1 or more non-faulty replicas eventually. 这里指的是，达到committed(m,v,n)只能保证至少f+1个正常节点被最终commit，而并不是所有正常节点都执行。这就造成正常节点间都状态不一致，需要最终同步状态。或者使用上文提到Section 4.4里提到都view-change场景下重走commit流程最终达成状态一致。[引用2:Fabric0.6里的最终状态同步方案,3:介绍实时全正常节点同步的方案] 3. Garbage Collectionreplica周期性对其本地状态生成快照，称为checkpoint。当replica i生成checkpoint同时，广播&lt;CHECKPOINT,n,d,i&gt;$i消息到其他所有replica，其中，n是生成该状态执行的最后的请求消息的序号，d是该状态的摘要digest。replica接收checkpoint消息，并且记录到log里，当收到2f+1个相同的checkpoint消息（n,d）相同，则该checkpoint变成stable checkpoint。此时，小于等于n的消息（pre-prepare, prepare, commit）都会被丢弃，同时，之前的stable checkpoint也被丢弃。 实际上，一个replica会同时持有这样几个状态副本：一个stable checkpoint，若干个未达到稳定状态的checkpoint（生成后暂未收集到2f+1个该状态的checkpoint消息），以及当前状态。stable checkpoint的最后的消息序号n就是低水位h,高水位H = h + k(where is big enough so that replicas do not stall waiting for a checkpoint to become stable). 4. View Changesclient在有效时间内没法收到f+1个有效结果，则广播request到所有的replica。backup收到后，若该请求从未执行，则转发给primary，并且启动timer，在timer到期前若没有收到primary发出的pre-prepare，则判断该primary异常，发出view-change消息。backup i,&lt;VIEW-CHANGE,v+1,n,C,P,i&gt;$i，v+1是新的view number，n是该backup的stable checkpoint s的最后消息序号，C是证明s的2f+1个checkpoint消息，P是Pm的集合，Pm是达到prepared(m,v,n,i) 状态的消息（包含pre-prepare消息和2f个来自其他replica的matching消息，其中消息m的序号大于n）。 v+1的primary（通过p = （v+1）mod |R|计算）收到2f个有效view-change消息后，广播&lt;NEW-VIEW,v+1,V,O&gt;$p，V是收到的和primary自身的view-change消息的集合，O是pre-prepare消息的集合。计算如下，比较复杂123451. The primary determines the sequence number `min-s` of the latest stable checkpoint in `V` and the highest sequence number `max-s` in a prepare message in `V`.2. The primary creates a new pre-prepare message for view `v+1` for each sequence number `n` between `min-s` and `max-s`. There are two cases: (1) there is at least one set in the component of some view-change message in with sequence number `n`, or (2) there is no such set. In the first case, the primary creates a new message `&lt;PRE-PREPARE,v+1,n,d&gt;$p` where `d` is the request digest in the pre-prepare message for sequence number `n` with the highest view number in `V`. In the second case, it creates a new pre-prepare message `&lt;PRE-PREPARE,v+1,n,d(null)&gt;$p` where `d(null)` is the digest of a special null request; a null request goes through the protocol like other requests, but its execution is a no-op. (Paxos [18] used a similar technique to fill in gaps.) 也就是说，primary从latest stable checkpoint重放之后所有达到prepared(m,v,n,i)状态的消息（min-s&lt;n&lt;=max-s），重发pre-prepare。replica会对这些消息重走流程，prepare和commit阶段，但是不再重新执行请求，因为在上一个view内，对这些达到prepared(m,v,n,i)状态的消息已经执行过。 同时，replica记录收到的所有replica的view-change里最新的（n最大）stable checkpoint记录到log里，并且对缺失的消息m和最新的stable checkpoint（自身的stable checkpoint不是全局最新的stable checkpoint情况下）从其他的replica同步。 5. Correctness（重点） Safety if prepared(m,v,n,i) is true then prepared(m’,v,n,j) is false for any non-faulty replica j(including i=j) and any m’ such that D(m’)!=D(m) 这是因为，到达prepare（m）意味着2f+1个replica选择发送prepare（m），假设异常节点个数为k(k&lt;=f), 则至少有2f+1-k个正常replica发送prepare（m），同时也意味着至少有2f+1-k个replica发送prepare（m’）。由于(2f+1-k)*2&gt;3f+1-k，说明这两部分正常节点有重叠，则至少有一个正常节点发送两个不同消息，矛盾。 The view-change protocol ensures that non-faulty replicas also agree on the sequence number of requests that commit locally in different views at different replicas. committed(m,v,n) is true if and only if prepared(m,v,n,i) is true for all i in some set of f+1 non-faulty replicas. if committed-local(m,v,n,i) is true for some non-faulty i then committed(m,v,n) is true. 这是因为，到达committed-local(m,v,n,i)状态，则意味着至少收到f+1个正常replica的commit消息，也就是至少有f+1个正常节点到达prepared（m,v,n,i)状态。而正常节点只有在接收到new-view消息才会从view v进入view v+1，而new-view消息里包含2f+1个view-change消息。与上面推导方式相似，则这两个集合必定有一个交集包含正常节点k。若然m已经包含在k的stable checkpont内，则最终被同步到所有正常节点；若然没有，则包含k的view-change消息内，然后在view v+1中被重新执行三阶段流程，最终也达到提交一致。 Liveness replica广播view-change进入v+1，等待2f+1个view-change消息后启动计时器T。如果在T到时前没有收到new-view（新的primary是异常节点），则进入v+2，计时器时间扩大成2T，这是为了避免view改变的过快。 在定时器过期前，如果收到f+1个view-change消息，并且比自身的view number大，则发送这些大view number中最小的一个 f+1个view-change消息才能触发view变更，而且primary的轮流选择方案也限定连续最多f轮选择就能选举到正常的primary。 Optimization1. Reducing Communication 避免传输large result A client request designates a replica to send the result; all other replicas send replies containing just the digest of the result.(指定单个replca回复完整result，其他回复该result的摘要用以校验)。 If the client does not receive a correct result from the designated replica, it retransmits the request as usual, requesting all replicas to send full replies. 减少流程步骤，replica提前返回reply。当replica收到足够prepare消息，达到了prepared(m,v,n,i) 状态，将要进入commit阶段。这时，如果小于该消息序号n的之前的消息都已经执行生成了当前状态，则replica执行该请求，并且直接返回到client，同时继续后续的commit阶段。 The client waits for 2f + 1 matching tentative replies. If it receives this many, the request is guaranteed to commit eventually. Otherwise, the client retransmits the request and waits for f + 1 non-tentative replies. A request that has executed tentatively may abort if there is a view change and it is replaced by a null request. In this case the replica reverts its state to the last stable checkpoint in the new-view message or to its last checkpointed state (depending on which one has the higher sequence number). 优化read-only操作。对于read-only请求，client直接发送到所有replica，replica直接返回结果到client。client收集2f+1个相同结果的回复，否则按照正常流程重新发起请求。 They(replica) send the reply only after all requests reflected in the tentative state have committed; this is necessary to prevent the client from observing uncommitted state. 题外话所有的replica间需要全联接并且多次交互通信，系统中这些消息随着节点数指数增长。单次请求的最少消息数为1 + 3f + 3f(3f-f) + (3f-f+1)(3f+1) + 3f-1. pBFT— Understanding the Consensus Algorithm 私货 为什么需要至少3f+1个节点？首先最后达成共识的正常节点数需要大于f，否则这f个异常节点就可以达成错误的共识。即至少需要f+1个正常节点达成共识。而达成共识时，有可能给最后共识投票的包含这f个异常节点，所以需要至少2f+1个节点投票达成共识，这样可以保证其中至少含有f+1个正常节点。（例如P为异常节点向不同的正常节点提议不同的值，这也是造成上文提到3f+1个节点最后达成共识时只能保证多于f+1个）。而最后的共识只能有一个，即达成共识的正常节点应该超过所有正常节点数的过半，否则f个异常节点可以重复投票导致最后正常节点分裂成两组以上各自达成共识。因此，实际上最后的阈值T&gt;=lower((R-f)/2)+1+f，这样就保证了correctness正确性。接下来考虑liveness可用性。因为异常节点有可能不参与投票（P为正常节点），需要保证正常节点总数应该大于阈值，即R-f&gt;=T。 References [1] M. Castro and B. Liskov. Practical Byzantine Fault Tolerance. Proceedings of the Third Symposium on Operating Systems Design and Implementation, New Orleans, USA, February 1999 [2] 区块链PBFT共识算法节点主动恢复设计与实现[3] 拜占庭将军问题深入探讨]]></content>
      <categories>
        <category>Consensus &amp; Cryptography</category>
      </categories>
      <tags>
        <tag>Consensus</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fabric 1.4源码分析 - peer的deliver过程(1) peer与orderer的交互]]></title>
    <url>%2F2019%2F03%2F20%2Ffabric-source-deliver-peer-to-order%2F</url>
    <content type="text"><![CDATA[peer端的gossip和deliver是紧密结合的。在fabric的结构中，存在leader peer节点，负责与orderer交互，通过deliver服务从orderer得到block。然后通过gossip服务扩散到其他的普通peer，从而达到全网同步。这里先介绍deliver服务。 从前文orderer的deliver过程可知，deliver的grpc服务是”/orderer.AtomicBroadcast/Deliver”,在源码中搜索并且一路回溯到方法peer/node/start.go#serve，这个方法是执行命令peer node start时的执行。（这里先介绍的当前peer重启的情况，新建立channel的情况后面介绍）。然后执行到core/peer/peer.go:Initialize，这里面执行ledgermgmt.Initialize(ConfigTxProcessors)恢复历史账本。然后从这些历史账本信息可以获取到ledgerId（即channelId）。然后对每个channel加载账本，并且从账本中获取最新的ConfigBlock，然后调用createChain重新构造channel信息。在core/peer/peer.go:createChain这个方法里，也是从ledger里获取到配置信息，从而得到orderer地址ordererAddresses := bundle.ChannelConfig().OrdererAddresses(),然后调用service.GetGossipService().InitializeChannel。在gossip/service/gossip_service.go:InitializeChannel里，启动deliver服务需要经过判断是否节点是leader peer，即peer.gossip.useLeaderElection（通过选举的方式动态决定leader peer）和peer.gossip.orgLeader（静态指定当前peer为leader peer），这两种方式是互斥的。前面提到过，只有leader才与orderer进行交互，如果不是，则返回，无需继续往下启动deliver服务。123456789101112131415161718192021// Parameters:// - peer.gossip.useLeaderElection// - peer.gossip.orgLeader//// are mutual exclusive, setting both to true is not defined, hence peer will panic and terminateleaderElection := viper.GetBool("peer.gossip.useLeaderElection")isStaticOrgLeader := viper.GetBool("peer.gossip.orgLeader")if leaderElection &amp;&amp; isStaticOrgLeader &#123; logger.Panic("Setting both orgLeader and useLeaderElection to true isn't supported, aborting execution")&#125;if leaderElection &#123; logger.Debug("Delivery uses dynamic leader election mechanism, channel", chainID) g.leaderElection[chainID] = g.newLeaderElectionComponent(chainID, g.onStatusChangeFactory(chainID, support.Committer))&#125; else if isStaticOrgLeader &#123; logger.Debug("This peer is configured to connect to ordering service for blocks delivery, channel", chainID) g.deliveryService[chainID].StartDeliverForChannel(chainID, support.Committer, func() &#123;&#125;)&#125; else &#123; logger.Debug("This peer is not configured to connect to ordering service for blocks delivery, channel", chainID)&#125; 假设当前节点是通过静态指定为leader，则进入创建deliver服务g.deliveryService[chainID].StartDeliverForChannel.在这个方法里主要分两部。 调用client := d.newClient(chainID, ledgerInfo)构造client。 123456789101112131415161718func (d *deliverServiceImpl) newClient(chainID string, ledgerInfoProvider blocksprovider.LedgerInfo) *broadcastClient &#123; requester := &amp;blocksRequester&#123; tls: viper.GetBool("peer.tls.enabled"), chainID: chainID, &#125; broadcastSetup := func(bd blocksprovider.BlocksDeliverer) error &#123; return requester.RequestBlocks(ledgerInfoProvider) &#125; connProd := comm.NewConnectionProducer(d.conf.ConnFactory(chainID), d.conf.Endpoints) bClient := NewBroadcastClient(connProd, d.conf.ABCFactory, broadcastSetup, backoffPolicy) requester.client = bClient return bClient&#125;func NewBroadcastClient(prod comm.ConnectionProducer, clFactory clientFactory, onConnect broadcastSetup, bos retryPolicy) *broadcastClient &#123; return &amp;broadcastClient&#123;prod: prod, onConnect: onConnect, shouldRetry: bos, createClient: clFactory, stopChan: make(chan struct&#123;&#125;, 1)&#125;&#125; 构建blocksProviderImpl，并与channel对应存储d.blockProviders[chainID] = blocksprovider.NewBlocksProvider.然后调用go d.launchBlockProvider(chainID, finalizer),进而调用了core/deliverservice/blocksprovider/blocksprovider.go#DeliverBlocks。在这里调用core/deliverservice/client.go#Recv方法，一路调用直到调用client.go#connect。 1234567891011121314151617181920212223242526func (bc *broadcastClient) doAction(action func() (interface&#123;&#125;, error), actionOnNewConnection func()) (interface&#123;&#125;, error) &#123; if bc.conn == nil &#123; err := bc.connect() actionOnNewConnection() &#125; resp, err := action() if err != nil &#123; bc.Disconnect(false) return nil, err &#125; return resp, nil&#125;func (bc *broadcastClient) connect() error &#123; // ... abc, err := bc.createClient(conn).Deliver(ctx) err = bc.afterConnect(conn, abc, cf, endpoint) // ...&#125;func (bc *broadcastClient) afterConnect(conn *grpc.ClientConn, abc orderer.AtomicBroadcast_DeliverClient, cf context.CancelFunc, endpoint string) error &#123; // ... bc.BlocksDeliverer = abc err := bc.onConnect(bc) // ...&#125; 这里先构建grpc client abc, err := bc.createClient(conn).Deliver(ctx).这里的createClient即上面构造broadcastClient时传入的参数d.conf.ABCFactory。这个参数可以回溯到gossip/service/gossip_service.go#Service构造时（这个方法在前面提及的InitializeChannel里被调用），指定ABCFactory: deliverclient.DefaultABCFactory。可见，这里对应的grpc方法即/orderer.AtomicBroadcast/Deliver. 1234567891011121314func DefaultABCFactory(conn *grpc.ClientConn) orderer.AtomicBroadcastClient &#123; return orderer.NewAtomicBroadcastClient(conn)&#125;func NewAtomicBroadcastClient(cc *grpc.ClientConn) AtomicBroadcastClient &#123; return &amp;atomicBroadcastClient&#123;cc&#125;&#125;func (c *atomicBroadcastClient) Deliver(ctx context.Context, opts ...grpc.CallOption) (AtomicBroadcast_DeliverClient, error) &#123; stream, err := grpc.NewClientStream(ctx, &amp;_AtomicBroadcast_serviceDesc.Streams[1], c.cc, "/orderer.AtomicBroadcast/Deliver", opts...) x := &amp;atomicBroadcastDeliverClient&#123;stream&#125; return x, nil&#125; 然后在connect方法里设置bc.BlocksDeliverer = abc,并且调用err = bc.afterConnect(conn, abc, cf, endpoint)，进而调用了err := bc.onConnect(bc).在上面的broadcastClient构造时传入参数onConnect为broadcastSetup,即执行requester.RequestBlocks(ledgerInfoProvider)。这里实际上是向orderer请求block，header为common.HeaderType_DELIVER_SEEK_INFO,同时分为两种情况，新的channel从oldest开始消费同步，而重启的channel则从指定的block开始（ledgerInfoProvider.LedgerHeight()）。以上为start值，stop选择math.MaxUint64,Behavior选择orderer.SeekInfo_BLOCK_UNTIL_READY.回顾前文，这里表明是一直同步。这里发出的Envelope也就是orderer接收。 12345678910111213141516171819202122232425262728293031323334func (b *blocksRequester) RequestBlocks(ledgerInfoProvider blocksprovider.LedgerInfo) error &#123; height, err := ledgerInfoProvider.LedgerHeight() if height &gt; 0 &#123; if err := b.seekLatestFromCommitter(height); err != nil&#123; return err &#125; &#125; else &#123; if err := b.seekOldest(); err != nil &#123; return err&#125; &#125; return nil&#125;func (b *blocksRequester) seekOldest() error &#123; seekInfo := &amp;orderer.SeekInfo&#123; Start: &amp;orderer.SeekPosition&#123;Type: &amp;orderer.SeekPosition_Oldest&#123;Oldest: &amp;orderer.SeekOldest&#123;&#125;&#125;&#125;, Stop: &amp;orderer.SeekPosition&#123;Type: &amp;orderer.SeekPosition_Specified&#123;Specified: &amp;orderer.SeekSpecified&#123;Number: math.MaxUint64&#125;&#125;&#125;, Behavior: orderer.SeekInfo_BLOCK_UNTIL_READY, &#125; //TODO- epoch and msgVersion may need to be obtained for nowfollowing usage in orderer/configupdate/configupdate.go msgVersion := int32(0) epoch := uint64(0) tlsCertHash := b.getTLSCertHash() env, err := utils.CreateSignedEnvelopeWithTLSBinding(common.HeaderType_DELIVER_SEEK_INFO, b.chainID, localmsp.NewSigner(), seekInfo, msgVersion, epoch, tlsCertHash) return b.client.Send(env)&#125;func (b *blocksRequester) seekLatestFromCommitter(height uint64) error &#123; seekInfo := &amp;orderer.SeekInfo&#123; Start: &amp;orderer.SeekPosition&#123;Type: &amp;orderer.SeekPosition_Specified&#123;Specified: &amp;orderer.SeekSpecified&#123;Number: height&#125;&#125;&#125;, Stop: &amp;orderer.SeekPosition&#123;Type: &amp;orderer.SeekPosition_Specified&#123;Specified: &amp;orderer.SeekSpecified&#123;Number: math.MaxUint64&#125;&#125;&#125;, Behavior: orderer.SeekInfo_BLOCK_UNTIL_READY, &#125; // ...&#125; 接下来转向消息的接收。真正处理DeliverResponse的地方在blocksprovider.go#DeliverBlocks，这里for循环判断!b.isDone()则使用msg, err := b.client.Recv()接收信息。core/deliverservice/client.go#Recv这个方法里，使用try方法，实际上就是重试机制，当失败时，并且没有达到重试阈值时（前文构造client时传入的参数backoffPolicy）则重新执行，这里包括先判断connection是否为nil，如果是则调用前面分析过的connect方法，然后调用action，也就是b.client.Recv()传入的方法参数，实际上是执行bc.BlocksDeliverer.Recv()。这里注意，在前面分析的afterConnect方法里，已经设置bc.BlocksDeliverer=abc(即orderer.AtomicBroadcast_DeliverClient)，所以这里就是从orderer的deliver grpc stream里读取，然后逐级返回到blocksprovider.go#DeliverBlocks。处理接收到的block这部分与gossip相关，留待后续再一起分析。]]></content>
      <categories>
        <category>Hyperledger Fabric</category>
      </categories>
      <tags>
        <tag>Hyperledger Fabric</tag>
        <tag>源码分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fabric 1.4源码分析 - orderer的deliver过程]]></title>
    <url>%2F2019%2F03%2F12%2Ffabric-source-orderer-deliver%2F</url>
    <content type="text"><![CDATA[在orderer端，deliver和broadcast是对等的，最初回溯到protos/orderer/ab.pb.go里的grpc定义，对应的grpc方法是/orderer.AtomicBroadcast/Deliver。对应源码orderer/common/server/server.go#Deliver -&gt; common/deliver/deliver.go#Handle -&gt; deliver.go#deliverBlocks。在deliverBlocks这个方法里，先反序列化cb.Envelope，获取并且校验Header，然后校验access control，判断client的identity证书是否过期，以及策略校验是否拥有读权限（policies.ChannelReaders，代码为orderer/common/server/server.go#Deliver : sf := msgprocessor.NewSigFilter(policies.ChannelReaders, chain)）。校验通过后，将envelope反序列化为ab.SeekInfo结构。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677// SeekInfo specifies the range of requested blocks to return// If the start position is not found, an error is immediately returned// Otherwise, blocks are returned until a missing block is encountered, then behavior is dictated by the SeekBehavior specified. // If BLOCK_UNTIL_READY is specified, the reply will block until the requested blocks are available// if FAIL_IF_NOT_READY is specified, the reply will return an error indicating that the block is not found. // To request that all blocks be returned indefinitely as they are created, behavior should be set to BLOCK_UNTIL_READY and the stop should be set to specified with a number of MAX_UINT64type SeekInfo struct &#123; Start *SeekPosition Stop *SeekPosition Behavior SeekInfo_SeekBehavior &#125;func (h *Handler) deliverBlocks(ctx context.Context, srv *Server, envelope *cb.Envelope) error &#123; // ... 校验，略 // 根据seekInfo.Stop计算range的终止block number cursor, number := chain.Reader().Iterator(seekInfo.Start) defer cursor.Close() switch stop := seekInfo.Stop.Type.(type) &#123; case *ab.SeekPosition_Oldest: stopNum = number case *ab.SeekPosition_Newest: stopNum = chain.Reader().Height() - 1 case *ab.SeekPosition_Specified: stopNum = stop.Specified.Number &#125; for &#123; // 如果range超越当前到chain高度，fail fast if seekInfo.Behavior == ab.SeekInfo_FAIL_IF_NOT_READY &#123; if number &gt; chain.Reader().Height()-1 &#123; return srv.SendStatusResponse(cb.Status_NOT_FOUND) &#125; &#125; iterCh := make(chan struct&#123;&#125;) go func() &#123; // 读取下一个block block, status = cursor.Next() close(iterCh) &#125;() select &#123; case &lt;-ctx.Done(): // "context finished before block retrieved" return errors.Wrapf(ctx.Err(), "context finished before block retrieved") case &lt;-erroredChan: // "Aborting deliver for request because of background error" return srv.SendStatusResponse(cb.Status_SERVICE_UNAVAILABLE) case &lt;-iterCh: // Iterator has set the block and status vars，完成读取 &#125; // increment block number to support FAIL_IF_NOT_READY deliver behavior number++ // 再次通过权限校验 access control if err := accessControl.Evaluate(); err != nil &#123; return srv.SendStatusResponse(cb.Status_FORBIDDEN) &#125; // 逐个block发送 if err := srv.SendBlockResponse(block); err != nil &#123; return err &#125; if stopNum == block.Header.Number &#123; break &#125; &#125; // 全部block发送完成后，再回复当前deliver rpc请求 if err := srv.SendStatusResponse(cb.Status_SUCCESS); err != nil &#123; return err &#125; return nil&#125; 在配置文件orderer.yaml里指定了General.LedgerType: file(可选有file，json，ram三种模式)，这里以file为例分析orderer的本地账本，实现在common/ledger/blockledger/file/impl.go。file/imple.go#Next方法里调用的i.commonIterator.Next()，如注释里所述，将会block直到可以获取到下一个block，因此只要client的请求里seekInfo.stop为最大值MAX_UINT64，SeekInfo_SeekBehavior选择BLOCK_UNTIL_READY，则新产生的block会源源不断的发送到client。(使用标准包的sync.cond)，1234567891011121314151617181920212223242526272829303132333435363738394041func (fl *FileLedger) Iterator(startPosition *ab.SeekPosition) (blockledger.Iterator, uint64) &#123; var startingBlockNumber uint64 switch start := startPosition.Type.(type) &#123; case *ab.SeekPosition_Oldest: // 结合deliverBlocks里的stopNum，得知SeekPosition_Oldest返回最早的一个block，start=stop=0 startingBlockNumber = 0 case *ab.SeekPosition_Newest: // 结合deliverBlocks里的stopNum，得知SeekPosition_Newest返回最新的一个block，start=stop=chain.height-1 info, err := fl.blockStore.GetBlockchainInfo() newestBlockNumber := info.Height - 1 startingBlockNumber = newestBlockNumber case *ab.SeekPosition_Specified: startingBlockNumber = start.Specified.Number height := fl.Height() if startingBlockNumber &gt; height &#123; return &amp;blockledger.NotFoundErrorIterator&#123;&#125;, 0 &#125; default: return &amp;blockledger.NotFoundErrorIterator&#123;&#125;, 0 &#125; iterator, err := fl.blockStore.RetrieveBlocks(startingBlockNumber) return &amp;fileLedgerIterator&#123;ledger: fl, blockNumber: startingBlockNumber, commonIterator: iterator&#125;, startingBlockNumber&#125;// Next blocks until there is a new block available, or until Close is called.// It returns an error if the next block is no longer retrievable.func (i *fileLedgerIterator) Next() (*cb.Block, cb.Status) &#123; // 使用标准包sync.cond result, err := i.commonIterator.Next() if err != nil &#123; logger.Error(err) return nil, cb.Status_SERVICE_UNAVAILABLE &#125; // Cover the case where another thread calls Close on the iterator. if result == nil &#123; return nil, cb.Status_SERVICE_UNAVAILABLE &#125; return result.(*cb.Block), cb.Status_SUCCESS&#125; 具体跟踪下这个等待对新block的机制，impl.go#Next : result, err := i.commonIterator.Next()-&gt;fsblkstorage/blocks_itr.go#Next() : itr.waitForBlock(itr.blockNumToRetrieve)-&gt;fsblkstorage/blocks_itr.go#waitForBlock()，这里执行了itr.mgr.cpInfoCond.Wait()，就是等待新block。 而相应的在添加block时，以solo方式为例，orderer/consensus/solo/consensus.go#main : ch.support.WriteBlock(block, nil)-&gt;orderer/common/multichannel/blockwriter.go#WriteBlock : bw.commitBlock(encodedMetadataValue)-&gt;blockwriter.go#commitBlock : bw.support.Append(bw.lastBlock)-&gt;common/ledger/blockledger/file/impl.go#Append : fl.blockStore.AddBlock(block)-&gt;fsblkstorage/fs_blockstore.go#AddBlock-&gt;fsblkstorage/blockfile_mgr.go#addBlock : mgr.updateCheckpoint(newCPInfo)-&gt;/fsblkstorage/blockfile_mgr.go#updateCheckpoint里，执行了mgr.cpInfoCond.Broadcast()，也就是当写入block后，通知到前面的wait，实现了产生每一个新block，立刻deliver该block的逻辑。]]></content>
      <categories>
        <category>Hyperledger Fabric</category>
      </categories>
      <tags>
        <tag>Hyperledger Fabric</tag>
        <tag>源码分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A Kafka-based Ordering Service for Fabric （orderer的应用kafka设计思想）学习笔记]]></title>
    <url>%2F2019%2F03%2F11%2Ffabric-orderer-kafka-design%2F</url>
    <content type="text"><![CDATA[A Kafka-based Ordering Service for Fabric这篇文章是orderer的kafka设计文档，详细介绍orderer应用kafka的设计思想演进过程，从基本点逐步深入，由问题到解决方案，值得阅读。这里列举学习过程中的笔记。 Solution 0（S0） ： 单个tx（transaction）构成block。 Problem 1（P1） ： 单个block的size很小，当网络有大量tx时会产生大量的block。而OSN（ordering service node）需要对block进行打包，签名，client需要解包，验证签名。overhead太大，效率太低。 S1 ：多个tx打包成一个block。 P2 : 但是需要缓存到指定数量的tx，如果网络tx太少，可能需要等很长时间才产生一个block，tx需要等待很久才得以确认。 S2 ： 增加时钟，如果指定时间内没有达到数量要求，时钟到时切分block。 P3 ： 各个OSN时钟不同步，可能导致block间切分的位置差异。 S3 ： OSN加入TTC-X（Time to Cut block-X），时钟到时发送该message到kafka，各个OSN收到改消息后切分block。 P4 ： Deliver服务需要读取指定block。 S4a ： block头部加上metadata，从block X-1可以知道block X的开始的kafka offset。OSN从该offset开始重读kafka，replay重新构建block。 P4a ： 如果client缺少block X-1，就无法得知block X的开始offset。 S4b ： OSN保存table，block number &lt;-&gt; offset number。 P5 ： 读取block X需要从kafka重读消息，重新打包，签名。 P6 ： 网络中存在重复的消息，例如re-order的消息 S5a ： kafka partition 0 保存tx，partition 1保存打包后的block，简历partition 1的table，block number &lt;-&gt; offset number。如果需要读取blcok X，则查表，从partition 1读取相应offset的block。但是，需要每次查表，而且无法解决P6。partiton 1中的每个block会被每个OSN都发送一次，大量重复的block。 S6a ：OSN发送前判断partition 1中是否有重复的block number，如果已存在则不在发送。但是所有的OSN之间无法做到严格同步，可能有多个block副本（来自不同的OSN的同一个block同时发送）处在in-flight状态，仍然没有解决重复的问题。 S6b ： 选举OSN leader，只有leader才能发送到partition 1。但是，如果就leader发送block X后crash，选举的新leader重发block X，导致重复（旧leader发的block X处在in flight状态）。无法使得block number = offset number，仍然需要存表查表。 S6c ： 使用kafka的log压缩特性，保留key相同（可以使用block number X作为key）只有一个offset，删除冗余的。但是，OSN要保留table，block number &lt;-&gt; offset number，并且log压缩后删除旧值，表里如果更新不及时会读取到旧offset（实际上已经被删除）。而且压缩后的block与offset间是乱序的。 S5b,S6d : OSN把block存储本地账本。无需使用table，从tx构建账本链可以保证顺序性，而且deliver也可以复用orderer broadcast代码。 The OSN will need to keep track of the last offset number it read though, just so that it knows where to seek to when consuming from Kafka upon reconnection. A downside of serving Deliver requests from the local ledger could be that it would be slower than serving them straight from Kafka. But we never serve straight from Kafka; there’s always some processing happening on the OSNs. (其他的方案在OSN也会引入开销) Overall an ordering service that uses a single partition (per chain) for incoming client transactions and TTC-X messages (as shown in Solution 3), and which stores the resulting blocks in a local ledger (again, per chain) strikes a nice balance between performance and complexity.]]></content>
      <categories>
        <category>Hyperledger Fabric</category>
      </categories>
      <tags>
        <tag>Hyperledger Fabric</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fabric 1.4源码分析 - orderer的kafka实现]]></title>
    <url>%2F2019%2F03%2F07%2Ffabric-source-orderer-kafka%2F</url>
    <content type="text"><![CDATA[在instantiate（8）里提到，orderer总共有solo和kafka,etcdraft三种方式，并且介绍了单orderer节点的solo方式。这里继续补充介绍多orderer节点下采用的kafka方式。kafka实现的代码放在orderer/consensus/kafka/chain.go里，与solo一样实现了orderer/common/broadcast/broadcast.go#Consenter这个接口。在orderer的处理消息时orderer/common/broadcast/broadcast.go#Handle，调用了其Order方法。12345678910111213141516171819202122232425262728293031323334353637383940func (chain *chainImpl) Order(env *cb.Envelope, configSeq uint64) error &#123; return chain.order(env, configSeq, int64(0))&#125;func (chain *chainImpl) order(env *cb.Envelope, configSeq uint64, originalOffset int64) error &#123; marshaledEnv, err := utils.Marshal(env) if !chain.enqueue(newNormalMessage(marshaledEnv, configSeq, originalOffset)) &#123; return fmt.Errorf("cannot enqueue") &#125; return nil&#125;// enqueue accepts a message and returns true on acceptance, or false otheriwse.func (chain *chainImpl) enqueue(kafkaMsg *ab.KafkaMessage) bool &#123; select &#123; case &lt;-chain.startChan: // The Start phase has completed select &#123; case &lt;-chain.haltChan: // The chain has been halted, stop here return false default: // The post path payload, err := utils.Marshal(kafkaMsg) message := newProducerMessage(chain.channel, payload) if _, _, err = chain.producer.SendMessage(message); err != nil &#123; return false &#125; return true &#125; default: // Not ready yet return false &#125;&#125;func newProducerMessage(channel channel, pld []byte) *sarama.ProducerMessage &#123; return &amp;sarama.ProducerMessage&#123; Topic: channel.topic(), Key: sarama.StringEncoder(strconv.Itoa(int(channel.partition()))), Value: sarama.ByteEncoder(pld), &#125;&#125; 首先构建了消息实体orderer/consensus/kafka/chain.go:KafkaMessage，入参为传递的消息cb.Envelope，当前配置序列号configSeq，固定的originalOffset值为0(这个值的含义后面会提到，表示的是是否当前消息是重新发送重排，值为0则说明当前消息是新消息)。然后构建传递kafka消息sarama.ProducerMessage，这里kafka相关的使用第三方库Shopify/sarama。消息里指定了topic，由partition构建key，然后就是将刚才构建的消息作为payload。后面可以看到topic和partition都是channel的配置参数（初始化或者后面的更新）。这里可以看出，所有的kafka消息的key都是相同的，意味这所有消息都发送到同一个partition内，按照key的算法也就是channel.partition()。这里保证了单个orderer发出的envelop的有序性，但同时从全局来说，只使用一个partition，并没有充分利用kafka多partition的带来的高性能。然后使用sarama.SyncProducer发送消息chain.producer.SendMessage。 接下来分析sarama.SyncProducer的初始化，以及相应的kafka consumer消费过程。kafka的初始化方法是orderer/consensus/kafka/chain.go#startThread。从这个一直回溯到chainsupport.go#start，总共有两个地方调用。一是registar.go#NewRegistrar，这里从ledgerFactory里的记录找出existingChains重建；二是registar.go#newChain,接收到消息的payload.Header.ChannelHeader.Type为cb.HeaderType_ORDERER_TRANSACTION时新建。这里以新建chain（channel）为例。newChain方法里调用了newChainSupport,里面初始化chainsupport的Chain参数cs.Chain, err = consenter.HandleChain(cs, metadata)。这里的consenter是consensus.go#Consenter接口,这里的实现类是orderer/consensus/kafka/consenter.go:consenterImpl.1234567891011121314151617181920212223242526272829303132333435363738394041424344454647func newChainSupport( registrar *Registrar, ledgerResources *ledgerResources, consenters map[string]consensus.Consenter, signer crypto.LocalSigner,) *ChainSupport &#123; // Read in the last block and metadata for the channel lastBlock := blockledger.GetBlock(ledgerResources, ledgerResources.Height()-1) metadata, err := utils.GetMetadataFromBlock(lastBlock, cb.BlockMetadataIndex_ORDERER) // Construct limited support needed as a parameter for additional support cs := &amp;ChainSupport&#123; ledgerResources: ledgerResources, LocalSigner: signer, cutter: blockcutter.NewReceiverImpl(ledgerResources), &#125; // Set up the msgprocessor cs.Processor = msgprocessor.NewStandardChannel(cs, msgprocessor.CreateStandardChannelFilters(cs)) // Set up the block writer cs.BlockWriter = newBlockWriter(lastBlock, registrar, cs) // Set up the consenter cs.Chain, err = consenter.HandleChain(cs, metadata) return cs&#125;// HandleChain creates/returns a reference to a consensus.Chain object for the given set of support resources. Implements the consensus.Consenter interface. // Called by consensus.newChainSupport(), which is itself called by multichannel.NewManagerImpl() when ranging over the ledgerFactory's existingChains.func (consenter *consenterImpl) HandleChain(support consensus.ConsenterSupport, metadata *cb.Metadata) (consensus.Chain, error) &#123; lastOffsetPersisted, lastOriginalOffsetProcessed, lastResubmittedConfigOffset := getOffsets(metadata.Value, support.ChainID()) return newChain(consenter, support, lastOffsetPersisted, lastOriginalOffsetProcessed, lastResubmittedConfigOffset)&#125;func getOffsets(metadataValue []byte, chainID string) (persisted int64, processed int64, resubmitted int64) &#123; if metadataValue != nil &#123; // Extract orderer-related metadata from the tip of the ledger first kafkaMetadata := &amp;ab.KafkaMetadata&#123;&#125; proto.Unmarshal(metadataValue, kafkaMetadata); return kafkaMetadata.LastOffsetPersisted, kafkaMetadata.LastOriginalOffsetProcessed, kafkaMetadata.LastResubmittedConfigOffset &#125; return sarama.OffsetOldest - 1, int64(0), int64(0) // default&#125; 从上一个区块blockledger.GetBlock(ledgerResources, ledgerResources.Height()-1)获取元数据，也就是BlockMetadata.Metadata内key为cb.BlockMetadataIndex_SIGNATURES的值，在instantiate（8）里提及其他三种。从getOffsets里看到，如果说当前channel已经存在也消费过数据，例如重启回复的情况下，则使用的是metadata里记录的值。如果是新建channel，则使用了默认LastOffsetPersisted,LastOriginalOffsetProcessed,LastResubmittedConfigOffset这三个值分别取sarama.OffsetOldest - 1,0,0。具体参数的含义后面分析。 接着调用chain.go#newChain，里面主要是构建了orderer/consensus/kafka/chain.go:chainImpl这个实体。其中需要关注的是chainImpl.channel这个参数初始化为newChannel(support.ChainID(), defaultPartition).即kafka的topic为support.ChainID()（即channel名），partition为固定值0，这也就是前面提到的channel的topic和partition。12345678910// channel.goconst defaultPartition = 0// Returns a new channel for a given topic name and partition number.func newChannel(topic string, partition int32) channel &#123; return &amp;channelImpl&#123; tpc: fmt.Sprintf("%s", topic), prt: partition, &#125;&#125; 下面再具体分析chain.go#startThread123456789101112131415161718192021// Called by Start().func startThread(chain *chainImpl) &#123; // Set up the producer chain.producer, err = setupProducerForChannel(chain.consenter.retryOptions(), chain.haltChan, chain.SharedConfig().KafkaBrokers(), chain.consenter.brokerConfig(), chain.channel) // Have the producer post the CONNECT message if err = sendConnectMessage(chain.consenter.retryOptions(), chain.haltChan, chain.producer, chain.channel); err != nil &#123;&#125; // Set up the parent consumer chain.parentConsumer, err = setupParentConsumerForChannel(chain.consenter.retryOptions(), chain.haltChan, chain.SharedConfig().KafkaBrokers(), chain.consenter.brokerConfig(), chain.channel) // Set up the channel consumer chain.channelConsumer, err = setupChannelConsumerForChannel(chain.consenter.retryOptions(), chain.haltChan, chain.parentConsumer, chain.channel, chain.lastOffsetPersisted+1) chain.doneProcessingMessagesToBlocks = make(chan struct&#123;&#125;) close(chain.startChan) // Broadcast requests will now go through chain.errorChan = make(chan struct&#123;&#125;) // Deliver requests will also go through chain.processMessagesToBlocks() // Keep up to date with the channel&#125; setupProducerForChannel尝试用配置好的参数创建kafka的producer(sarama.SyncProducer)，并且加上了失败重试机制。配置在orderer.yaml文件里。 sendConnectMessage使用刚创建的kafka producer发送ab.KafkaMessage_Connect消息，payload为nil，只是为了保证配置正确能正常返送。（Post a CONNECT message to the channel using the given retry options. This prevents the panicking that would occur if we were to set up a consumer and seek on a partition that hadn’t been written to yet. ）这里也加上失败重连机制。 setupParentConsumerForChannel和setupChannelConsumerForChannel构造sarama.PartitionConsumer,只消费指定partition的数据，这里消费的就是初始化时设置的channle.Partition(),实际上是const defaultPartition = 0。失败重试。 processMessagesToBlocks真正的sarama.PartitionConsumer消费数据，处理数据。这个方法里有比较多的select-case，首先是处理case kafkaErr := &lt;-chain.channelConsumer.Errors():，这里错误分两种，一种是sarama.ErrOffsetOutOfRange:,这类错误无法通过自动重试回复，则重新发送连接消息推进offset,go sendConnectMessage(chain.consenter.retryOptions(), chain.haltChan, chain.producer, chain.channel)；另一种错误则是可以通过自带的重试机制回复，这里有两个变量和case分支。1)case &lt;-topicPartitionSubscriptionResumed:错误时添加监听器，成功重连后打印日志；2）case &lt;-deliverSessionTimedOut:重连超时，则再次发送连接消息go sendConnectMessage(...)。这个select-case里交织着处理这两个变量分支的代码。 接下来是重要的两个case。第一个case &lt;-chain.timer:这里是在当前channel设置的timer到期后，发送&amp;ab.KafkaMessage_TimeToCut消息到kafka通知所有orderer，这里的参数为下一个block的number（chain.lastCutBlockNumber+1）。第二个case in, ok := &lt;-chain.channelConsumer.Messages():里kafka收到的消息类型总共有三种。 1234567891011121314for &#123; select &#123; case in, ok := &lt;-chain.channelConsumer.Messages(): switch msg.Type.(type) &#123; case *ab.KafkaMessage_Connect: _ = chain.processConnect(chain.ChainID()) case *ab.KafkaMessage_TimeToCut: if err := chain.processTimeToCut(msg.GetTimeToCut(), in.Offset); err != nil &#123;...&#125; case *ab.KafkaMessage_Regular: if err := chain.processRegular(msg.GetRegular(), in.Offset); err != nil &#123;...&#125; &#125; case &lt;-chain.timer: if err := sendTimeToCut(chain.producer, chain.channel, chain.lastCutBlockNumber+1, &amp;chain.timer); err != nil &#123;...&#125; &#125; case *ab.KafkaMessage_Connect:前面提到的在startThread启动时发送连接消息和遇到错误时尝试重联都回发送此类消息以推进offset。这里对这类消息不需要处理，仅仅作为记录。 case *ab.KafkaMessage_TimeToCut:判断消息里要求cut的blocknum是否是当前节点本地block数据的下一个block，如果是的话，直接将当前汇集的batch切割，剩下的跟instantiate（8）里solo的流程一致，写入block。这里有所区别的是这里要提交kafka的metadata到block里，而这部分metadata写入了bw.lastBlock.Metadata.Metadata[cb.BlockMetadataIndex_ORDERER里，也就是上面提到在重启chain时候从block的BlockMetadata.Metadata内取key为cb.BlockMetadataIndex_SIGNATURES的值来获取kafka消费消息。回顾对比solo方式，在调用chain.WriteBlock()时，传入的第二个参数为nil。 1234567891011121314151617181920212223242526272829303132333435 func (chain *chainImpl) processTimeToCut(ttcMessage *ab.KafkaMessageTimeToCut, receivedOffset int64) error &#123; ttcNumber := ttcMessage.GetBlockNumber() if ttcNumber == chain.lastCutBlockNumber+1 &#123; chain.timer = nil batch := chain.BlockCutter().Cut() block := chain.CreateNextBlock(batch) metadata := utils.MarshalOrPanic(&amp;ab.KafkaMetadata&#123; LastOffsetPersisted: receivedOffset, LastOriginalOffsetProcessed: chain.lastOriginalOffsetProcessed, &#125;) chain.WriteBlock(block, metadata) chain.lastCutBlockNumber++ return nil &#125; else if ttcNumber &gt; chain.lastCutBlockNumber+1 &#123; return fmt.Errorf(...) &#125; return nil&#125;func (bw *BlockWriter) WriteBlock(block *cb.Block, encodedMetadataValue []byte) &#123; bw.committingBlock.Lock() bw.lastBlock = block go func() &#123; defer bw.committingBlock.Unlock() bw.commitBlock(encodedMetadataValue) &#125;()&#125;func (bw *BlockWriter) commitBlock(encodedMetadataValue []byte) &#123; // Set the orderer-related metadata field if encodedMetadataValue != nil &#123; bw.lastBlock.Metadata.Metadata[cb.BlockMetadataIndex_ORDERER] = utils.MarshalOrPanic(&amp;cb.Metadata&#123;Value: encodedMetadataValue&#125;) &#125; // 省略...&#125; case *ab.KafkaMessage_Regular:处理消息（envelop） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147func (chain *chainImpl) processRegular(regularMessage *ab.KafkaMessageRegular, receivedOffset int64) error &#123; // When committing a normal message, we also update `lastOriginalOffsetProcessed` with `newOffset`. It is caller's responsibility to deduce correct value of `newOffset` based on following rules: // - if Resubmission is switched off, it should always be zero // - if the message is committed on first pass, meaning it's not re-validated and re-ordered, this value should be the same as current `lastOriginalOffsetProcessed` // - if the message is re-validated and re-ordered, this value should be the `OriginalOffset` of that Kafka message, so that `lastOriginalOffsetProcessed` is advanced // 其实，这个方法就是完成3件事情。 // 1）如果不需要切割，并且如果chain.timer没有设置则重设timer（用以倒计时发送*ab.KafkaMessage_TimeToCut）； // 2）切割，更新KafkaMetadata.LastOffsetPersisted为这个block最后envelope的offset； // 3）更新chain.lastOriginalOffsetProcessed，并且用作KafkaMetadata.LastOriginalOffsetProcessed。 commitNormalMsg := func(message *cb.Envelope, newOffset int64) &#123; batches, pending := chain.BlockCutter().Ordered(message) if len(batches) == 0 &#123; // If no block is cut, we update the `lastOriginalOffsetProcessed`, start the timer if necessary and return chain.lastOriginalOffsetProcessed = newOffset if chain.timer == nil &#123; // configtx.yaml里的Orderer: &amp;OrdererDefaults.BatchTimeout chain.timer = time.After(chain.SharedConfig().BatchTimeout()) &#125; return &#125; chain.timer = nil offset := receivedOffset if pending || len(batches) == 2 &#123; offset-- &#125; else &#123; chain.lastOriginalOffsetProcessed = newOffset &#125; // Commit the first block block := chain.CreateNextBlock(batches[0]) metadata := utils.MarshalOrPanic(&amp;ab.KafkaMetadata&#123; // LastOffsetPersisted记录的是这个block的最后envelope的offset，也就是消费的最后offset LastOffsetPersisted: offset, // LastOriginalOffsetProcessed记录指的是截止该block，originalOffset小于这个值的所有message都已经被排序，即最新处理的originalOffset LastOriginalOffsetProcessed: chain.lastOriginalOffsetProcessed, // LastResubmittedConfigOffset这个记录的是最近提交的重新校验排序的configMsg的offset LastResubmittedConfigOffset: chain.lastResubmittedConfigOffset, &#125;) chain.WriteBlock(block, metadata) chain.lastCutBlockNumber++ // Commit the second block if exists if len(batches) == 2 &#123;...&#125; &#125; seq := chain.Sequence() env := &amp;cb.Envelope&#123;&#125; // 这部分主要是为了兼容v1.1前的版本，前面的版本不支持re-submission // 这里的配置在configtx.yaml里，V1_1:true则`chain.SharedConfig().Capabilities().Resubmission()`返回为true。这里分析默认都使用V1.1后的版本 //Capabilities: // Orderer: &amp;OrdererCapabilities // V1_1: true if regularMessage.Class == ab.KafkaMessageRegular_UNKNOWN || !chain.SharedConfig().Capabilities().Resubmission() &#123;...&#125; switch regularMessage.Class &#123; case ab.KafkaMessageRegular_NORMAL: // This is a message that is re-validated and re-ordered // 普通消息的OriginalOffset为0，因为configSeq改变被重新验证和加入重排序 if regularMessage.OriginalOffset != 0 &#123; // chain.lastOriginalOffsetProcessed记录着最近的处理的重排消息的offset，意味着小于这个的消息都已经处理过，故返回不再重复处理。 // 从下面的`regularMessage.ConfigSeq &lt; seq`可知，这是因为有多个orderer节点，每个节点都会往kafka重发消息，故可能会存在多条同样的消息（OriginalOffset相同但offset不同） if regularMessage.OriginalOffset &lt;= chain.lastOriginalOffsetProcessed &#123; return nil &#125; // 未处理的重排消息按照正常流程往下处理 &#125; // The config sequence has advanced，当前的配置已经更新，因此需要重新验证，重发kafka进行re-order if regularMessage.ConfigSeq &lt; seq &#123; configSeq, err := chain.ProcessNormalMsg(env) if err != nil &#123; return fmt.Errorf("discarding bad normal message because = %s", err) &#125; // 新配置下重新校验通过 // For both messages that are ordered for the first time or re-ordered, we set original offset to current received offset and re-order it. if err := chain.order(env, configSeq, receivedOffset); err != nil &#123;...&#125; return nil &#125; // 下面的commitNormalMsg方法里可以看到用offset更新chain.lastOriginalOffsetProcessed， // 因此，offset或者保持原值chain.lastOriginalOffsetProcessed（当前消息不是re-order消息），或者采用当前消息的OriginalOffset（当前消息是re-order消息） offset := regularMessage.OriginalOffset if offset == 0 &#123; offset = chain.lastOriginalOffsetProcessed &#125; commitNormalMsg(env, offset) case ab.KafkaMessageRegular_CONFIG: // This is a message that is re-validated and re-ordered，同上 if regularMessage.OriginalOffset != 0 &#123; // 同上, normalMsg if regularMessage.OriginalOffset &lt;= chain.lastOriginalOffsetProcessed &#123; return nil &#125; // lastResubmittedConfigOffset这个记录的是最近提交的被重新校验排序的configMsg的offset，如果消息的OriginalOffset等于该值，并且configSeq也相等，则说明已经本地已更新了配置，并且是最新的配置，可以关闭doneReprocessingMsgInFlight这个channel，继续消费（后面详述） if regularMessage.OriginalOffset == chain.lastResubmittedConfigOffset &amp;&amp; // This is very last resubmitted config message regularMessage.ConfigSeq == seq &#123; // AND we don't need to resubmit it again close(chain.doneReprocessingMsgInFlight) // Therefore, we could finally close the channel to unblock broadcast &#125; // Somebody resubmitted message at offset X, whereas we didn't. This is due to non-determinism where // that message was considered invalid by us during revalidation, however somebody else deemed it to // be valid, and resubmitted it. We need to advance lastResubmittedConfigOffset in this case in order // to enforce consistency across the network. if chain.lastResubmittedConfigOffset &lt; regularMessage.OriginalOffset &#123; chain.lastResubmittedConfigOffset = regularMessage.OriginalOffset &#125; &#125; // The config sequence has advanced if regularMessage.ConfigSeq &lt; seq &#123; // ProcessConfigUpdateMsg will attempt to apply the config impetus msg to the current configuration, and if successful // return the resulting config message and the configSeq the config was computed from. If the config impetus message // is invalid, an error is returned. // 在这个方法里，将envelop里指定的配置尝试作用于本地 configEnv, configSeq, err := chain.ProcessConfigMsg(env) // 同上, normalMsg。For both messages that are ordered for the first time or re-ordered, we set original offset to current received offset and re-order it. if err := chain.configure(configEnv, configSeq, receivedOffset); err != nil &#123; return fmt.Errorf("error re-submitting config message because = %s", err) &#125; // 更新lastResubmittedConfigOffset，最新的重新提交configMsg的offset chain.lastResubmittedConfigOffset = receivedOffset // Keep track of last resubmitted message offset chain.doneReprocessingMsgInFlight = make(chan struct&#123;&#125;) // Create the channel to block ingress messages return nil &#125; // 同上，normalMsg offset := regularMessage.OriginalOffset if offset == 0 &#123; offset = chain.lastOriginalOffsetProcessed &#125; // 同上，commitNormalMsg commitConfigMsg(env, offset) return nil&#125; 回顾instantiate（8）里orderer/common/broadcast/broadcast.go#Handle提到的，orderer的broacast服务调用processor.WaitReady(), 这里select-case case &lt;-chain.doneReprocessingMsgInFlight:,也就是说，在configMsg进行re-order时，不再对外提供服务接收处理新数据，直到最新的配置已更新，关闭doneReprocessingMsgInFlight这个channel。 1234567891011121314151617181920212223242526272829303132// Handle starts a service thread for a given gRPC connection and services the broadcast connectionfunc (bh *handlerImpl) Handle(srv ab.AtomicBroadcast_BroadcastServer) error &#123;for &#123; msg, err := srv.Recv() // ... if err = processor.WaitReady(); err != nil &#123; return srv.Send(&amp;ab.BroadcastResponse&#123;Status: cb.Status_SERVICE_UNAVAILABLE, Info: err.Error()&#125;) &#125; if !isConfig &#123; configSeq, err := processor.ProcessNormalMsg(msg) err = processor.Order(msg, configSeq) &#125; else &#123; // isConfig config, configSeq, err := processor.ProcessConfigUpdateMsg(msg) err = processor.Configure(config, configSeq) &#125; err = srv.Send(&amp;ab.BroadcastResponse&#123;Status: cb.Status_SUCCESS&#125;)&#125;&#125;func (chain *chainImpl) WaitReady() error &#123; select &#123; case &lt;-chain.startChan: // The Start phase has completed select &#123; case &lt;-chain.haltChan: // The chain has been halted, stop here return error // Block waiting for all re-submitted messages to be reprocessed case &lt;-chain.doneReprocessingMsgInFlight: return nil &#125; &#125;&#125; 总结来说，fabric的共识算法需要解决两个问题，一是交易消息的有序性，二是恶意节点的拜占庭问题。当前提供的两种共识机制单节点solo和多节点kafka。kafka在fabric的应用中，始终使用了单个partition，这样削弱了kafka本身提供的多分区带来的高性能。这样的考虑处于要最大程度保证交易排序和最后执行的有序性，虽然在v1.1版本后提供的可以re-validate和re-order特性在一定程度上违背了这种强有序性，但是在fabric里共识更重要的是全局一致性，即关键的是block是有序的，而且是一致的，而不在乎顺序是怎样的。但是最重要的是，kafka本身的共识算法并不能解决拜占庭问题，无法容忍网络里恶意节点的存在。这个通过fabric本身的准入审核，签名和策略等机制可以一定程度上预防这个问题。对于拜占庭问题，相对成熟的算法是pbft。在fabric的v0.6版本提供pbft共识机制，v1.0后采用了分割出orderer后，目前仍未提供pbft的相关实现。这个值得后续持续关注。 Hyperledger Fabric Ordering ServiceHyperLeger Fabric - Bringing up a Kafka-based Ordering ServiceHyperLeger Fabric - Introduction - ConsensusHyperledger Fabric Model - ConsensusA Kafka-based Ordering Service for Fabric : 详细介绍fabri orderer应用kafka的设计思想演进过程]]></content>
      <categories>
        <category>Hyperledger Fabric</category>
      </categories>
      <tags>
        <tag>Hyperledger Fabric</tag>
        <tag>源码分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[共识算法(1) - paxos & raft]]></title>
    <url>%2F2019%2F03%2F06%2Fconsensus-paxos-raft%2F</url>
    <content type="text"><![CDATA[Paxos 算法的介绍在wikipedia里解释很清楚，可以结合example里的例子加深理解。概括来说，paxos分为prepare和accept两个阶段，将角色分为proposer，acceptor和learner，最基本的标准是majority quorum。每次proposer以（sequence number n, value v）的形式向acceptor提议（n为每个proposer节点自行管理的单增值，注意，这里会有多个proposer在并行的提议）。如果将prepare request到最终accept视作一轮，那么最终accept的提议是来自n最高的proposer，但是v是在prepare阶段最早得到quorum accept的，因为acceptor收到prepare request作出这样的promise（即承诺不再accept比该proposal的n更小或相等的值），并且返回收到并回复的prepare request的值。 Paxos (computer science) - wiki Paxos By Example Raft 大致思路是，raft角色分为leader，follower和candidate（在leader election阶段）。每个leader的当选任期有全局统一且历史递增的term number。所有的写请求被路由或者follow重定向提交到leader，相当于将proposer缩减成唯一一个leader，leader给每个请求log entry添加index。整个过程也分为两个阶段，prepare阶段leader将log entry同步到所有follower，收到majority quorum回复同步后进入commit阶段，leader先commit本地，发送commit消息到follower，并且回复client。follower再commit本地执行，commit该index也意味着之前小于该index的消息都已同步并且全网一致。leader失败后则进入新一轮的选举（term值增加），最新最全数据的节点当选，新leader将其本地数据全网同步，follower相应补充或者剪枝，保证全局数据一致。 The Raft Consensus Algorithm : Raft的github page Raft example : 国外的动画，非常形象 Understanding the Raft consensus algorithm: an academic article summary Raft 共识算法 ： 可以参看这篇etcd里介绍raft共识算法的节点异常情况 综述 paxos最早的理论基础，但是工程上实现难度比较大，所以zookeeper（07年）自己设计了zab。13年raft出现，借鉴了zab的设计思想，同时工程上相对paxos更容易实现和理解。这些算法大量用于分布式应用及组件设计，假设前提是没有恶意节点，但并不能解决拜占庭将军问题（Paxos可以针对性扩展为Byzantine Paxos）。 分布式系统的基石：深入浅出共识算法 ： 共识算法的综述，涵盖了paxos，raft，zab Zab vs. Paxos]]></content>
      <categories>
        <category>Consensus &amp; Cryptography</category>
      </categories>
      <tags>
        <tag>Consensus</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fabric 1.4源码分析 - chaincode install(2）endorsor的install流程]]></title>
    <url>%2F2019%2F02%2F25%2Ffabric-source-chaincode-install-endorsor%2F</url>
    <content type="text"><![CDATA[endorsor端流程也一致，入口都是core/endorser/endorser.go#ProcessProposal,具体不再重复展开，可以参看instantiate(3)。主要的区别在于core/endorser/endorser.go#callChaincode里，(“lscc”, “deploy”)的if条件不再满足，不需要进入（回顾一下，这里主要是launch用户application chaincode的容器，执行application chaincode的Init方法等，在install阶段当然不需要）。真正的区别在于lscc.go#Invoke的select-case里，进入的是INSTALL分支（部署chaincode时进入的是DEPLOY分支）。INSTALL首先执行lscc.policyChecker.CheckPolicyNoChannel(mgmt.Admins, sp)检查策略，这部分以后详述。从名字也印证出install是不指定channel的。然后执行lscc的executeInstall方法. 1234567891011121314151617181920212223242526272829303132// executeInstall implements the "install" Invoke transactionfunc (lscc *lifeCycleSysCC) executeInstall(stub shim.ChaincodeStubInterface, ccbytes []byte) error &#123; ccpack, err := ccprovider.GetCCPackage(ccbytes) // Get any statedb artifacts from the chaincode package, e.g. couchdb index definitions statedbArtifactsTar, err := ccprovider.ExtractStatedbArtifactsFromCCPackage(ccpack) if err = isValidStatedbArtifactsTar(statedbArtifactsTar); err != nil &#123; return InvalidStatedbArtifactsErr(err.Error()) &#125; chaincodeDefinition := &amp;cceventmgmt.ChaincodeDefinition&#123; Name: ccpack.GetChaincodeData().Name, Version: ccpack.GetChaincodeData().Version, Hash: ccpack.GetId()&#125; // Note - The chaincode 'id' is the hash of chaincode's (CodeHash || MetaDataHash), aka fingerprint // HandleChaincodeInstall will apply any statedb artifacts (e.g. couchdb indexes) to // any channel's statedb where the chaincode is already instantiated // Note - this step is done prior to PutChaincodeToLocalStorage() since this step is idempotent and harmless until endorsements start, // that is, if there are errors deploying the indexes the chaincode install can safely be re-attempted later. err = cceventmgmt.GetMgr().HandleChaincodeInstall(chaincodeDefinition, statedbArtifactsTar, lscc.sccprovider) defer func() &#123; cceventmgmt.GetMgr().ChaincodeInstallDone(err == nil) &#125;() // Finally, if everything is good above, install the chaincode to local peer file system so that endorsements can start if err = lscc.support.PutChaincodeToLocalStorage(ccpack); err != nil &#123; return err &#125; return nil&#125; ccpack, err := ccprovider.GetCCPackage(ccbytes)这里讲传入的ChaincodeDeploymentSpec字节反序列化，抽取信息构建CDSPackage或者SignedCDSPackage，根据是否执行peer chaincode package并签名。 ccprovider.ExtractStatedbArtifactsFromCCPackage(ccpack)和cceventmgmt.GetMgr().HandleChaincodeInstall(chaincodeDefinition, statedbArtifactsTar, lscc.sccprovider)涉及couchdb相关，笔者尚未深入分析，以后专题详述。 lscc.support.PutChaincodeToLocalStorage(ccpack)将代码package放到本地路径，在instantiate里launch时候取出。这里确定的放置package路径为path := fmt.Sprintf(&quot;%s/%s.%s&quot;, chaincodeInstallPath, ChaincodeId.Name, ChaincodeId.Version),这里的path为chaincodeInstallPath(config.GetPath(&quot;peer.fileSystemPath&quot;) + string(filepath.Separator) + &quot;chaincodes&quot;), 配置项peer.fileSystemPath是peer启动时从配置文件core.yaml里读取的。 至此完成了install工作，在endorser节点的chaincodeInstallPath路径下，存在名为ChaincodeName.ChaincodeVersion的源码包，供instantiate（3）中在安装时从该路径取出。]]></content>
      <categories>
        <category>Hyperledger Fabric</category>
      </categories>
      <tags>
        <tag>Hyperledger Fabric</tag>
        <tag>源码分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fabric 1.4源码分析 - chaincode install(1）peer端的install流程]]></title>
    <url>%2F2019%2F02%2F25%2Ffabric-source-chaincode-install-peer%2F</url>
    <content type="text"><![CDATA[Building Your First Network例子中，命令为 1peer chaincode install -n mycc -v 1.0 -p github.com/chaincode/chaincode_example02/go/ 客户端调用peer chaincode install命令安装chaincode，代码在‘peer/chaincode/install.go’这个文件下，命令关联的方法是chaincodeInstall。[install的流程与前面介绍的instantiate大体相似，重复不再展开，只勾勒差别处] 123456789101112131415161718192021222324252627282930313233343536// chaincodeInstall installs the chaincode. If remoteinstall, does it via a lscc callfunc chaincodeInstall(cmd *cobra.Command, ccpackfile string, cf *ChaincodeCmdFactory) error &#123; if cf == nil &#123; cf, err = InitCmdFactory(cmd.Name(), true, false) &#125; var ccpackmsg proto.Message // ccpackfile = args[0] if ccpackfile == "" &#123; // 检查提供的flag参数, 例如chaincodePath(-p), chaincodeVersion(-v), chaincodeName(-n) //generate a raw ChaincodeDeploymentSpec ccpackmsg, err = genChaincodeDeploymentSpec(cmd, chaincodeName, chaincodeVersion) &#125; else &#123; //read in a package generated by the "package" sub-command (and perhaps signed by multiple owners with the "signpackage" sub-command) ccpackmsg, cds, err = getPackageFromFile(ccpackfile) //get the chaincode details from cds and validate ChaincodeId.Name, ChaincodeId.Version if user provided &#125; err = install(ccpackmsg, cf) return err&#125;//install the depspec to "peer.address"func install(msg proto.Message, cf *ChaincodeCmdFactory) error &#123; creator, err := cf.Signer.Serialize() prop, _, err := utils.CreateInstallProposalFromCDS(msg, creator) signedProp, err = utils.GetSignedProposal(prop, cf.Signer) // install is currently only supported for one peer proposalResponse, err := cf.EndorserClients[0].ProcessProposal(context.Background(), signedProp) return nil&#125; install与instantiate方法类似，首先InitCmdFactory初始化命令工厂辅助，参见instantiate（1）. 获取chaincode的文件描述。这里分为两种方式，区别在于是否先使用peer chaincode package命令，得到的package作为chaincode istall参数，通常还经过多个参与者签名。具体可以参看chaincode lifecycle。package命令以后再详述。Tutorial里的例子没有先经过package，参数ccpackfile为空，则进入genChaincodeDeploymentSpec第一种方式产生cds。 genChaincodeDeploymentSpec过程与instantiate大致一致，可以参考instantiate（2），这里不同的是peer/chaincode/common.go:getChaincodeDeploymentSpec里会进入到if条件里，执行到codePackageBytes, err = container.GetChaincodePackageBytes(spec)。这个方法会区分不同的语言平台（这个语言选择是命令行里-l指定的）将代码gzip压缩，tar打包（以golang为例，为了精简包大小，去除golang标准库，fabric框架包等，加入第三方依赖包，META-INF信息等），返回[]byte。后面的流程与instantiate类似，可参考不再详述，可以认真对比下区别在于cis的Input和cds。 产生ccpackmsg(pb.ChaincodeDeploymentSpec结构)执行install。正如源码里的注释和前面instantiate（1）介绍的InitCmdFactory，install方法只能作用于唯一一个endorser（”peer.address”）。注意，这里与instantiate不同，在instantiate里还有第三步cf.BroadcastClient.Send(env).这里说明，install命令不需要将message发往orderer去排序和产生区块。 12345678910111213141516171819202122232425262728// peer.ChaincodeInvokeSpec(cis)&#123; "ChaincodeInvokeSpec" : &#123; "ChaincodeSpec" : &#123; // pb.ChaincodeSpec "Type" : "ChaincodeSpec_GOLANG", "ChaincodeId" : &#123; "Name" : "lscc" &#125;, "Input" : &#123; "Args" : ["install", "$&#123;chaincodDeploySpec&#125;"] &#125; &#125; &#125;&#125;// pb.chaincodDeploySpec(cds)&#123; "ChaincodeSpec": &#123; "Type" : "ChaincodeSpec_GOLANG", "ChaincodeId" : &#123; "Path" : "$&#123;chaincodePath&#125;", "Name" : "$&#123;chaincodeName&#125;", "Version" : "$&#123;chaincodeVersion&#125;" &#125;, "Input" : "nil" &#125;, "CodePackage": "#&#123;container.GetChaincodePackageBytes(spec)&#125;"&#125; 至此，peer端的install完成，该节点仅仅是作为执行install命令，需要本地拥有install的代码包，install并不是安装到该peer节点而是endorser节点。]]></content>
      <categories>
        <category>Hyperledger Fabric</category>
      </categories>
      <tags>
        <tag>Hyperledger Fabric</tag>
        <tag>源码分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fabric 1.4源码分析 - chaincode instantiate(8）orderer的排序过程]]></title>
    <url>%2F2019%2F02%2F24%2Ffabric-source-chaincode-instantiate-orderer-order%2F</url>
    <content type="text"><![CDATA[在instantiate(1)里提到，peer收集完endorsements后创建common.Envelope, 通过grpc发送到/orderer.AtomicBroadcast/Broadcast. common.Envelope的数据结构如下 12345678910111213141516171819202122232425262728293031323334353637383940// common.Envelope&#123; "Payload": $&#123;common.Payload&#125;.byte, "Signature": #signer.Sign($&#123;common.Payload&#125;.byte)&#125;// common.Payload&#123; "Header": $&#123;proposal.Header&#125;, // proposal参考instantiate（2） "Data": $&#123;peer.Transaction&#125;.byte&#125;// peer.Transaction&#123; "Actions": taas := make([]*peer.TransactionAction, 1), taas[0] = $&#123;peer.TransactionAction&#125;&#125;// peer.TransactionAction&#123; "Header": $&#123;proposal.Header.SignatureHeader&#125;, "Payload": $&#123;peer.ChaincodeActionPayload&#125;&#125;// peer.ChaincodeActionPayload&#123; "ChaincodeProposalPayload": $&#123;peer.ChaincodeProposalPayload&#125;.byte, "Action": $&#123;peer.ChaincodeEndorsedAction&#125;&#125; // peer.ChaincodeProposalPayload &#123; "Input": $&#123;proposal.Payload.Input&#125;, "TransientMap": nil&#125;// peer.ChaincodeEndorsedAction， response结构参考instantiate（7）&#123; "ProposalResponsePayload": $&#123;resps[0].Payload&#125;, // 所有response的payload都应该是一致的，在前面已经校验过 "Endorsements": #endorsements := make([]*peer.Endorsement, len(resps)) // 所有的endorsement&#125; 全局搜索，orderer端执行protos/orderer/ab.pb.go#_AtomicBroadcast_Broadcast_Handler -&gt; orderer/common/server/server.go#Broadcast -&gt; orderer/common/broadcast/broadcast.go#Handle. 12345678910111213141516171819202122232425262728293031// Handle starts a service thread for a given gRPC connection and services the broadcast connectionfunc (bh *handlerImpl) Handle(srv ab.AtomicBroadcast_BroadcastServer) error &#123; for &#123; msg, err := srv.Recv() chdr, isConfig, processor, err := bh.sm.BroadcastChannelSupport(msg) if err != nil &#123; return srv.Send(&amp;ab.BroadcastResponse&#123;Status: cb.Status_BAD_REQUEST, Info: err.Error()&#125;) &#125; if err = processor.WaitReady(); err != nil &#123; return srv.Send(&amp;ab.BroadcastResponse&#123;Status: cb.Status_SERVICE_UNAVAILABLE, Info: err.Error()&#125;) &#125; if !isConfig &#123; configSeq, err := processor.ProcessNormalMsg(msg) if err != nil &#123; return srv.Send(&amp;ab.BroadcastResponse&#123;Status: ClassifyError(err), Info: err.Error()&#125;) &#125; err = processor.Order(msg, configSeq) &#125; else &#123; // isConfig config, configSeq, err := processor.ProcessConfigUpdateMsg(msg) if err != nil &#123; return srv.Send(&amp;ab.BroadcastResponse&#123;Status: ClassifyError(err), Info: err.Error()&#125;) &#125; err = processor.Configure(config, configSeq) &#125; err = srv.Send(&amp;ab.BroadcastResponse&#123;Status: cb.Status_SUCCESS&#125;) &#125;&#125; for循环里srv.Recv()从grpc stream里接收数据。 bh.sm.BroadcastChannelSupport(msg)检查并返回proposal.header.channelHeader.Type,这里的HeaderType为HeaderType_ENDORSER_TRANSACTION(参考instantiate-2)，判断为normalMsg。 processor.WaitReady()。这里分三个实现，solo(orderer/consensus/solo/consensus.go), etcdraft(orderer/consensus/etcdraft/chain.go，V1.4.1版本引入)和kafka（orderer/consensus/kafka/chain.go）。solo方法是orderer单节点，不存在数据交换汇集，不会阻塞，不做任何操作直接返回。而etcdraft，kafka实现是多节点orderer，节点间需要通信。这个后面详述。 configSeq, err := processor.ProcessNormalMsg(msg)，返回当前配置的seq（单增），并且策略检查msg。这些策略可以回溯到fabric/orderer/common/multichannel/chainsupport.go#newChainSupport里创建cs.Processor = msgprocessor.NewStandardChannel(cs, msgprocessor.CreateStandardChannelFilters(cs))时初始化。策略fliter的规则可以通过名称大致猜测。 123456789101112131415161718// ProcessNormalMsg will check the validity of a message based on the current configuration. It returns the current configuration sequence number and nil on success, or an error if the message is not validfunc (s *StandardChannel) ProcessNormalMsg(env *cb.Envelope) (configSeq uint64, err error) &#123; configSeq = s.support.Sequence() err = s.filters.Apply(env) return&#125;// &lt;orderer/common/msgprocessor/standardchannel.go&gt;// CreateStandardChannelFilters creates the set of filters for a normal (non-system) chainfunc CreateStandardChannelFilters(filterSupport channelconfig.Resources) *RuleSet &#123; ordererConfig, ok := filterSupport.OrdererConfig() return NewRuleSet([]Rule&#123; EmptyRejectRule, // 校验envelop的payload不为nil NewExpirationRejectRule(filterSupport), // 校验payload.Header.SignatureHeade.Creator的x509证书未过期 NewSizeFilter(ordererConfig), // 校验enveloy的size不能超过配置的batch.AbsoluteMaxBytes，对应于configtx.yaml中的Orderer.BatchSize.AbsoluteMaxBytes（10MB） NewSigFilter(policies.ChannelWriters, filterSupport), // 检查policy当前请求拥有当前channel的写权限 &#125;)&#125; 这里简述下solo的方式，kafka后面专题详述。solo把message直接放入ch.sendChan内, solo的相关代码在orderer/consensus/solo/consensus.go 123456789101112// solo： Order accepts normal messages for orderingfunc (ch *chain) Order(env *cb.Envelope, configSeq uint64) error &#123; select &#123; case ch.sendChan &lt;- &amp;message&#123; configSeq: configSeq, normalMsg: env, &#125;: return nil case &lt;-ch.exitChan: return fmt.Errorf("Exiting") &#125;&#125; 在orderer/consensus/solo/consensus.go:main内处理channle sendChan里的消息12345678910111213141516171819202122232425262728func (ch *chain) main() &#123; var timer &lt;-chan time.Time var err error for &#123; seq := ch.support.Sequence() err = nil select &#123; case msg := &lt;-ch.sendChan: if msg.configMsg == nil &#123; // NormalMsg if msg.configSeq &lt; seq &#123; _, err = ch.support.ProcessNormalMsg(msg.normalMsg) &#125; batches, _ := ch.support.BlockCutter().Ordered(msg.normalMsg) for _, batch := range batches &#123; block := ch.support.CreateNextBlock(batch) ch.support.WriteBlock(block, nil) &#125; &#125; else &#123; // ConfigMsg ... &#125; &#125; &#125;&#125; msg.configSeq &lt; seq判断配置在写入channel和读出channel过程中是否有更改，如果有，则重新执行ProcessNormalMsg用新的规则校验消息是否合法。 batches, _ := ch.support.BlockCutter().Ordered(msg.normalMsg)将消息切片，短小消息合并成一个批次，大消息切分多个批次，使其不超过block规定长度。源码注释有详细介绍。 12345678910111213141516// messageBatches length: 0, pending: false// - impossible, as we have just received a message// messageBatches length: 0, pending: true// - no batch is cut and there are messages pending.// - 没有切分block，并且加上当前envelop后继续等待添加// messageBatches length: 1, pending: false// - the message count reaches BatchSize.MaxMessageCount。 // - 切分block，因为加上当前envelop后达到数量限制，对应configtx.yaml中的Orderer.BatchSize.MaxMessageCount(10)// messageBatches length: 1, pending: true// - the current message will cause the pending batch size in bytes to exceed BatchSize.PreferredMaxBytes. // - 切分block，因为如果加上当前envelop后，所有的batch大小超过配置的configtx.yaml中的Orderer.BatchSize.PreferredMaxBytes(512KB)，所以将之前的切分block，而当前的envelop继续等待后续添加（当前envelop大小小于PreferredMaxBytes）// messageBatches length: 2, pending: false// - the current message size in bytes exceeds BatchSize.PreferredMaxBytes, therefore isolated in its own batch.// - 当前envelop大小大于PreferredMaxBytes，将之前的作为第一个block，将当前的作为第二个block（前面已经校验过，envelop大小不会超过AbsoluteMaxBytes（10MB））// messageBatches length: 2, pending: true// - impossible CreateNextBlock创建下一个区块，可以看出，header的number（深度）+1，并且附带上前一个区块的hash，并且计算当前区块data数据的哈希，这些构成区块的信息。 123456789101112131415161718192021&#123; "Header": $&#123;BlockHeader&#125;, "Data": $&#123;BlockData&#125;, "Metadata": $&#123;BlockMetadata&#125;&#125;// BlockHeader&#123; "Number": $&#123;lastBlock.Header.Number&#125;+1, "PreviousHash": #lastBlock.Header.Hash(), "DataHash": #$&#123;BlockData&#125;.Hash()&#125;// BlockData&#123; "Data": #make([][]byte, len(envelop))&#125;// BlockMetadata&#123; "Metadata": [][]byte&#125; WriteBlock提交区块。这里会加上block的签名，然后提交。 BlockWriter.support主要有三种实现，file，json（一个区块创建一个文件，以深度命名，写入json格式），ram。这里写入orderer本地，后面通过deleiver服务同步到peer节点真正的提交到各自的本地账本db。 12345678910111213141516171819202122232425262728293031// WriteBlock should be invoked for blocks which contain normal transactions.// It sets the target block as the pending next block, and returns before it is committed.// Before returning, it acquires the committing lock, and spawns a go routine which will// annotate the block with metadata and signatures, and write the block to the ledger// then release the lock. This allows the calling thread to begin assembling the next block// before the commit phase is complete.func (bw *BlockWriter) WriteBlock(block *cb.Block, encodedMetadataValue []byte) &#123; bw.committingBlock.Lock() bw.lastBlock = block go func() &#123; defer bw.committingBlock.Unlock() bw.commitBlock(encodedMetadataValue) &#125;()&#125;// commitBlock should only ever be invoked with the bw.committingBlock held// this ensures that the encoded config sequence numbers stay in syncfunc (bw *BlockWriter) commitBlock(encodedMetadataValue []byte) &#123; // Set the orderer-related metadata field // 在BlockMetadata.Metadata内加上key为cb.BlockMetadataIndex_ORDERER的值 if encodedMetadataValue != nil &#123; bw.lastBlock.Metadata.Metadata[cb.BlockMetadataIndex_ORDERER] = utils.MarshalOrPanic(&amp;cb.Metadata&#123;Value: encodedMetadataValue&#125;) &#125; // 在BlockMetadata.Metadata内加上key为cb.BlockMetadataIndex_SIGNATURES的值 bw.addBlockSignature(bw.lastBlock) // 在BlockMetadata.Metadata内加上key为cb.BlockMetadataIndex_LAST_CONFIG的值 bw.addLastConfigSignature(bw.lastBlock) err := bw.support.Append(bw.lastBlock)&#125; 对于configMsg，流程非常相似，processor.ProcessConfigUpdateMsg(msg)相对而言，除了normalMsg的步骤外，还通过提交过来的config和本地存储的现行config比较，计算出readSet和writeSet，校验通过后，签名生成envelope(utils.CreateSignedEnvelope(cb.HeaderType_CONFIG, s.support.ChainID(), s.support.Signer(), configEnvelope, msgVersion, epoch)).而processor.Configure(config, configSeq)则与processor.Order(msg, configSeq)完全一致了，仅是名称不同。 至此，完成了orderer端的工作，这些提交到本地的区块，后续将通过deliver服务同步到peer。具体后面专题介绍。]]></content>
      <categories>
        <category>Hyperledger Fabric</category>
      </categories>
      <tags>
        <tag>Hyperledger Fabric</tag>
        <tag>源码分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fabric 1.4源码分析 - chaincode instantiate(7）endoser的endorseProposal过程]]></title>
    <url>%2F2019%2F02%2F24%2Ffabric-source-chaincode-instantiate-endorser-endorse-proposal%2F</url>
    <content type="text"><![CDATA[回顾到endorser.go#SimulateProposal,执行完callChaincode后，回顾前面，在这里有两次调用chaincode，分别是lscc的Inovke和acc的Init，callChaincode这里返回的是第一次调用的lscc的response。调用acc Init的结果只判断调用是否正常完成返回pb.ChaincodeMessage_COMPLETED。这是因为在handleInit里对初始化失败，即pb.Response.status不等于200的统一返回pb.ChaincodeMessage_ERROR，这点与Invoke的处理方式是不一样的。Invoke允许pb.ChaincodeMessage.Type=pb.ChaincodeMessage_COMPLETED,但是内层的pb.Response.status不等于200. 若返回pb.ChaincodeMessage_ERROR，则向上层返回error，最终返回&amp;pb.ProposalResponse{Response: &amp;pb.Response{Status: 500, Message: err.Error()}}到peer chainncode instantiate...这个命令的执行节点peer。 紧接着调用txsim.GetTxSimulationResults()获取上节提到的lockbased_tx_simulator的读写集合。先看下这个txsim是如何传递的。首先初始化后在callChaincode加入contextctxt = context.WithValue(ctxt, chaincode.TXSimulatorKey, txsim)。之后在chaincode/handler里构建TransactionContext实体对象时将txsim从context取出，包裹进这个新建的TransactionContext对象内。这个对象前面有提及，是关联于chainID, txID这两个存放于chaincode/handler.TXContexts.map[string]*TransactionContext里。后面在chaincode/handler.go#handlerTranction里调用txContext, err = h.isValidTxSim(msg.ChannelId, msg.Txid)}也是以此取出该txsim，将读写结果放进读写集readwriteSet。 txsim.GetTxSimulationResults()这里在返回前，先对读写集合按照key排序。这里包含pub和pri两部分数据，先关注pub部分。rwset这部分以后展开专题介绍。然后调用txsim.Done()，签名初始化txsim时候提到获取了ledger的读锁，因此这里需要释放h.txmgr.commitRWLock.RUnlock()。返回共有数据的读写集到endorser#ProcessProposal。 第二步，背书模拟结果。主要是signer对payload进行签名，并且附上自身签名的identityBytes构造成response。然后这个resp就返回方法，通过grpc发送回调用方peer.1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465// endorse the proposal by calling the ESCCfunc (e *Endorser) endorseProposal(_ context.Context, chainID string, txid string, signedProp *pb.SignedProposal, proposal *pb.Proposal, response *pb.Response, simRes []byte, event *pb.ChaincodeEvent, visibility []byte, ccid *pb.ChaincodeID, txsim ledger.TxSimulator, cd ccprovider.ChaincodeDefinition) (*pb.ProposalResponse, error) &#123; isSysCC := cd == nil // 1) extract the name of the escc that is requested to endorse this chaincode, ie, "lscc" or system chaincodes if isSysCC &#123; escc = "escc" &#125; else &#123; escc = cd.Endorsement() &#125; // marshalling event bytes if event != nil &#123; eventBytes, err = putils.GetBytesChaincodeEvent(event) &#125; ctx := Context&#123; PluginName: escc, Channel: chainID, SignedProposal: signedProp, ChaincodeID: ccid, Event: eventBytes, SimRes: simRes, Response: response, Visibility: visibility, Proposal: proposal, TxID: txid, &#125; return e.s.EndorseWithPlugin(ctx)&#125;// EndorseWithPlugin endorses the response with a pluginfunc (pe *PluginEndorser) EndorseWithPlugin(ctx Context) (*pb.ProposalResponse, error) &#123; plugin, err := pe.getOrCreatePlugin(PluginName(ctx.PluginName), ctx.Channel) prpBytes, err := proposalResponsePayloadFromContext(ctx) endorsement, prpBytes, err := plugin.Endorse(prpBytes, ctx.SignedProposal) resp := &amp;pb.ProposalResponse&#123; Version: 1, Endorsement: endorsement, Payload: prpBytes, Response: ctx.Response, &#125; return resp, nil&#125;// &lt;默认的escc ： github.com/hyperledger/fabric/core/handlers/endorsement/builtin/default_endorsement.go&gt;// Endorse signs the given payload(ProposalResponsePayload bytes), and optionally mutates it.// Returns:// The Endorsement: A signature over the payload, and an identity that is used to verify the signature// The payload that was given as input (could be modified within this function)// Or error on failurefunc (e *DefaultEndorsement) Endorse(prpBytes []byte, sp *peer.SignedProposal) (*peer.Endorsement, []byte, error) &#123; signer, err := e.SigningIdentityForRequest(sp) // serialize the signing identity identityBytes, err := signer.Serialize() // sign the concatenation of the proposal response and the serialized endorser identity with this endorser's key signature, err := signer.Sign(append(prpBytes, identityBytes...)) endorsement := &amp;peer.Endorsement&#123;Signature: signature, Endorser: identityBytes&#125; return endorsement, prpBytes, nil&#125; 最后，返回的结构体为pb.ProposalResponse1234567891011121314151617181920212223242526&#123; "Version": 1, // 固定值 "Response": $&#123;pb.Response&#125;, // 上节instantiate(6)里从`chaincode/handler`返回的`pb.Response`. A response message indicating whether the endorsement of the action was successful "Payload": $&#123;peer.ProposalResponsePayload&#125;, "Endorsement": $&#123;peer.Endorsement&#125; // 来源于`fabric/core/handlers/endorsement/plugin/plugin.go#Endorse`&#125;// peer.ProposalResponsePayload&#123; "Extension": $&#123;&amp;peer.ChaincodeAction&#125;.byte[], "ProposalHash": #putils.GetProposalHash1(proposal.hdr, proposal.Payload, ctx.Visibility)&#125;// peer.ChaincodeAction&#123; "Events": $&#123;pb.chaincodeEvent&#125;, // 上节instantiate(6)里从`chaincode/handler`返回的`pb.chaincodeEvent` "Results": $&#123;txSim.PubSimulationResults&#125;, // 模拟结果，read-write set "Response": $&#123;pb.Response&#125;, // 上节instantiate(6)里从`chaincode/handler`返回的`pb.Response`, 与外层的Response一致 "ChaincodeId": &#123;"Name":"lscc","version":"latests"&#125;&#125;// peer.Endorsement&#123; "Signature": #signer.Sign(append(prpBytes, identityBytes...)) // 将payload和dentity联合起来产生签名 "Endorser": #identityBytes = signer.Serialize()&#125;]]></content>
      <categories>
        <category>Hyperledger Fabric</category>
      </categories>
      <tags>
        <tag>Hyperledger Fabric</tag>
        <tag>源码分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fabric 1.4源码分析 - chaincode instantiate(6）application chaincode的初始化]]></title>
    <url>%2F2019%2F02%2F24%2Ffabric-source-chaincode-instantiate-chaincode-init%2F</url>
    <content type="text"><![CDATA[接着继续分析acc的初始化过程，在instantiate(3)里提到endorser.callChaincode有两次的ChaincodeSupport#Execute调用,由代码分析两次不同的ChaincodeMessage的MessageType分别为MessageType_TRANSACTION（对应cis）和MessageType_INIT（对应cds），相应调用lscc的Invoke和acc的Init。具体过程可以参考上节分析。 先看lscc的Invoke.参考上节提及的execute在两端chaincode/handler.go（peer端）和shim/handler.go（chaincode容器端）的流程，lscc相当于chaincode容器（尽管是inproccontainer），使用shim/handler.go#handleReady里select-case，lscc的ChaincodeMessage_TRANSACTION进入handleTransaction方法。这里的主要流程与上节介绍的handleInit一致，不同的是这里调用的是lscc的Invoke方法。这里需要强调一点，在handleTransaction,handleInit这些handleXXX方法里，进入就是启动goroutinne来运行所有的逻辑，这样可以立刻返回并行来处理下一个请求。 123456789101112131415161718192021222324252627282930313233343536373839404142// Invoke implements lifecycle functions "deploy", "start", "stop", "upgrade".// Deploy's arguments - &#123;[]byte("deploy"), []byte(&lt;chainname&gt;), &lt;unmarshalled pb.ChaincodeDeploymentSpec&gt;&#125;//// Invoke also implements some query-like functions// Get chaincode arguments - &#123;[]byte("getid"), []byte(&lt;chainname&gt;), []byte(&lt;chaincodename&gt;)&#125;func (lscc *lifeCycleSysCC) Invoke(stub shim.ChaincodeStubInterface) pb.Response &#123; function := string(args[0]) switch function &#123; case DEPLOY, UPGRADE: channel := string(args[1]) // 获取channel的设置，这个在create channel时传入配置 ac, exists := lscc.sccprovider.GetApplicationConfig(channel) // the maximum number of arguments depends on the capability of the channel // 校验当前传入参数与channel配置（ac）是否冲突，这里校验主要是ac.Capabilities().PrivateChannelData() depSpec := args[2] cds, err := utils.GetChaincodeDeploymentSpec(depSpec) // optional arguments here (they can each be nil and may or may not be present) // args[3] is a marshalled SignaturePolicyEnvelope representing the endorsement policy // args[4] is the name of escc, default "escc"(系统自带) // args[5] is the name of vscc, default "vscc"(系统自带） // args[6] is a marshalled CollectionConfigPackage struct var EP []byte p := cauthdsl.SignedByAnyMember(peer.GetMSPIDs(channel)) EP, err = utils.Marshal(p) var collectionsConfig []byte // we proceed with a non-nil collection configuration only if we support the PrivateChannelData capability // PrivateChannelData， channel内的私有数据 if ac.Capabilities().PrivateChannelData() &amp;&amp; len(args) &gt; 6 &#123; collectionsConfig = args[6] &#125; cd, err := lscc.executeDeployOrUpgrade(stub, channel, cds, EP, escc, vscc, collectionsConfig, function) cdbytes, err := proto.Marshal(cd) return shim.Success(cdbytes) &#125; ...&#125; 进入lscc的Invoke方法。回顾instantiate(2)里cis的数据结构&quot;Args&quot; : [&quot;deploy&quot;, &quot;${channelId}&quot;, &quot;${chaincodDeploySpec}&quot;],这里的select-case进入DEPLOY,UPGRADE分支。分支里主要是校验以及处理传入参数，包括指定的scc和private data相关，重要的方法是lscc.executeDeployOrUpgrade，传参就是刚获取和校验过的参数。 进入executeDeployOrUpgrade。首先执行lscc.support.GetChaincodeFromLocalStorage(chaincodeName, chaincodeVersion)，获取ccprovider.CCPackage,这是安装阶段执行peer chaincode install命令时打包并且上传到该节点的chaincode定义，放在peer节点本地目录的（这个在install里具体分析其结构）。然后执行lscc.executeDeploy。 123456789101112131415161718192021222324252627282930313233// executeDeploy implements the "instantiate" Invoke transactionfunc (lscc *lifeCycleSysCC) executeDeploy( stub shim.ChaincodeStubInterface, chainname string, cds *pb.ChaincodeDeploymentSpec, policy []byte, escc []byte, vscc []byte, cdfs *ccprovider.ChaincodeData, ccpackfs ccprovider.CCPackage, collectionConfigBytes []byte,) (*ccprovider.ChaincodeData, error) &#123; //just test for existence of the chaincode in the LSCC chaincodeName := cds.ChaincodeSpec.ChaincodeId.Name _, err := lscc.getCCInstance(stub, chaincodeName) //retain chaincode specific data and fill channel specific ones cdfs.Escc = string(escc) cdfs.Vscc = string(vscc) cdfs.Policy = policy // retrieve and evaluate instantiation policy cdfs.InstantiationPolicy, err = lscc.support.GetInstantiationPolicy(chainname, ccpackfs) // get the signed instantiation proposal signedProp, err := stub.GetSignedProposal() err = lscc.support.CheckInstantiationPolicy(signedProp, chainname, cdfs.InstantiationPolicy) err = lscc.putChaincodeData(stub, cdfs) err = lscc.putChaincodeCollectionData(stub, cdfs, collectionConfigBytes) return cdfs, nil&#125; 这里先检查该chaincode是否已经部署过lscc.getCCInstance(stub, chaincodeName)，实际上是调用stub.GetState(ccname),构建&amp;pb.ChaincodeMessage{Type: pb.ChaincodeMessage_PUT_STATE, Payload: payloadBytes, Txid: txid, ChannelId: channelId}发送到对端chaincode/handler。这里会执行到handler.callPeerWithChaincodeMsg方法。其实里面就是hanlder本身维护着一个名叫responseChannel的map[string]chan pb.ChaincodeMessage, 先创建一个channel放入map中，key是由channelID和txid构造而成，即关联当前请求，在退出方法时使用defer从map中删除这个channle。而在这个过程中，在handler.sendReceive里使用handler.serialSendAsync发送ChaincodeMessage，然后select-case一直等待从channel里读取消息。 相应的，当对端回复消息pb.ChaincodeMessage_RESPONSE，shim/handler会进入fabric/core/chaincode/shim/handler.go:handleReady-&gt;handler.sendChannel(msg),这里从responseChannel这个map里拿出相应的channel，放入消息。实际上，通过这个实现了异步通信，结合上面提到的每次进入handlerXXX方法都使用goroutine处理而立刻返回，实现了其并发通信。 123456789101112131415161718192021222324252627282930313233// callPeerWithChaincodeMsg sends a chaincode message (for e.g., GetState along with the key) to the peer for a given txid and receives the response.func (handler *Handler) callPeerWithChaincodeMsg(msg *pb.ChaincodeMessage, channelID, txid string) (pb.ChaincodeMessage, error) &#123; // Create the channel on which to communicate the response from the peer var respChan chan pb.ChaincodeMessage var err error if respChan, err = handler.createChannel(channelID, txid); err != nil &#123; return pb.ChaincodeMessage&#123;&#125;, err &#125; defer handler.deleteChannel(channelID, txid) return handler.sendReceive(msg, respChan)&#125;//sends a message and selectsfunc (handler *Handler) sendReceive(msg *pb.ChaincodeMessage, c chan pb.ChaincodeMessage) (pb.ChaincodeMessage, error) &#123; errc := make(chan error, 1) for &#123; select &#123; case outmsg, val := &lt;-c: return outmsg, nil &#125; &#125;&#125;func (handler *Handler) sendChannel(msg *pb.ChaincodeMessage) error &#123; handler.Lock() defer handler.Unlock() txCtxID := handler.getTxCtxId(msg.ChannelId, msg.Txid) handler.responseChannel[txCtxID] &lt;- *msg return nil&#125; chaincode/handler对该类型请求在hanldeMessage里HandleGetState一路判断最后执行res, err = txContext.TXSimulator.GetState(chaincodeName, getState.Key),从本地账本db获取namespace为lscc，key为${chaincodeName}的数据（最终执行queryHelper.txmgr.db.GetState(ns, key）。在executeDeploy稍后可以看到，执行lscc.putChaincodeData(stub, cdfs)操作时执行相对应的put state操作，即deploy之后会把该chaincode写入peer本地db的lscc的namespace里。因此，通过校验get回来的数据为空来判断该chaincode没有deploy过。 123456789101112131415161718192021222324252627282930// core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/helper.go:40func (h *queryHelper) getState(ns string, key string) ([]byte, error) &#123; if err := h.checkDone(); err != nil &#123; return nil, err &#125; versionedValue, err := h.txmgr.db.GetState(ns, key) val, ver := decomposeVersionedValue(versionedValue) if h.rwsetBuilder != nil &#123; h.rwsetBuilder.AddToReadSet(ns, key, ver) &#125; return val, nil&#125;// core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/helper.gofunc decomposeVersionedValue(versionedValue *statedb.VersionedValue) ([]byte, *version.Height) &#123; var value []byte var ver *version.Height if versionedValue != nil &#123; value = versionedValue.Value ver = versionedValue.Version &#125; return value, ver&#125;// core/ledger/kvledger/txmgmt/rwsetutil/rwset_builder.go// AddToReadSet adds a key and corresponding version to the read-setfunc (b *RWSetBuilder) AddToReadSet(ns string, key string, version *version.Height) &#123; nsPubRwBuilder := b.getOrCreateNsPubRwBuilder(ns) nsPubRwBuilder.readMap[key] = NewKVRead(key, version)&#125; 从上面看出，账本的value是statedb.VersionedValue结构，包含value和version(结构为*version.Height{BlockNum uint64, TxNum uint64})。获取的结果加入readSet,其中参数（namesapce：lscc，key：${chaincodeName}, ver: 版本） 然后获取并检查部署策略lscc.support.CheckInstantiationPolicy。然后执行lscc.putChaincodeData(stub, cdfs)，调用的是stub.PutState(cd.Name, cdbytes)。入参是chaincode.Name和ccprovider.ChaincodeData序列化字节，这个是通过从本地存储的codepackage和刚刚传入的escc，vscc等参数构建成的。向对端chaincode/handler发送pb.ChaincodeMessage_PUT_STATE。 1234567891011121314151617181920//-------- ChaincodeData is stored on the LSCC -------// ChaincodeData defines the datastructure for chaincodes to be serialized by proto// Type provides an additional check by directing to use a specific package after instantiation// Data is Type specifc (see CDSPackage and SignedCDSPackage)type ChaincodeData struct &#123; // Name of the chaincode Name string `protobuf:"bytes,1,opt,name=name"` Version string `protobuf:"bytes,2,opt,name=version"` Escc string `protobuf:"bytes,3,opt,name=escc"` Vscc string `protobuf:"bytes,4,opt,name=vscc"` // Policy endorsement policy for the chaincode instance Policy []byte `protobuf:"bytes,5,opt,name=policy,proto3"` // Data data specific to the package Data []byte `protobuf:"bytes,6,opt,name=data,proto3"` // Id of the chaincode that's the unique fingerprint for the CC This is not currently used anywhere but serves as a good eyecatcher Id []byte `protobuf:"bytes,7,opt,name=id,proto3"` // InstantiationPolicy for the chaincode InstantiationPolicy []byte `protobuf:"bytes,8,opt,name=instantiation_policy,proto3"`&#125; 数据分为普通channle公开信息和private消息，这里只讨论前者。相应的，对端chaincode/handler在hanldeMessage里HandlePutState一路判断最后执行res, err = txContext.TXSimulator.SetState(chaincodeName, putState.Key, putState.Value).跟上面区别的是，这里在ockbased_tx_simulator.go文件里，清楚指明这是模拟交易。同时，只把(namespace:lscc, key:${chaincode.Name}, value:${ChaincodeData}序列化字节）的组合加入到writeSet内，然后就返回结果ChaincodeMessage_RESPONSE或者错误消息 ChaincodeMessage_ERROR，并没有真正作用于账本db。同时，没有指定version，这个是在orderer排序打包时添加的。关于这里的read-write set，可以参考官网概念Read-Write set semantics。 12345678// core/ledger/kvledger/txmgmt/txmgr/lockbasedtxmgr/lockbased_tx_simulator.gofunc (s *lockBasedTxSimulator) SetState(ns string, key string, value []byte) error &#123; err := s.helper.checkDone() err := s.checkBeforeWrite() err := s.helper.txmgr.db.ValidateKeyValue(key, value) s.rwsetBuilder.AddToWriteSet(ns, key, value) return nil&#125; 最后列举下Invoke lscc后从shim/handler返回到chaincode/handler的数据结构pb.ChaincodeMessage1234567891011121314151617181920212223242526// pb.ChaincodeMessage&#123; "Type": pb.ChaincodeMessage_COMPLETED, "Payload": $&#123;pb.Response&#125;.byte[], "Txid": $&#123;msg.Txid&#125;, "ChaincodeEvent": $&#123;pb.chaincodeEvent&#125;, // nil "ChannelId": $&#123;stub.ChannelId&#125;// pb.Response &#123; "Status": 200, // ok "Payload": $&#123;ChaincodeData&#125;.byte[]&#125;// pb.ChaincodeData &#123; "Name": $&#123;application chaincode.name&#125;, // CDSPackage.depSpec.ChaincodeSpec.ChaincodeId.Name, 同“Data” "Version": &#123;application chaincode.version&#125;, // CDSPackage.depSpec.ChaincodeSpec.ChaincodeId.Version, 同“Data” "Escc": "escc", "Vscc": "vscc", "Policy": $&#123;cb.SignaturePolicyEnvelope&#125;.byte[], // endorsement policy, 来源于lscc.go#Invoke, cauthdsl.SignedByAnyMember(peer.GetMSPIDs(channel)) "Data": , // CDSPackage.datab, 执行lscc.support.GetChaincodeFromLocalStorage(chaincodeName, chaincodeVersion).GetChaincodeData()，来源于install chaincode打包构建的fabric/core/common/ccprovider/ccprovider.go:GetCCPackage "Id": , // CDSPackage.id, 同“Data” "InstantiationPolicy": $&#123;InstantiationPolicy&#125;.byte[] // 来源于lscc.go#executeDeploy, lscc.support.GetInstantiationPolicy(chainname, ccpackfs)&#125; 这里继续直接看acc的Init，之前的步骤与lscc完全一致，可以参考上节。Tutorial里的例子的chaincode代码在github.com/hyperledger/fabric/examples/chaincode/go/example02/chaincode.go,里面的Init方法主要是读入参数，然后调用stub.PutState后返回success。细节如上，不再复述。]]></content>
      <categories>
        <category>Hyperledger Fabric</category>
      </categories>
      <tags>
        <tag>Hyperledger Fabric</tag>
        <tag>源码分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fabric 1.4源码分析 - chaincode instantiate(5）system chaincode的初始化]]></title>
    <url>%2F2019%2F02%2F24%2Ffabric-source-chaincode-instantiate-system-chaincode%2F</url>
    <content type="text"><![CDATA[接下来分析lscc的执行过程，在这之前，先看下其初始化注册过程。在peer/node/start.go#Serve里registerChaincodeSupport(ccSrv, ccEndpoint, ca, aclProvider)。进入方法，主要是scc.CreateSysCCs(ccp, sccp, aclProvider)，就是引入了scc/importsysccs.go#builtInSystemChaincodes里定义的SCC,即cscc,lscc和gscc（包含chaincode的Name，Path代码地址，Chaincode对象，acl，policy等）。然后逐个执行sccp.RegisterSysCC(cc)注册SCC,实际上就是加入SystemChaincodeProvider.Registrar.typeRegistry（map[string]*inprocContainer）里，key是chaincode的Name，这个后面初始化时候会使用到。 这里完成注册后，在serve()里稍后的位置执行sccp.DeploySysCCs(&quot;&quot;, ccp)进行部署。第一个参数是channelId为“”，说明这些scc是chainless的，可以供所有的channel使用。进入方法，首先构造cds&amp;pb.ChaincodeDeploymentSpec{ExecEnv: pb.ChaincodeDeploymentSpec_SYSTEM, ChaincodeSpec: spec}.这里注意到，部署类型是ChaincodeDeploymentSpec_SYSTEM，回顾上节acc类型是ChaincodeDeploymentSpec_DOCKER，说明SCC是部署在peer内的，而acc是部署在另外新的chaincode docker容器里。然后执行ccprov.Execute(ctxt, cccid, chaincodeDeploymentSpec),最终实际上又回到前面介绍过的ChaincodeSupport.Execute方法上。后面的执行与acc是一致的，所区别的在于launch。区别于acc使用的是dockercontroller/DockerVM#start,scc使用的是inproccontroller.InprocVM#start.首先vm.registry.typeRegistry[path],这个在前面已经注册了可以获取出来。然后开始真正的launch。 12345678910111213141516171819202122232425262728293031323334353637func (ipc *inprocContainer) launchInProc(ctxt context.Context, id string, args []string, env []string, ccSupport ccintf.CCSupport) error &#123; peerRcvCCSend := make(chan *pb.ChaincodeMessage) ccRcvPeerSend := make(chan *pb.ChaincodeMessage) ccchan := make(chan struct&#123;&#125;, 1) ccsupportchan := make(chan struct&#123;&#125;, 1) go func() &#123; defer close(ccchan) err := shim.StartInProc(env, args, ipc.chaincode, ccRcvPeerSend, peerRcvCCSend) &#125;() go func() &#123; defer close(ccsupportchan) inprocStream := newInProcStream(peerRcvCCSend, ccRcvPeerSend) err := ccSupport.HandleChaincodeStream(ctxt, inprocStream) &#125;() select &#123; case &lt;-ccchan: close(peerRcvCCSend) case &lt;-ccsupportchan: close(ccRcvPeerSend) case &lt;-ipc.stopChan: close(ccRcvPeerSend) close(peerRcvCCSend) &#125;&#125;// StartInProc is an entry point for system chaincodes bootstrap. It is not an API for chaincodes.func StartInProc(env []string, args []string, cc Chaincode, recv &lt;-chan *pb.ChaincodeMessage, send chan&lt;- *pb.ChaincodeMessage) error &#123; stream := newInProcStream(recv, send) err := chatWithPeer(chaincodename, stream, cc)&#125;type inProcStream struct &#123; recv &lt;-chan *pb.ChaincodeMessage send chan&lt;- *pb.ChaincodeMessage&#125; 这里主要有两个goroutine，分别处理chaincode shim和peer的handle stream。同时，有两个channel，peerRcvCCSend,ccRcvPeerSend,可以从名字判断出消息的流向。 启动第一个goroutineshim.StartInProc,向chaincodeSupport发送注册。用于这里的chatWithPeer跟acc介绍的是同一个都在chaincode.go下，在这里面构造了该scc的handler（设置inProcStream这个stream）发送了&amp;pb.ChaincodeMessage{Type: pb.ChaincodeMessage_REGISTER, Payload: payload})这个消息。这个类比于acc里在chaincode容器的handler。 启动第二个goroutineccSupport.HandleChaincodeStream(ctxt, inprocStream)，用于处理注册请求，这个类比于acc里的peer的handler，所不同的是，acc里的stream是grpc server stream，是由grpc server构建的，而这里则是封装了peerRcvCCSend,ccRcvPeerSend这两个channel。总而言之，在acc内，新chaincode容器内通过grpc方式与peer通信，而scc由于都处在同一个peer容器内，是通过go channel的方法实现。其他的主要流程都是一致的。 第三部分的select-case则是在处理chaincode退出的情况。这个方法在外层InprocVM#Start是go routine启动的，最外层等待的是select-case等待launchState.Done()channel的返回，这个跟acc是一致的。 一路返回到core/chaincode/chaincode_support.go#Invoke方法，在执行完Launch后，最后执行ChaincodeSupport#execute.在前面instantiate（3）里介绍过execute方法，这里直接看发送的ChaincodeMessage。在chaincode_support.go#Invoke里指定部署ChaincodeDeploymentSpec的ChaincodeMessage_Type为pb.ChaincodeMessage_INIT,由chaincode/handler.go#serialSendAsync,对端的shim/handler.go#handleMessage。前面初始化完成后，两端handler状态都为ready，select-case进入hanlder.state=ready,ChaincodeMessageType=ChaincodeMessage_INIT执行到#handleInit.12345678910111213141516171819202122232425262728func (handler *Handler) handleInit(msg *pb.ChaincodeMessage, errc chan error) &#123; // The defer followed by triggering a go routine dance is needed to ensure that the previous state transition // is completed before the next one is triggered. The previous state transition is deemed complete only when // the beforeInit function is exited. Interesting bug fix!! go func() &#123; var nextStateMsg *pb.ChaincodeMessage defer func() &#123; handler.triggerNextState(nextStateMsg, errc) &#125;() // Call chaincode's Run // Create the ChaincodeStub which the chaincode can use to callback // 用传入的数据初始化stub stub := new(ChaincodeStub) err := stub.init(handler, msg.ChannelId, msg.Txid, input, msg.Proposal) res := handler.cc.Init(stub) // Send COMPLETED message to chaincode support and change state nextStateMsg = &amp;pb.ChaincodeMessage&#123;Type: pb.ChaincodeMessage_COMPLETED, Payload: resBytes, Txid: msg.Txid, ChaincodeEvent: stub.chaincodeEvent, ChannelId: stub.ChannelId&#125; &#125;()&#125;# github.com/hyperledger/fabric/core/scc/lscc//Init is mostly useless for SCCfunc (lscc *lifeCycleSysCC) Init(stub shim.ChaincodeStubInterface) pb.Response &#123; return shim.Success(nil)&#125; 在这里，主要是stub.init构建stub，这个桩的概念在远程调用里常见。然后执行handler.cc.Init(stub)。这里看lscc.go#Init.对于lscc(大多数scc)来说，初始化并不需要进行特殊处理，因此这里只是简单返回shim.Success的resp.在handleInit里，defer func将结果返回，这里如果执行成功，返回pb.ChaincodeMessage_COMPLETED，否则返回pb.ChaincodeMessage_ERROR.同样的，在chaincode/handler.go#handleMessage里对pb.ChaincodeMessage_COMPLETED, pb.ChaincodeMessage_ERROR这两个调用chaincode shim的回复消息调用handler.Notify(msg)，即tctx.ResponseNotifier &lt;- msg.这个在前面的instantiate(3)小节提过，异步调用的错误以及结果都发送到txctx.ResponseNotifie这个chan内再做后续的处理。至此，完成lscc的初始化和注册，后面再介绍acc的初始化的rpc过程。 命名缩写参考cc : chaincodescc : system chaincodeccp, ccprov : chaincode providersccp : system chaincode providercccid : ccprovider.CCContext]]></content>
      <categories>
        <category>Hyperledger Fabric</category>
      </categories>
      <tags>
        <tag>Hyperledger Fabric</tag>
        <tag>源码分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fabric 1.4源码分析 - chaincode instantiate(4）chaincode容器启动及注册]]></title>
    <url>%2F2019%2F02%2F24%2Ffabric-source-chaincode-instantiate-chaincode-launch%2F</url>
    <content type="text"><![CDATA[接下来回到容器启动后在容器中初始化和注册chaincode。在上一节构建docker里提到，容器的启动命令（指默认语言或者显式设置语言-l golang）为lc.Args = []string{&quot;chaincode&quot;, fmt.Sprintf(&quot;-peer.address=%s&quot;, c.PeerAddress)}。在构建容器时fabric/core/chaincode/platforms/golang/platform.go#GenerateDockerBuild，指定了构建参数DockerBuildOptions util.DockerBuildOptions{ Cmd: fmt.Sprintf(&quot;GOPATH=/chaincode/input:$GOPATH go build -tags \&quot;%s\&quot; %s -o /chaincode/output/chaincode %s&quot;, gotags, ldflagsOpt, pkgname), InputStream: codepackage, OutputStream: binpackage, } 这里指的是，将pkgname(spec.ChaincodeId.Path)指定的chaincode代码运行go build -o chaincode打包成名为chaincode的可执行文件。因此容器启动时，实际上调用了install的chaincode代码里的main方法，这也是分析的入口。在Building Your First Network官方教程，在fabric-samples/first-network/scripts的scripts.sh和utils.sh里找到install chaincode路径为${CC_SRC_PATH},在scripts.sh里可以得到CC_SRC_PATH=&quot;github.com/chaincode/chaincode_example02/go/&quot;. 该目录下chaincode_example02.go的main方法err := shim.Start(new(SimpleChaincode))启动。进入到chaincode.Start(cc Chaincode)方法 SetupChaincodeLogging()设置日志相关 获取变量chaincode名称viper.GetString(&quot;chaincode.id.name&quot;)。回顾上节在创建chaincode容器时在container_runtime.go设置了变量append(c.CommonEnv, &quot;CORE_CHAINCODE_ID_NAME=&quot;+cname)。 factory.InitFactories(factory.GetDefaultOpts())初始化BCCSP（blockchain crypto service）服务。待详述。 userChaincodeStreamGetter构建与peer节点的grpc client。首先，获取peerAddress,这个在container_runtime.go设置了flag启动参数-peer.address，即endorser的peerAddress。然后，与该peer建立连接，构造/protos.ChaincodeSupport/Register的grpc stream。 chatWithPeer(chaincodename string, stream PeerChaincodeStream, cc Chaincode).新建handler := newChaincodeHandler(stream, cc)，（这里是shim/handler.go），这时候handler.state = created。这是在chaincode容器里的，与之相对的，在peer容器里也有handler，对等处理. 然后handler.serialSend(&amp;pb.ChaincodeMessage{Type: pb.ChaincodeMessage_REGISTER, Payload: payload});这里payload是&amp;pb.ChaincodeID{Name: chaincodename}。将注册信息发送到peer节点后，启动goroutine。这个goroutine里是for循环，每次循环都新启动一个子goroutine，这个子go routine负责从stream里接受一条消息，放到两个channel（msgAvail, errc）里。msgAvail, errc = stream.Recv()，接收完一条即完成子goroutine的生命周期，下次循环再另起一个。父goroutine读取这两个channel，对接收到的消息msgAvail，则调用handler.handleMessage(in, errc)处理。最外层waitC等待终止信号，期间包含定期发送的pb.ChaincodeMessage_KEEPALIVE，直到遇到接收错误等才终止通信stream，返回异常。 再看peer端的对等handler注册。（这里是chaincode/handler.go）这部分在peer node start启动时候执行，注册pb.ChaincodeSupportServer(chaincodeSupport)。每次收到shim/handler的register方法意味着新的Register(&amp;chaincodeSupportRegisterServer{stream},一路下去在fabric/core/chaincode/chaincode_support#HandleChaincodeStream里新建chaincode/handler，handler.state没有显式设置则为0值，即Created。然后进入主要方法chaincode/handler.ProcessStream。这方法流程跟上面的chatWithPeer非常相似，多了主动发起KeepAlive，以及在异常退出时取消注册hanlder。这也就意味着，chaincode容器与peer容器始终建立连接。hanlder从steam里读取message，然后handleMessage。这里实际上也就是在Handler.Registry（即ChaincodeSupport.HandlerRegistry)里注册该hanlder，跟上节从registry里通过chaincode.cname查找hanlder呼应。此外，在这个处理过程中，peer相应的改变registry里handler的状态，给chaincode容器回复&amp;pb.ChaincodeMessage{Type: pb.ChaincodeMessage_REGISTERED}。然后，handler置成established状态，对端的handler收到pb.ChaincodeMessage_REGISTERED，也将状态置为established。 chaincode/handler在置成established状态后马上执行notifyRegistry，这里主要两件事情，1.发送pb.ChaincodeMessage_READY，并且状态改为ready；2. 通知HandlerRegistry已完成注册（即从launching里delete当前chaincodename，关闭LaunchState.done channel可以判断以完成注册，这样，在core/chaincode/runtime_launcher.go#start里的select {case &lt;-launchState.Done():可以继续往下走，因为这里调用的RuntimeLauncher.Runtime.Start是通过goroutine异步启动的），这里与上节介绍启动时注册aunching防止重复部署对应。对端收到通知后，也置状态为ready。至此完成了chaincode的启动工作，后续两端handler进入处理Init／Invoke请求流程。]]></content>
      <categories>
        <category>Hyperledger Fabric</category>
      </categories>
      <tags>
        <tag>Hyperledger Fabric</tag>
        <tag>源码分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fabric 1.4源码分析 - chaincode instantiate(3）endorser的propocessProposal主过程]]></title>
    <url>%2F2019%2F02%2F24%2Ffabric-source-chaincode-instantiate-endorser-process-proposal%2F</url>
    <content type="text"><![CDATA[由peer的grpc请求到/protos.Endorser/ProcessProposal，在endorser里对应着protos/peer/peer.pb.go#_Endorser_ProcessProposal_Handler，进入处理方法core/endorser/endorser.go#ProcessProposal。endorser的处理过程主要分为三个阶段，preProcess, simulate和endorse。其中，preProcess主要做校验工作，包括数据结构的完整性(header)，唯一性防止重放攻击，校验签名，对application chaincode（以后简称acc）检查ACL策略等等。这部分校验比较直观和简单，这里不做深入分析。需要注意的是，在这里还获取了TX模拟器txsim, err = e.s.GetTxSimulator(chainID, txid);和历史请求执行器historyQueryExecutor, err = e.s.GetHistoryQueryExecutor(chainID);。1234567891011121314// GetTxSimulator returns the transaction simulator for the specified ledger// a client may obtain more than one such simulator; they are made unique// by way of the supplied txidfunc (s *SupportImpl) GetTxSimulator(ledgername string, txid string) (ledger.TxSimulator, error) &#123; lgr := s.Peer.GetLedger(ledgername) return lgr.NewTxSimulator(txid)&#125;// NewTxSimulator implements method in interface `txmgmt.TxMgr`func (txmgr *LockBasedTxMgr) NewTxSimulator(txid string) (ledger.TxSimulator, error) &#123; s, err := newLockBasedTxSimulator(txmgr, txid) txmgr.commitRWLock.RLock() return s, nil&#125; 这个模拟器和执行器是对应于账本ledger的，也就是channel，入参ledgername就是channelId。chainless的proposals的ledger.TxSimulator和ledger.HistoryQueryExecutor都是nil。模拟器是基于锁的，在生成模拟器时，同时加上了读锁，在下一阶段simulate结束后就立即释放。同时，也生成了历史数据查询器。关于这个锁，代码里有下面一段解释。 txsim acquires a shared lock on the stateDB. As this would impact the block commits (i.e., commit of valid write-sets to the stateDB), we must release the lock as early as possible. Hence, this txsim object is closed in simulateProposal() as soon as the tx is simulated and rwset is collected before gossip dissemination if required for privateData. 接下来主要分析simulate过程。在endorser.SimulateProposal这个方法中，可以看到存在多处对当前chaincode是否System chaincode系统链码的判断。（注意，当前请求channel是lscc）在当前篇幅里主要分析instantiate的过程，其他的判断分支以后再详述。在SimulateProposal里，首先判断当前是否是scc，这里是lscc，进入分支获取util.GetSysCCVersion()，scc的版本固定为string常量‘latest’.整个流程方法调用顺序为SimulateProposal-&gt;endorser.callChaincode-&gt;endorser.support.Execute-&gt;endorser.support.ChaincodeSupport.Execute-&gt;ChaincodeSupport.Invoke-&gt;ChaincodeSupport.execute-&gt;cs.HandlerRegistry.Handler(cname).Execute在这里，首先关键方法是e.callChaincode1234567891011121314151617181920212223242526/ call specified chaincode (system or user)func (e *Endorser) callChaincode(ctxt context.Context, chainID string, version string, txid string, signedProp *pb.SignedProposal, prop *pb.Proposal, cis *pb.ChaincodeInvocationSpec, cid *pb.ChaincodeID, txsim ledger.TxSimulator) (*pb.Response, *pb.ChaincodeEvent, error) &#123; ctxt = context.WithValue(ctxt, chaincode.TXSimulatorKey, txsim) // is this a system chaincode scc := e.s.IsSysCC(cid.Name) res, ccevent, err = e.s.Execute(ctxt, chainID, cid.Name, version, txid, scc, signedProp, prop, cis) // ----- BEGIN - SECTION THAT MAY NEED TO BE DONE IN LSCC ------ // if this a call to deploy a chaincode, We need a mechanism // to pass TxSimulator into LSCC. Till that is worked out this // special code does the actual deploy, upgrade here so as to collect // all state under one TxSimulator // // NOTE that if there's an error all simulation, including the chaincode table changes in lscc will be thrown away if cid.Name == "lscc" &amp;&amp; len(cis.ChaincodeSpec.Input.Args) &gt;= 3 &amp;&amp; (string(cis.ChaincodeSpec.Input.Args[0]) == "deploy" || string(cis.ChaincodeSpec.Input.Args[0]) == "upgrade") &#123; userCDS, err := putils.GetChaincodeDeploymentSpec(cis.ChaincodeSpec.Input.Args[2]) // this should not be a system chaincode _, _, err = e.s.Execute(ctxt, chainID, cds.ChaincodeSpec.ChaincodeId.Name, cds.ChaincodeSpec.ChaincodeId.Version, txid, false, signedProp, prop, cds) &#125; // ----- END ------- return res, ccevent, err&#125; 此处将模拟器txsim加入context。此处存在两次调用endorser.support.Execute.第一次调用的是外层的ChaincodeInvokeSpeccis。在endorser.support.Execute里先构建ccprovider.NewCCContext（cccid），这里的cccid.canonicalName设置为name + &quot;:&quot; + version，即lscc:latest。然后获取系统定义的decorator，这个是在node启动时peer/node.go#Serve里初始化的，从配置文件core.yaml里读取配置。配置文件里的定义:”append or mutate the chaincode input passed to the chaincode”. library: /opt/lib/decorator.so。这里将decorator作用于此cis。下一步就是调用ChaincodeSupport.Execute。ChaincodeSupport.Execute先执行ChaincodeSupport.Invoke。在invoke方法里，先执行cs.Launch(ctxt, cccid, spec)。这里执行的chaincode是lscc，系统链码已经在启动阶段launch状态为running了，这个在后面会说到，因此直接返回到ChaincodeSupport.Invoke。然后用chaincodeSpec.Input(这里其实是[&quot;deploy&quot;, &quot;${channelId}&quot;, &quot;${chaincodDeploySpec}&quot;])构建pb.ChaincodeMessage，进入到Chaincode.execute方法执行cs.execute(ctxt, cccid, ccMsg)。这里的pb.ChaincodeMessage类型为pb.ChaincodeMessage_TRANSACTION(cis). Launch starts executing chaincode if it is not already running. This method blocks until the peer side handler gets into ready state or encounters a fatal error. If the chaincode is already running, it simply returns. 下面是三个入参的结构123456789101112131415161718192021ctx = context.WithValue(ctx, chaincode.HistoryQueryExecutorKey, historyQueryExecutor)ctxt = context.WithValue(ctxt, chaincode.TXSimulatorKey, txsim)cccid := &amp;CCContext&#123; ChainID: cname, // channelID Name: name, // ChaincodeID.name (lscc) Version: version, // metadata.Version (latest) TxID: txid, Syscc: syscc, // is this a system chaincode (true) SignedProposal: signedProp, Proposal: prop, canonicalName: name + ":" + version, ProposalDecorations: nil, &#125;ccmsg := &amp;pb.ChaincodeMessage&#123; Type: messageType, // pb.ChaincodeMessage_TRANSACTION Payload: payload, // chaincodeSpec.Input (cis.cs.Input) Txid: txid, ChannelId: cid,&#125; 在ChaincodeSupport.execute先判断处理当前chaincode（根据canonicalName区分,lscc:latest）的handler是否已经注册，这个handler注册的过程后面介绍。然后获取这个handler执行。1234567891011121314151617181920212223242526272829303132333435363738394041424344func (h *Handler) Execute(ctxt context.Context, cccid *ccprovider.CCContext, msg *pb.ChaincodeMessage, timeout time.Duration) (*pb.ChaincodeMessage, error) &#123; txctx, err := h.TXContexts.Create(ctxt, msg.ChannelId, msg.Txid, cccid.SignedProposal, cccid.Proposal) defer h.TXContexts.Delete(msg.ChannelId, msg.Txid) h.setChaincodeProposal(cccid.SignedProposal, cccid.Proposal, msg) h.serialSendAsync(msg, true) var ccresp *pb.ChaincodeMessage select &#123; case ccresp = &lt;-txctx.ResponseNotifier: // response is sent to user or calling chaincode. ChaincodeMessage_ERROR are typically treated as error case &lt;-time.After(timeout): err = errors.New("timeout expired while executing transaction") &#125; return ccresp, err&#125;// serialSendAsync serves the same purpose as serialSend (serialize msgs so gRPC will be happy). In addition, it is also asynchronous so send-remoterecv--localrecv loop can be nonblocking. Only errors need to be handled and these are handled by communication on supplied error channel. A typical use will be a non-blocking or nil channelfunc (h *Handler) serialSendAsync(msg *pb.ChaincodeMessage, sendErr bool) &#123; go func() &#123; if err := h.serialSend(msg); err != nil &#123; if sendErr &#123; // provide an error response to the caller h.Notify(resp) &#125; &#125; &#125;()&#125;func (h *Handler) Notify(msg *pb.ChaincodeMessage) &#123; tctx := h.TXContexts.Get(msg.ChannelId, msg.Txid) tctx.ResponseNotifier &lt;- msg tctx.CloseQueryIterators()&#125;// serialSend serializes msgs so gRPC will be happyfunc (h *Handler) serialSend(msg *pb.ChaincodeMessage) error &#123; h.serialLock.Lock() defer h.serialLock.Unlock() if err := h.chatStream.Send(msg); err != nil &#123;&#125;&#125; 构造TransactionContext,其中有个参数为ResponseNotifier: make(chan *pb.ChaincodeMessage, 1),用以异步执行时接收结果。构造前先判断channelID和txId的组合是否已存在，构造过程加锁，避免重复处理。 core/chaincode/handler.go#serialSendAsync异步处理，实际上即启用go routine调用handler#serialSend，使用handler.chatStream发送，此处也是grpc调用。错误以及结果都发送到txctx.ResponseNotifie这个chan内(rpc结果通过调用handler#Notify)。由于是异步调用，函数立即返回，处理结果在外层handler.Execute等待从这个chan读取数据，这里也加上了超时的机制。这里的lscc的调用后面再分析。 如果这里调用一切正常，流程将退回到callChaincode继续往下走,然后满足“lscc”，参数为“deploy”的条件，所以从cis里的Input参数获取cds并反序列化成ChaincodeDeploymentSpec对象，重复刚才的步骤。这里稍微有些不同的是，这里的pb.ChaincodeMessage类型为pb.ChaincodeMessage_INIT(cds)，并且这个用户自定义的application chaincode需要launch。123456789101112131415161718192021222324252627282930313233343536373839404142434445// Launch starts executing chaincode if it is not already running. This method// blocks until the peer side handler gets into ready state or encounters a fatal// error. If the chaincode is already running, it simply returns.func (cs *ChaincodeSupport) Launch(ctx context.Context, cccid *ccprovider.CCContext, spec ccprovider.ChaincodeSpecGetter) error &#123; cname := cccid.GetCanonicalName() if cs.HandlerRegistry.Handler(cname) != nil &#123; return nil &#125; // UserRunsCC值一直回溯到 node.start#registerChaincodeSupport 里的 userRunsCC := chaincode.IsDevMode() // 这个值实际上是配置文件 core.yaml 里的 peer.mode : net，配置文件的描述如下 // # In dev mode, user runs the chaincode after starting peer from command line on local machine. // # In net mode, peer will run chaincode in a docker container. if cs.UserRunsCC &amp;&amp; !cccid.Syscc &#123; chaincodeLogger.Error( "You are attempting to perform an action other than Deploy on Chaincode that is not ready and you are in developer mode.", ) &#125; // The only user of this context value is the in-process controller used to support system chaincode. // context加入“CCHANDLER” : ChaincodeSupport ctx = context.WithValue(ctx, ccintf.GetCCHandlerKey(), cs) return cs.Launcher.Launch(ctx, cccid, spec)&#125;// core/chaincode/runtime_launcher.go// Launch chaincode with the appropriate runtime.func (r *RuntimeLauncher) Launch(ctx context.Context, cccid *ccprovider.CCContext, spec ccprovider.ChaincodeSpecGetter) error &#123; chaincodeID := spec.GetChaincodeSpec().ChaincodeId cds, _ := spec.(*pb.ChaincodeDeploymentSpec) if cds == nil &#123; cds, err = r.getDeploymentSpec(ctx, cccid, chaincodeID) &#125; if cds.CodePackage == nil &amp;&amp; cds.ExecEnv != pb.ChaincodeDeploymentSpec_SYSTEM &#123; ccpack, err := r.PackageProvider.GetChaincode(chaincodeID.Name, chaincodeID.Version) cds = ccpack.GetDepSpec() &#125; err := r.start(ctx, cccid, cds) return nil&#125; 这里需要部署chaincode需要代码code，在instantiate的cds里参数CodePackage为nil,同时ExecEnv=pb.ChaincodeDeploymentSpec_DOCKER。这里是通过r.PackageProvider.GetChaincode(chaincodeID.Name, chaincodeID.Version)获取具体的代码，实际上是core/common/ccprovider/ccprovider.go#GetChaincodeFromPath从本地路径获取，而这些code是在peer chaincode install时由lscc放到本地目录下，在instantiate时按照chaincodeName和chaincodeVersion取出。这里的path为chaincodeInstallPath,这个是peer启动时从配置文件core.yaml里读取的，配置项为peer.fileSystemPath. 然后执行r.start(ctx, cccid, cds)用这部分代码启动。HandlerRegistry.Launching(cname)先注册launching状态，防止被重复部署。接着启动go routine执行r.Runtime.Start(ctx, cccid, cds)。这里首先获取容器启动参数设置，环境变量等配置，这些配置将作用于后续启动的容器。这里需要留意的是，设置了lc.Args = []string{&quot;chaincode&quot;, fmt.Sprintf(&quot;-peer.address=%s&quot;, c.PeerAddress)}，这里是chaincode容器启动的执行命令。然后根据cds的ExecEnv设置选择容器类型，这里共有两种类型，SYSTEM和DOCKER，用户定义的chaincode是启动在docker容器里的。然后开始启动容器的步骤c.Processor.Process(ctxt, vmtype, scr);。先对容器名加锁vmc.lockContainer(ccid.GetName())防止同时执行创建容器操作，然后执行container.StartContainerReq的Do方法。这里的dockercontroller.Start方法是真正创建容器的地方。 构造镜像名（需要保证唯一性）和容器名，停止重名的容器（如果存在，stop,kill,remove），然后使用该镜像创建容器。创建过程遇到err == docker.ErrNoSuchImage，事实上这是必须的，因为要保证是第一次部署，当前环境没有该镜像。入参提供了container.Builder，此处尝试重新构建和部署镜像，然后重新创建容器。如果还是失败，则排除了镜像不存在的错误原因，直接返回错误结果。 这里，存在attachStdout := viper.GetBool(&quot;vm.docker.attachStdout&quot;)这个配置，将容器的输出直接到控制台，方便开发调试。 将需要上传的文件传到容器里，这里的文件是各种配置，例如TLS的key和cert。 使用第三方库fsouza/go-dockerclient来启动docker容器err = client.StartContainer(containerName, nil). 由于启动容器是异步操作，马上返回到runtime_launcher.start，这里是select-cast,等待启动完成的chan信号，或者异常，或者超时。如果启动失败，则停止容器，取消注册RuntimeLauncher.Registry里到handler以及launchState状态。 在core/container/dockercontroller/dockercontroller.go:preFormatImageName定义了docker镜像和容器名，其中，镜像名为${NetworkID}-${PeerId}-${ChaincodeName}-${Version}-${Hash}，容器名为${NetworkID}-${PeerId}-${ChaincodeName}-${Version}，其中NetworkID从配置文件core.yml里的peer.NetworkId配置项。 这部分docker相关比较繁琐，暂时没有时间精力深入学习和分析源码，可以参考fabric源码解析20——ACC的部署]]></content>
      <categories>
        <category>Hyperledger Fabric</category>
      </categories>
      <tags>
        <tag>Hyperledger Fabric</tag>
        <tag>源码分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fabric 1.4源码分析 - chaincode instantiate(2) peer端的调用流程]]></title>
    <url>%2F2019%2F02%2F23%2Ffabric-source-chaincode-instantiate-peer%2F</url>
    <content type="text"><![CDATA[接下来具体执行env, err := instantiate(cmd, cf)，这里可以看到入参是cobra.Command和刚刚初始化的ChaincodeCmdFactory,结果是提交给Orderer的Envelope messageprotos/common.Envelope. 12345678910111213141516171819202122//instantiate the command via Endorserfunc instantiate(cmd *cobra.Command, cf *ChaincodeCmdFactory) (*protcommon.Envelope, error) &#123; spec, err := getChaincodeSpec(cmd) cds, err := getChaincodeDeploymentSpec(spec, false) creator, err := cf.Signer.Serialize() prop, _, err := utils.CreateDeployProposalFromCDS(channelID, cds, creator, policyMarshalled, []byte(escc), []byte(vscc), collectionConfigBytes) var signedProp *pb.SignedProposal signedProp, err = utils.GetSignedProposal(prop, cf.Signer) // instantiate is currently only supported for one peer proposalResponse, err := cf.EndorserClients[0].ProcessProposal(context.Background(), signedProp) if proposalResponse != nil &#123; // assemble a signed transaction (it's an Envelope message) env, err := utils.CreateSignedTx(prop, cf.Signer, proposalResponse) return env, nil &#125; return nil, nil&#125; getChaincodeSpec和getChaincodeDeploymentSpec从提交的command里获取相应的参数，检查以及设置参数（例如是否指定escc(endorsement system chaincode), vscc(verification system chaincode), ConstructorJSON 构造参数），进而构造chaincode描述结构体pb.ChaincodeSpec和部署结构体pb.ChaincodeDeploymentSpec。pb.ChaincodeSpec(cs)里指定了ChaincodeSpec_Type语言类型，默认为go，ChaincodeID，chaincode的标识（包括路径Path，名称Name，版本Version），以及Input（即ConstructorJSON 构造参数，初始化的值，命令行里-c参数值，例子里的’{“Args”:[“init”,”a”, “100”, “b”,”200”]}’）。pb.ChaincodeDeploymentSpec(cds)里包裹ChaincodeSpec，以及CodePackage，打包后用于安装部署的代码，ExecEnv，指定运行于同一系统中或者docker容器。getChaincodeDeploymentSpec入参区分是否当前是dev开发模式，而且是否需要构造CodePackage。构造打包是在install安装时进行的，后面分析install时详述。 CreateDeployProposalFromCDS从上面构造的部署结构体得到peer.ChaincodeInvocationSpec(cis),进而得到peer.Proposal,然后使用初始化时候得到的signer对这个提案进行签名，这个步骤与初始化时候非常相似。 Transaction的结构可以参看总结Hyperledger Fabric V1.0: Block Structure pb.ChaincodeInvokeSpec(cis)里指定Type为golang，ChaincodeID指定为lscc（lifecycle system chaincode），然后将上面构造的cds序列化在[]byte作为Input字段。CreateDeployProposalFromCDS这个方法从部署结构体cds出发，生成随机字符串NONCE，使用签名生成ProposalTxID; 指定类型Type为HeaderType_ENDORSER_TRANSACTION(此处多补充一句，初始化时获取配置的类型为HeaderType_CONFIG),指定channelId，chaincodeID(包括chaincode的version，path，name)。这里的chaincodeID是构造的peer.ChaincodeInvocationSpec的参数，指定了Name为lscc，lifecycle systme chaincode。此外，还有初始化过程MSP获取的creator(即序列化后的signer)表明身份以及创建时间Timestamp等。这些构成了transaction的header,连同序列化后的ChaincodeInvocationSpec对象作为payload,共同组成了peer.Proposal{Header: hdrBytes, Payload: ccPropPayloadBytes}.最后使用signer进行签名。 rpc调用。初始化时候构建了endorser client组（实际上instantiate只允许有一个）。使用该client发送请求grpc.Invoke(ctx, &quot;/protos.Endorser/ProcessProposal&quot;, in, out, c.cc, opts...)，这里ctx传入了golang的context.Background()，后面的rpc调用派生自该顶层context。服务端的处理后面再详述。注意这里是同步调用。endorser rpc的数据结构SignedProposal如下 // peer.SignedProposal: { &quot;ProposalBytes&quot;: &quot;${propBytes}&quot;, &quot;Signature&quot;: &quot;${signature}&quot; // signature, err := signer.Sign(propBytes) } // peer.Proposal : { &quot;Header&quot;: &quot;${hdrBytes}&quot;, &quot;Payload&quot;: &quot;${ccPropPayloadBytes}&quot; } // common.Header(hdrBytes) : { &quot;ChannelHeader&quot; : { // common.ChannelHeader &quot;Type&quot; : &quot;HeaderType_ENDORSER_TRANSACTION&quot;, // common.HeaderType &quot;TxId&quot; : &quot;#ComputeProposalTxID(nonce, creator)&quot;, &quot;Timestamp&quot;: &quot;timestamp&quot;, // util.CreateUtcTimestamp() &quot;ChannelId&quot;: &quot;${chainID}&quot;, &quot;Extension&quot;: &quot;${ccHdrExtBytes}&quot;, &quot;Epoch&quot;: &quot;epoch&quot; // always 0 }, &quot;SignatureHeader&quot; : { // common.SignatureHeader &quot;Nonce&quot;: &quot;nonce&quot;, // nonce, err := crypto.GetRandomNonce() &quot;Creator&quot;: &quot;creator&quot; // creator, err := cf.Signer.Serialize() } } // peer.ChaincodeProposalPayload(ccPropPayload) { &quot;Input&quot;: &quot;cisBytes&quot;, &quot;TransientMap&quot;: nil } // peer.ChaincodeHeaderExtension(ccHdrExtBytes) : { &quot;ChaincodeId&quot; : &quot;${cis.ChaincodeSpec.ChaincodeId}&quot; } // peer.ChaincodeInvokeSpec(cis) { &quot;ChaincodeInvokeSpec&quot; : { &quot;ChaincodeSpec&quot; : { // pb.ChaincodeSpec &quot;Type&quot; : &quot;ChaincodeSpec_GOLANG&quot;, &quot;ChaincodeId&quot; : { &quot;Name&quot; : &quot;lscc&quot; }, &quot;Input&quot; : { &quot;Args&quot; : [&quot;deploy&quot;, &quot;${channelId}&quot;, &quot;${chaincodDeploySpec}&quot;] } } } } // pb.chaincodDeploySpec(cds) { &quot;ChaincodeSpec&quot;: { &quot;Type&quot; : &quot;ChaincodeSpec_GOLANG&quot;, &quot;ChaincodeId&quot; : { &quot;Path&quot; : &quot;${chaincodePath}&quot;, &quot;Name&quot; : &quot;${chaincodeName}&quot;, &quot;Version&quot; : &quot;${chaincodeVersion}&quot; }, &quot;Input&quot; : { &quot;Args&quot; : [&quot;init&quot;,&quot;a&quot;, &quot;100&quot;, &quot;b&quot;,&quot;200&quot;] } }, &quot;CodePackage&quot;: nil } 收集endorser的response，然后构建签名后的transaction。下面的方法是普适的。 // CreateSignedTx assembles an Envelope message from proposal, endorsements, and a signer. // This function should be called by a client when it has collected enough endorsements // for a proposal to create a transaction and submit it to peers for ordering func CreateSignedTx(proposal *peer.Proposal, signer msp.SigningIdentity, resps ...*peer.ProposalResponse) (*common.Envelope, error) { // the original header hdr, err := GetHeader(proposal.Header) // the original payload pPayl, err := GetChaincodeProposalPayload(proposal.Payload) shdr, err := GetSignatureHeader(hdr.SignatureHeader) // get header extensions so we have the visibility field hdrExt, err := GetChaincodeHeaderExtension(hdr) // ensure that all actions are bitwise equal and that they are successful // 比较所有的response返回的结果是一致的 var a1 []byte for n, r := range resps { if n == 0 { a1 = r.Payload if r.Response.Status != 200 { return nil, fmt.Errorf(&quot;Proposal response was not successful, error code %d, msg %s&quot;, r.Response.Status, r.Response.Message) } continue } if bytes.Compare(a1, r.Payload) != 0 { return nil, fmt.Errorf(&quot;ProposalResponsePayloads do not match&quot;) } } // fill endorsements endorsements := make([]*peer.Endorsement, len(resps)) for n, r := range resps { endorsements[n] = r.Endorsement } // 构造各级数据结构并序列化，包含原始的proposal的payload，header等信息， // 以及repsponse的payload，各个resp相应的endorsement， // 构造了新的peer.Transaction，然后使用signer进行签名。 // 最后将序列化后的transaction和签名组装成common.Envelope并返回。 } 最后，初始化时候得到的cf.BroadcastClient.Send(env).至此，完成了chaincode instantiate在peer端的所有操作。]]></content>
      <categories>
        <category>Hyperledger Fabric</category>
      </categories>
      <tags>
        <tag>Hyperledger Fabric</tag>
        <tag>源码分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fabric 1.4源码分析 - chaincode instantiate(1）peer端的初始化过程]]></title>
    <url>%2F2019%2F02%2F23%2Ffabric-source-chaincode-instantiate-initcmdfactory%2F</url>
    <content type="text"><![CDATA[推荐参考两个博客： fabric源码解析——序 : 国内同道的源码分析系列，模块化 Hyperledger Fabric源码深度解析 : 对MSP，BCCSP的介绍到位 To know the internals of certain permissioned blockchain platform : IBM员工，参与HF项目的概念总结文章系列 选择peer chaincode instantiate作为开篇，是因为笔者学习官网过程中没有完全理解install与instantiate的区别，所以选择深入源码学习来加深理解 官网Building Your First Network例子中，命令为 1peer chaincode instantiate -o orderer.example.com:7050 --tls --cafile /opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem -C $CHANNEL_NAME -n mycc -v 1.0 -c &apos;&#123;&quot;Args&quot;:[&quot;init&quot;,&quot;a&quot;, &quot;100&quot;, &quot;b&quot;,&quot;200&quot;]&#125;&apos; -P &quot;AND (&apos;Org1MSP.peer&apos;,&apos;Org2MSP.peer&apos;)&quot; 此前，需要先设置环境变量 1234CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/users/Admin@org2.example.com/mspCORE_PEER_ADDRESS=peer0.org2.example.com:7051CORE_PEER_LOCALMSPID=&quot;Org2MSP&quot;CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/peers/peer0.org2.example.com/tls/ca.crt 客户端调用peer chaincode instatiate命令部署chaincode，代码在‘peer/chaincode/instantiate.go’这个文件下，命令关联的方法是chaincodeDeploy(使用了流行的第三方命令行cobra库，具体可自行学习)。 12345678910func chaincodeDeploy(cmd *cobra.Command, args []string, cf *ChaincodeCmdFactory) error &#123; if cf == nil &#123; cf, err = InitCmdFactory(cmd.Name(), true, true) &#125; defer cf.BroadcastClient.Close() env, err := instantiate(cmd, cf) err = cf.BroadcastClient.Send(env)&#125; 这个方法主要分为3个步骤（源码只列出关键步骤，省略判断逻辑及错误处理等，下同）。 入参cf *ChaincodeCmdFactory为nil，未初始化。（可回溯到peer启动peer/main.go#main时看到mainCmd.AddCommand(chaincode.Cmd(nil))）。初始化chaincode命令工厂，包含客户端及辅助类。这个方法在所有的chaincode命令前都先执行，入参有所不同。12345678// ChaincodeCmdFactory holds the clients used by ChaincodeCmdtype ChaincodeCmdFactory struct &#123; EndorserClients []pb.EndorserClient // 与endorser交互的客户端 DeliverClients []api.PeerDeliverClient // Certificate tls.Certificate // Signer msp.SigningIdentity BroadcastClient common.BroadcastClient // 与orderer交互的客户端&#125; 下面关注下初始化的过程123456789101112131415161718192021222324252627282930313233343536// peer/chaincode/common.gofunc InitCmdFactory(cmdName string, isEndorserRequired, isOrdererRequired bool) (*ChaincodeCmdFactory, error) &#123; if isEndorserRequired &#123; if err = validatePeerConnectionParameters(cmdName); err != nil &#123;&#125; for i, address := range peerAddresses &#123; endorserClient, err := common.GetEndorserClientFnc(address, tlsRootCertFile) endorserClients = append(endorserClients, endorserClient) deliverClient, err := common.GetPeerDeliverClientFnc(address, tlsRootCertFile) deliverClients = append(deliverClients, deliverClient) &#125; &#125; certificate, err := common.GetCertificateFnc() signer, err := common.GetDefaultSignerFnc() var broadcastClient common.BroadcastClient if isOrdererRequired &#123; endorserClient := endorserClients[0] orderingEndpoints, err := common.GetOrdererEndpointOfChainFnc(channelID, signer, endorserClient) // override viper env viper.Set("orderer.address", orderingEndpoints[0]) broadcastClient, err = common.GetBroadcastClientFnc() &#125; return &amp;ChaincodeCmdFactory&#123; EndorserClients: endorserClients, DeliverClients: deliverClients, Signer: signer, BroadcastClient: broadcastClient, Certificate: certificate, &#125;, nil&#125; 入参为cmdName(命令名) string(命令名), isEndorserRequired（是否需要初始化endorser client）, isOrdererRequired（是否需要初始化orderer broadcast client） bool。这个是根据命令传参，例如instantiate部署chaincode既需要endorser背书，也需要orderer排序打包并且扩散全网，而install安装则只需要背书。 instantiate需要背书，isEndorserRequired=true。validatePeerConnectionParameters校验命令参数。例如，校验peerAddresses个数及相应的tlsRootCertFiles匹配。注意，这里有// currently only support multiple peer addresses for invoke注释，对非invoke方法只能传入一个peerAddress，如install，instantiate。这也说明，这些操作只能作用于单个节点，而且下面也可以看到只需要单个节点的背书。 GetEndorserClientFnc和GetPeerDeliverClientFnc方法非常类似。都是构造以下这个client(主要是grpc client, hyperledger fabric使用grpc作为rpc方案)，数据结构完全一致，只是后续的用途有所不同。 1234567891011121314151617181920// GetEndorserClient returns a new endorser client. If the both the address and// tlsRootCertFile are not provided, the target values for the client are taken// from the configuration settings for "peer.address" and// "peer.tls.rootcert.file"func GetEndorserClient(address, tlsRootCertFile string) (pb.EndorserClient, error) &#123; if address != "" &#123; // 从命令行传入peer address, --peerAddresses, --tlsRootCertFiles peerClient, err = NewPeerClientForAddress(address, tlsRootCertFile) &#125; else &#123; // 从环境变量读取peer address peerClient, err = NewPeerClientFromEnv() &#125; return peerClient.Endorser()&#125;&amp;PeerClient&#123; commonClient: commonClient&#123; GRPCClient: gClient, // grpc client. 工程使用grpc作为节点间的远程调用方案 address: address, // “peer.address” sn: override // "peer.tls.serverhostoverride" 在上面的例子中，命令里并没有提供peerAddress和tlsRootCertFile这两个参数，因此从环境变量里读取。在源码中，使用了流行的第三方库viper来进行配置加载和管理。也就是我们要先设置的变量CORE_PEER_ADDRESS和CORE_PEER_TLS_ROOTCERT_FILE。 生成客户端X509证书GetCertificateFnc。客户端通信时是否需要校验X509证书是由环境变量peer.tls.enabled和peer.tls.clientAuthRequired这两个参数决定的。在该tutorial例子中并没有设置。如果需要校验，则另外需要提供两个参数peer.tls.clientKey.file和peer.tls.clientCert.file，分别对应节点的public/private key文件。需要说明的是，在前面构造的endorseClient和peerDeliverClient过程中，也已经读取这两个参数和文件，生成X509 key pair，可以进行tls通信。这里的方法也是构建peerClient，获取该key pair。（TODO：以后专题分析加密） GetDefaultSignerFnc调用本地MSPmspmgmt.GetLocalMSP().GetDefaultSigningIdentity()获取signer，后续用于发送的proposal进行签名，构造signedProposal。（TODO：以后专题分析msp成员管理服务） isOrdererRequired这个参数初始化时候传入，根据后续操作是否需要与orderer交互乃至全网同步而是否选择初始化。chaincode instantiate需要全网同步，因此需要初始化。命令行-o参数指定orderer地址，如果没有指定，则通过GetOrdererEndpointOfChainFnc获取orderer地址。这个是通过调用cscc（configuration system chaincode，配置系统链码）的GetConfigBlock方法（参数为channelId）实现的。流程与一般的chaincode调用相似。 12345678910111213141516171819202122func GetOrdererEndpointOfChain(chainID string, signer msp.SigningIdentity, endorserClient pb.EndorserClient) ([]string, error) &#123; // query cscc for chain config block invocation := &amp;pb.ChaincodeInvocationSpec&#123; ChaincodeSpec: &amp;pb.ChaincodeSpec&#123; Type: pb.ChaincodeSpec_Type(pb.ChaincodeSpec_Type_value["GOLANG"]), ChaincodeId: &amp;pb.ChaincodeID&#123;Name: "cscc"&#125;, Input: &amp;pb.ChaincodeInput&#123;Args: [][]byte&#123;[]byte(cscc.GetConfigBlock), []byte(chainID)&#125;&#125;, &#125;, &#125; creator, err := signer.Serialize() prop, _, err := putils.CreateProposalFromCIS(pcommon.HeaderType_CONFIG, "", invocation, creator) signedProp, err := putils.GetSignedProposal(prop, signer) proposalResp, err := endorserClient.ProcessProposal(context.Background(), signedProp) // parse config block block, err := putils.GetBlockFromBlockBytes(proposalResp.Response.Payload) envelopeConfig, err := putils.ExtractEnvelope(block, 0) bundle, err := channelconfig.NewBundleFromEnvelope(envelopeConfig) return bundle.ChannelConfig().OrdererAddresses(), nil&#125; 构造ChaincodeInvocationSpeccis描述该chaincode请求，包含请求的语言（GOLAND），chaincodeId（“cscc”），具体请求Input（cscc.GetConfigBlock）等。这个请求体格式通用于所有的系统及普通chaincode。然后，构造proplsal，并通过上面MSP获取的signer进行签名，在被调用方需要依次判断该节点是否有权限调用chaincode。 接着，通过上面构建等endorserClient进行grpc调用grpc.Invoke(ctx, &quot;/protos.Endorser/ProcessProposal&quot;, in, out, c.cc, opts...)，简单介绍下参数(ctx context.Context，in *SignedProposal, opts …grpc.CallOption, out ProposalResponse, c.cc endorser.grpc.ClientConn)。此处grpc调用的方法是/protos.Endorser/ProcessProposal，endorser端校验，调用cscc，得到结果ProposalResponse返回。这个后面详述。 最后，从rpc调用的结果bytes依次反序列化成block，获取并反序列化block中第一个envelope，把envelope的payload反序列化成configtx，并且获取相应的信息channelConfig，最后从channelConfig中获取到OrdererAddresses。（这里的结构处理是根据cscc的GetConfigBlock调用结果）选择第一个orderer的地址重写为系统变量orderer.address。(TODO：这里是否会把所有的请求都集中到第一个orderer有待研究cscc的方法) GetBroadcastClientFnc与上面的GetEndorserClient主要流程非常相似，主要区别有两点。第一，GetEndorserClient构建的client，使用的是peer的设置，而GetBroadcastClientFnc获取的是orderer的设置，从环境变量里获取构造参数，configFromEnv(&quot;orderer&quot;)，这里orderer.address这个参数正是上面的方法从chaincode config block里获取并设置为环境变量的。第二，调用了orderClient.Broadcast()，这里grpc.NewClientStream(ctx, &amp;_AtomicBroadcast_serviceDesc.Streams[0], c.cc, &quot;/orderer.AtomicBroadcast/Broadcast&quot;, opts...)直接指明了grpc的远程调用方法为&quot;/orderer.AtomicBroadcast/Broadcast&quot;. 至此，完成了chaincode调用的初始化构造过程。后续的chaincode调用就可以直接使用此过程中构建的client及signer等执行不同的操作。]]></content>
      <categories>
        <category>Hyperledger Fabric</category>
      </categories>
      <tags>
        <tag>Hyperledger Fabric</tag>
        <tag>源码分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hyperledger Fabric（V1.4.0）官网Tutorial学习]]></title>
    <url>%2F2019%2F02%2F23%2Ffabric-tutorial-learning%2F</url>
    <content type="text"><![CDATA[官方教程Writing Your First Application在安装最新的node（v11.7.0）和npm（v6.5.0）后，在fabric-samples/fabcar/javascript目录下执行npm install报错如下 1error Failed at the grpc@1.14.2 install script &apos;node-pre-gyp install --fallback-to-build --library=static_library&apos;. Solution : 只需要在fabric-samples/fabcar/javascript目录下先执行npm install grpc@1.14.2（注意，需要先删除‘node_modules’这个目录和’package-lock.json’这个文件），然后再在该目录下执行npm install即可。 Update : 该grpc版本比较老，新版本node不兼容支持，可以将node版本降级。参考讨论 grpc-node issue-689 Building Your First Network中，默认开启TSL链接，如果关闭配置，出现以下异常报错。 1232019-10-22 08:26:05.220 UTC [grpc] switchBalancer -&gt; DEBU 03b ClientConn switching balancer to &quot;pick_first&quot;2019-10-22 08:26:05.220 UTC [grpc] HandleSubConnStateChange -&gt; DEBU 03c pickfirstBalancer: HandleSubConnStateChange: 0xc0004e38c0, CONNECTING2019-10-22 08:26:05.223 UTC [grpc] createTransport -&gt; DEBU 03d grpc: addrConn.createTransport failed to connect to &#123;peer0.org1.example.com:7051 0 &lt;nil&gt;&#125;. Err :connection error: desc = &quot;transport: authentication handshake failed: tls: first record does not look like a TLS handshake&quot;. Reconnecting... 这个是在以下3个配置文件里配置，调用./byfn.sh up，最后会执行/fabric-samples/first-network/scripts里的script.sh和utils.sh，里面的peer channel create, peer chaincode install等命令都携带参数--tls $CORE_PEER_TLS_ENABLED. orderer ： ORDERER_GENERAL_TLS_ENABLED(/fabric-samples/first-network/base/peer-base.yaml) ca ：FABRIC_CA_SERVER_TLS_ENABLED(/fabric-samples/first-network/base/peer-base.yaml) peer ：CORE_PEER_TLS_ENABLED(/fabric-samples/first-network/docker-compose-ca.yaml)]]></content>
      <categories>
        <category>Hyperledger Fabric</category>
      </categories>
      <tags>
        <tag>Hyperledger Fabric</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[起手式]]></title>
    <url>%2F2019%2F02%2F23%2Fforeword%2F</url>
    <content type="text"><![CDATA[关于博客创建这个博客的缘由是今年在学习区块链的知识，想记录和分享下学习和进步的过程，为了随时可以翻看和补充修正。此外，记录下平时工作（大数据开发）过程中遇到的问题，踩过的坑。如果在这个基础上还能做到知识输出，或者能做到分享让大家互相学习和指正，那就算喜出望外了。 关于区块链浪潮过去后才能看出谁在裸泳。区块链的热度去年随着韭菜风波过去后，进入技术的沉淀期。始终认为区块链只是一门技术，而且还是没有杀手级应用，还不成熟的技术，更不应该上升到信仰的高度。一项新技术在短期内会被高估，在长期则被低估。选择这个人少的方向，也意味着冒更大的风险，无从估计三年后的发展，也意味着可能会一直碌碌无为。我希望像竹子一样，用三年时间好好扎根沉淀，静待破土。 关于Hyperledger Fabric学习我从未从事过区块链相关开发工作，甚至也未曾工作中使用过go语言。学习区块链纯属出于个人兴趣，而选择Hyperledger Fabric则是选择联盟链里最火热的框架。题外话，我目前还不了解看不清公有链DAPP的发展，还是专注于联盟链的学习。Fabric的学习，主要是从官网获得概念，以源码分析来补充和加深理解细节。 因为缺乏项目经验，尚且无法高屋建瓴的去模块化分析和概括整个框架，而更多的选择从点及面的延伸性学习。因此，你可能无法从我的博客里获得非常系统的学习，甚至于也无法保证博客内容的完整性和百分百的准确性，我只是记录着自己学习Hyperledger Fabric（基于v1.4）的历程，以及与同道交流。希望以此督促自己坚持下去，工作业余时间能深入去学习和分享，定期来更新。]]></content>
  </entry>
</search>
